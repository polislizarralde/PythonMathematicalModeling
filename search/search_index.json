{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my Python playground for Mathematical Modeling","text":"<p>Hola! I'm a Diana Lizarralde, a researcher at the Centre for Ecological and Evolutionary Synthesis (CEES) at the University of Oslo (UiO), working on the Nordemics project. This website is dedicated to exploring various mathematical modeling projects using Python.</p>"},{"location":"#featured-project-the-plague-in-scania","title":"Featured Project: The Plague in Scania","text":"<p>In this project, we delve into the mysteries surrounding the Plague in Scania by using advanced mathematical models and Python programming. Our aim is to uncover hidden patterns, trends, and facts that have remained unknown until now. Stay tuned for updates and findings from this exciting research!</p> <p>Feel free to browse through my website to learn more about these fascinating projects. If you have any questions or would like to collaborate, don't hesitate to reach out or open a pull request. Happy exploring!</p> <p>For more personal information, please refer to my personal webpage at https://polislizarralde.github.io.</p>"},{"location":"template/","title":"Template","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install leafmap\n</pre> # !pip install leafmap In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"PlagueProject/ClusterFittingSouthScania/","title":"ClusterFittingSouthScania","text":"In\u00a0[447]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[448]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[449]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[450]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[451]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[452]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] In\u00a0[453]: Copied! <pre># Set the working directory for private files\n\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n</pre> # Set the working directory for private files  # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) In\u00a0[454]: Copied! <pre># Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n# Create a GeoDataFrame from the DataFrame\nsouthScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry')\n\n# Assigning the coordinate reference system (CRS) to the GeoDataFrame\nsouthScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")\n\n# For checking the CRS\n# southScaniaMap.crs\n</pre> # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads) # Create a GeoDataFrame from the DataFrame southScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry')  # Assigning the coordinate reference system (CRS) to the GeoDataFrame southScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")  # For checking the CRS # southScaniaMap.crs <p>We will focus only in the parishes affected by the plague. To do so, first we filter the data frame.</p> In\u00a0[455]: Copied! <pre>colorByColumn(southScaniaMap, 'EndPlaguePeriod')\nplagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red']\nlen(plagueSouthScania)\n</pre> colorByColumn(southScaniaMap, 'EndPlaguePeriod') plagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red'] len(plagueSouthScania) Out[455]: <pre>88</pre> <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[456]: Copied! <pre>plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan)\nplagueSouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['BeginPlaguePeriod'], format='%b %Y')\nplagueSouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['EndPlaguePeriod'], format='%b %Y')\n</pre> plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan) plagueSouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['BeginPlaguePeriod'], format='%b %Y') plagueSouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['EndPlaguePeriod'], format='%b %Y') <p>Doing the clusters under the condition of shared borders</p> In\u00a0[457]: Copied! <pre># Create a graph\nG = nx.Graph()\n\n# Add nodes\nfor index, row in plagueSouthScania.iterrows():\n    G.add_node(index, polygon=row['geometry'])\n\n# Add edges\nfor i, row_i in plagueSouthScania.iterrows():\n    for j, row_j in plagueSouthScania.iterrows():\n        if i != j and row_i['geometry'].touches(row_j['geometry']):\n            G.add_edge(i, j)\n# Find connected components\nconnected_components = list(nx.connected_components(G))\n\n# Create new geodataframes for each subset of connected polygons\ngdfs = [gpd.GeoDataFrame(plagueSouthScania.loc[list(component)], crs=plagueSouthScania.crs) for component in connected_components]\n\nplagueSouthScania = plagueSouthScania.copy()\n\n# Initialize a new column in the original dataframe\nplagueSouthScania['component'] = -1\n\n# Loop over the list of connected components\nfor i, component in enumerate(connected_components):\n    # For each component, set the 'component' value of the corresponding rows to the current component number\n    plagueSouthScania.loc[list(component), 'component'] = i\n</pre> # Create a graph G = nx.Graph()  # Add nodes for index, row in plagueSouthScania.iterrows():     G.add_node(index, polygon=row['geometry'])  # Add edges for i, row_i in plagueSouthScania.iterrows():     for j, row_j in plagueSouthScania.iterrows():         if i != j and row_i['geometry'].touches(row_j['geometry']):             G.add_edge(i, j) # Find connected components connected_components = list(nx.connected_components(G))  # Create new geodataframes for each subset of connected polygons gdfs = [gpd.GeoDataFrame(plagueSouthScania.loc[list(component)], crs=plagueSouthScania.crs) for component in connected_components]  plagueSouthScania = plagueSouthScania.copy()  # Initialize a new column in the original dataframe plagueSouthScania['component'] = -1  # Loop over the list of connected components for i, component in enumerate(connected_components):     # For each component, set the 'component' value of the corresponding rows to the current component number     plagueSouthScania.loc[list(component), 'component'] = i In\u00a0[458]: Copied! <pre>color_list = ['steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen', 'coral'\n                , 'steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen']\nfig, ax = plt.subplots(figsize=(7, 4))\nSkaneMap.plot(ax=ax, color = 'whitesmoke', edgecolor='black',\n              legend=False)\nsouthScaniaMap.plot(ax=ax, color = 'azure',\n                        edgecolor='darkgray', legend=False)\nfor i in range(len(gdfs)):\n    cluster_i = plagueSouthScania[plagueSouthScania['component'] == i]\n    if len(cluster_i) &lt; 3:\n        cluster_i.plot(ax=ax, color = 'peachpuff',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) == 4:\n        cluster_i.plot(ax=ax, color = 'deepskyblue',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) == 9:\n        cluster_i.plot(ax=ax, color = 'salmon',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) &gt; 9:\n        cluster_i.plot(ax=ax, color = 'aquamarine',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> color_list = ['steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen', 'coral'                 , 'steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen'] fig, ax = plt.subplots(figsize=(7, 4)) SkaneMap.plot(ax=ax, color = 'whitesmoke', edgecolor='black',               legend=False) southScaniaMap.plot(ax=ax, color = 'azure',                         edgecolor='darkgray', legend=False) for i in range(len(gdfs)):     cluster_i = plagueSouthScania[plagueSouthScania['component'] == i]     if len(cluster_i) &lt; 3:         cluster_i.plot(ax=ax, color = 'peachpuff',                         edgecolor='black', legend=False)     elif len(cluster_i) == 4:         cluster_i.plot(ax=ax, color = 'deepskyblue',                         edgecolor='black', legend=False)     elif len(cluster_i) == 9:         cluster_i.plot(ax=ax, color = 'salmon',                         edgecolor='black', legend=False)     elif len(cluster_i) &gt; 9:         cluster_i.plot(ax=ax, color = 'aquamarine',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() In\u00a0[459]: Copied! <pre>cluster_0 = plagueSouthScania[plagueSouthScania['component'] == 0]\ncluster_1 = plagueSouthScania[plagueSouthScania['component'] == 1]\ncluster_2 = plagueSouthScania[plagueSouthScania['component'] == 2]\ncluster_3 = plagueSouthScania[plagueSouthScania['component'] == 3]  \ncluster_4 = plagueSouthScania[plagueSouthScania['component'] == 4]\ncluster_5 = plagueSouthScania[plagueSouthScania['component'] == 5]\ncluster_6 = plagueSouthScania[plagueSouthScania['component'] == 6]\ncluster_7 = plagueSouthScania[plagueSouthScania['component'] == 7]\ncluster_8 = plagueSouthScania[plagueSouthScania['component'] == 8]\ncluster_9 = plagueSouthScania[plagueSouthScania['component'] == 9]\ncluster_10 = plagueSouthScania[plagueSouthScania['component'] == 10]\ncluster_11 = plagueSouthScania[plagueSouthScania['component'] == 11]\ncluster_12 = plagueSouthScania[plagueSouthScania['component'] == 12]\ncluster_13 = plagueSouthScania[plagueSouthScania['component'] == 13]\ncluster_14 = plagueSouthScania[plagueSouthScania['component'] == 14]  \nbig_cluster = pd.concat([cluster_6,cluster_2,cluster_13], ignore_index=True) \nsmall_cluster = pd.concat([cluster_11, cluster_12], ignore_index=True)\n</pre> cluster_0 = plagueSouthScania[plagueSouthScania['component'] == 0] cluster_1 = plagueSouthScania[plagueSouthScania['component'] == 1] cluster_2 = plagueSouthScania[plagueSouthScania['component'] == 2] cluster_3 = plagueSouthScania[plagueSouthScania['component'] == 3]   cluster_4 = plagueSouthScania[plagueSouthScania['component'] == 4] cluster_5 = plagueSouthScania[plagueSouthScania['component'] == 5] cluster_6 = plagueSouthScania[plagueSouthScania['component'] == 6] cluster_7 = plagueSouthScania[plagueSouthScania['component'] == 7] cluster_8 = plagueSouthScania[plagueSouthScania['component'] == 8] cluster_9 = plagueSouthScania[plagueSouthScania['component'] == 9] cluster_10 = plagueSouthScania[plagueSouthScania['component'] == 10] cluster_11 = plagueSouthScania[plagueSouthScania['component'] == 11] cluster_12 = plagueSouthScania[plagueSouthScania['component'] == 12] cluster_13 = plagueSouthScania[plagueSouthScania['component'] == 13] cluster_14 = plagueSouthScania[plagueSouthScania['component'] == 14]   big_cluster = pd.concat([cluster_6,cluster_2,cluster_13], ignore_index=True)  small_cluster = pd.concat([cluster_11, cluster_12], ignore_index=True) <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[460]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].values\n\n    def numPatches(self):\n        return len(self.patchNames())\n\n    def patchPop(self, column_pop: str = 'BEF1699'):\n        return self.gdf[column_pop].values\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].values      def numPatches(self):         return len(self.patchNames())      def patchPop(self, column_pop: str = 'BEF1699'):         return self.gdf[column_pop].values      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()              <p>Generating the differential equations</p> In\u00a0[461]: Copied! <pre>SEASONALITY = True\n</pre> SEASONALITY = True In\u00a0[462]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta']\n    p = parameters['p']\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    mu = parameters['mu']\n    N = parameters['N']\n    n = parameters['n']\n    \n    # Create a vector of variables\n    vars = y\n    def entryfun(i, offset): return vars[5 * i + offset]\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n  \n    beta_matrix =  transmission_matrix_beta(gdf)\n    p_matrix = transmission_matrix_p(gdf)\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n        \n    matrix = lambda w : (beta + seasonal_rate(w)) * beta_matrix + (p + seasonal_rate(w))  * p_matrix\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = gamma * (1 - mu) * entry[:, 2]\n    dD = gamma * mu * entry[:, 2]\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n\n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n    output = scipy.odeint(func=model, y0=initConditions, t = t, args=((genInput,),), full_output=1)\n    # output is a tuple with two elements, the first element is the solution\n    # array and the second element is an object with additional information\n    solution = output[0]\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch : int, var : str): \n        indice : int = 5*patch + indexVar[var]\n        return solution[:, indice]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta']     p = parameters['p']     gamma = parameters['gamma']     sigma = parameters['sigma']     mu = parameters['mu']     N = parameters['N']     n = parameters['n']          # Create a vector of variables     vars = y     def entryfun(i, offset): return vars[5 * i + offset]     # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])        beta_matrix =  transmission_matrix_beta(gdf)     p_matrix = transmission_matrix_p(gdf)      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0              matrix = lambda w : (beta + seasonal_rate(w)) * beta_matrix + (p + seasonal_rate(w))  * p_matrix     sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = gamma * (1 - mu) * entry[:, 2]     dD = gamma * mu * entry[:, 2]     derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()      return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]      T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']     output = scipy.odeint(func=model, y0=initConditions, t = t, args=((genInput,),), full_output=1)     # output is a tuple with two elements, the first element is the solution     # array and the second element is an object with additional information     solution = output[0]      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch : int, var : str):          indice : int = 5*patch + indexVar[var]         return solution[:, indice]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Trying a small dataframe</p> In\u00a0[463]: Copied! <pre>cluster = big_cluster\ncluster = cluster.loc[(cluster['BeginPlaguePeriod']!= 'JAN 1711') \n                      &amp; (cluster['BeginPlaguePeriod']!= 'JUN 1711')\n                      &amp; (cluster['BeginPlaguePeriod']!= 'OCT 1711') \n                      &amp; (cluster['BeginPlaguePeriod']!= 'DEC 1711')  \n                      &amp; (cluster['BeginPlaguePeriod']!= 'FEB 1715')]\n</pre> cluster = big_cluster cluster = cluster.loc[(cluster['BeginPlaguePeriod']!= 'JAN 1711')                        &amp; (cluster['BeginPlaguePeriod']!= 'JUN 1711')                       &amp; (cluster['BeginPlaguePeriod']!= 'OCT 1711')                        &amp; (cluster['BeginPlaguePeriod']!= 'DEC 1711')                         &amp; (cluster['BeginPlaguePeriod']!= 'FEB 1715')] <p>Defining the three connected groups that seem to spread the plague between them.</p> In\u00a0[464]: Copied! <pre>group1 = cluster[(cluster['ParishName'] == 'GENARP')\n                 | (cluster['ParishName'] == 'GR\u00d6NBY')\n                 | (cluster['ParishName'] == 'VEBER\u00d6D')\n                 | (cluster['ParishName'] == 'VOMB') \n                 | (cluster['ParishName'] == 'SLIMMINGE')\n                 | (cluster['ParishName'] == 'SKURUP')\n                 | (cluster['ParishName'] == 'B\u00d6RRINGE')\n                 | (cluster['ParishName'] == 'SVEDALA')\n                 | (cluster['ParishName'] == 'SKABERSJ\u00d6')\n                 | (cluster['ParishName'] == 'T\u00d6RRINGE')\n                 | (cluster['ParishName'] == 'S\u00d6DRA \u00c5KARP')\n                 | (cluster['ParishName'] == 'ARRIE')\n]     \ngroup1 = group1.reset_index(drop=True)\n</pre> group1 = cluster[(cluster['ParishName'] == 'GENARP')                  | (cluster['ParishName'] == 'GR\u00d6NBY')                  | (cluster['ParishName'] == 'VEBER\u00d6D')                  | (cluster['ParishName'] == 'VOMB')                   | (cluster['ParishName'] == 'SLIMMINGE')                  | (cluster['ParishName'] == 'SKURUP')                  | (cluster['ParishName'] == 'B\u00d6RRINGE')                  | (cluster['ParishName'] == 'SVEDALA')                  | (cluster['ParishName'] == 'SKABERSJ\u00d6')                  | (cluster['ParishName'] == 'T\u00d6RRINGE')                  | (cluster['ParishName'] == 'S\u00d6DRA \u00c5KARP')                  | (cluster['ParishName'] == 'ARRIE') ]      group1 = group1.reset_index(drop=True) In\u00a0[465]: Copied! <pre>group2 = cluster[(cluster['ParishName'] == 'YSTAD')\n                 | (cluster['ParishName'] == '\u00d6JA')\n                 | (cluster['ParishName'] == 'BROMMA')\n                 | (cluster['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (cluster['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (cluster['ParishName'] == 'VALLEBERGA')\n                 | ((cluster['ParishName'] == 'H\u00d6RUP') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((cluster['ParishName'] == 'GLEMMINGE') &amp; (cluster['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (cluster['ParishName'] == 'INGELSTORP')\n                 | (cluster['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (cluster['ParishName'] == 'HEDESKOGA')\n                 | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup2 = group2.reset_index(drop=True)\n</pre> group2 = cluster[(cluster['ParishName'] == 'YSTAD')                  | (cluster['ParishName'] == '\u00d6JA')                  | (cluster['ParishName'] == 'BROMMA')                  | (cluster['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (cluster['ParishName'] == 'STORA K\u00d6PINGE')                  | (cluster['ParishName'] == 'VALLEBERGA')                  | ((cluster['ParishName'] == 'H\u00d6RUP') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))                  | ((cluster['ParishName'] == 'GLEMMINGE') &amp; (cluster['BeginPlaguePeriod']== 'AUG 1712'))                  | (cluster['ParishName'] == 'INGELSTORP')                  | (cluster['ParishName'] == 'HAMMENH\u00d6G')                  | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))                  | (cluster['ParishName'] == 'HEDESKOGA')                  | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'NOV 1712')) ]      group2 = group2.reset_index(drop=True) In\u00a0[466]: Copied! <pre>group2.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup2.at[1, 'EndPlaguePeriod'] = np.NaN\n</pre> group2.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group2.at[1, 'EndPlaguePeriod'] = np.NaN In\u00a0[467]: Copied! <pre>group3 = cluster[(cluster['ParishName'] == 'S\u00d6DRA \u00c5SUM')\n                 |(cluster['ParishName'] == '\u00d6VED')\n                 | (cluster['ParishName'] == 'S\u00d6VDE')\n                 | (cluster['ParishName'] == 'BRANDSTAD')\n                 | (cluster['ParishName'] == 'BALDRINGE') \n                 | (cluster['ParishName'] == 'RAMS\u00c5SA')\n                 | (cluster['ParishName'] == 'TRYDE') \n                 | (cluster['ParishName'] == 'TRAN\u00c5S')\n                 | (cluster['ParishName'] == 'L\u00d6VESTAD')\n                 | (cluster['ParishName'] == 'VANSTAD')\n                 | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))\n                 | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1713'))\n                 | (cluster['ParishName'] == 'SK\u00c5RBY')\n                 | (cluster['ParishName'] == 'S\u00d6VESTAD')\n                 | (cluster['ParishName'] == 'VILLIE')\n                 | (cluster['ParishName'] == 'BALK\u00c5KRA')\n                 | (cluster['ParishName'] == 'SN\u00c5RESTAD')\n                 | (cluster['ParishName'] == 'V\u00c4STRA N\u00d6BBEL\u00d6V')\n                 | (cluster['ParishName'] == '\u00d6STRA VEMMENH\u00d6G')\n                 | (cluster['ParishName'] == 'SVENSTORP')\n                 | (cluster['ParishName'] == 'LILLA BEDDINGE')\n]     \ngroup3 = group3.reset_index(drop=True)\n</pre> group3 = cluster[(cluster['ParishName'] == 'S\u00d6DRA \u00c5SUM')                  |(cluster['ParishName'] == '\u00d6VED')                  | (cluster['ParishName'] == 'S\u00d6VDE')                  | (cluster['ParishName'] == 'BRANDSTAD')                  | (cluster['ParishName'] == 'BALDRINGE')                   | (cluster['ParishName'] == 'RAMS\u00c5SA')                  | (cluster['ParishName'] == 'TRYDE')                   | (cluster['ParishName'] == 'TRAN\u00c5S')                  | (cluster['ParishName'] == 'L\u00d6VESTAD')                  | (cluster['ParishName'] == 'VANSTAD')                  | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))                  | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1713'))                  | (cluster['ParishName'] == 'SK\u00c5RBY')                  | (cluster['ParishName'] == 'S\u00d6VESTAD')                  | (cluster['ParishName'] == 'VILLIE')                  | (cluster['ParishName'] == 'BALK\u00c5KRA')                  | (cluster['ParishName'] == 'SN\u00c5RESTAD')                  | (cluster['ParishName'] == 'V\u00c4STRA N\u00d6BBEL\u00d6V')                  | (cluster['ParishName'] == '\u00d6STRA VEMMENH\u00d6G')                  | (cluster['ParishName'] == 'SVENSTORP')                  | (cluster['ParishName'] == 'LILLA BEDDINGE') ]      group3 = group3.reset_index(drop=True) In\u00a0[468]: Copied! <pre>cluster_6\n</pre> cluster_6 Out[468]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry color new_format_BeginPlaguePeriod new_format_EndPlaguePeriod component 107 SOUTHWEST BARA BURL\u00d6V BURL\u00d6VS 312 352 332.0 1 5 AUG 1712 NOV 1712 70 POLYGON ((4190984.276 3193599.608, 4191158.103... red 1712-08-01 1712-11-01 6 111 SOUTHWEST BARA G\u00d6RSL\u00d6V G\u00d6RSL\u00d6VS 87 98 92.5 1 5 NOV 1712 NOV 1712 5 POLYGON ((4190984.276 3193599.608, 4190788.786... red 1712-11-01 1712-11-01 6 121 SOUTHWEST BARA TOTTARP TOTTARPS 159 179 169.0 1 4 JUL 1712 SEP 1712 40 POLYGON ((4192980.631 3196551.418, 4193078.773... red 1712-07-01 1712-09-01 6 140 SOUTHWEST OXIE FOSIE FOSIE 226 255 240.5 2 4 AUG 1712 OCT 1712 15 POLYGON ((4186932.526 3187710.935, 4187078.934... red 1712-08-01 1712-10-01 6 143 SOUTHWEST OXIE HUSIE HUSIE 247 279 263.0 2 4 AUG 1712 NOV 1712 5 POLYGON ((4188971.830 3191686.652, 4189143.299... red 1712-08-01 1712-11-01 6 144 SOUTHWEST OXIE LIMHAMN LIMHAMNS 226 255 240.5 1 5 JUL 1712 OCT 1712 2 POLYGON ((4180468.340 3190111.599, 4181258.388... red 1712-07-01 1712-10-01 6 146 SOUTHWEST OXIE LOCKARP LOCKARPS 102 115 108.5 2 5 OCT 1712 NOV 1712 3 POLYGON ((4188237.472 3186601.883, 4187920.789... red 1712-10-01 1712-11-01 6 147 SOUTHWEST OXIE MALM\u00d6 SANKT PETRI MALM\u00d6 SANKT PETRI 5740 1960 3850.0 1 4 JUN 1712 JAN 1713 335 POLYGON ((4184045.037 3190353.566, 4183906.760... red 1712-06-01 1713-01-01 6 159 SOUTHWEST OXIE V\u00c4STRA SKR\u00c4VLINGE V\u00c4STRA SKR\u00c4VLINGE 220 248 234.0 2 4 JUL 1712 OCT 1712 60 POLYGON ((4186932.526 3187710.935, 4186323.382... red 1712-07-01 1712-10-01 6 In\u00a0[469]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ngroup = get_centroid(add_Begin_End_days(sort_by_date(cluster_6), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\nmodel_input = Initial_Model(group)\n</pre> # Getting the centroid of each polygon for defining the transmission matrix group = get_centroid(add_Begin_End_days(sort_by_date(cluster_6), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) model_input = Initial_Model(group) In\u00a0[435]: Copied! <pre># Model_test = {'model': SEIRD_model,\n#               'init': {\n#                   'S': model_input.S0,\n#                   'E': model_input.E0,\n#                   'I': model_input.I0,\n#                   'R': model_input.R0,\n#                   'D': model_input.D0,\n#               },  # defining the initial values for the model\n#               'gdf': group,  # defining the dataframe to work with\n#               'beta': 0.3,\n#               'p': 0.5,\n#               'bump_center': 0.1,\n#               'bump_width': 180.0,\n#               'bump_height': 30.0,\n#               'gamma': 0.32,\n#               'sigma': 0.2,\n#               'mu': 0.4,\n#               'N': model_input.patchPop(),\n#               'n': model_input.n,\n#               'T': model_input.maxDays()}\n\n# model_dict = generate_sol(Model_test)\n</pre> # Model_test = {'model': SEIRD_model, #               'init': { #                   'S': model_input.S0, #                   'E': model_input.E0, #                   'I': model_input.I0, #                   'R': model_input.R0, #                   'D': model_input.D0, #               },  # defining the initial values for the model #               'gdf': group,  # defining the dataframe to work with #               'beta': 0.3, #               'p': 0.5, #               'bump_center': 0.1, #               'bump_width': 180.0, #               'bump_height': 30.0, #               'gamma': 0.32, #               'sigma': 0.2, #               'mu': 0.4, #               'N': model_input.patchPop(), #               'n': model_input.n, #               'T': model_input.maxDays()}  # model_dict = generate_sol(Model_test) In\u00a0[436]: Copied! <pre># %matplotlib inline\n\n# # Set up the data to fit\n# beginTime = group['BeginDaysPlague'].values\n# endTime = group['EndDaysPlague'].values\n# deathData = group['VictimsNumber'].values\n\n# # Number of patches\n# n = Model_test['n']\n\n# # Set the figsize for each subplot\n# figsize_single_subplot = (8, 2)\n\n# # Calculate the total figure height based on the number of subplots and their height\n# fig_height = figsize_single_subplot[1] * n\n\n# # Create a figure and an array of axes with nrows=n and ncols=1\n# fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n#     figsize_single_subplot[0], fig_height), sharex=False)\n\n# # Convert axes to a list if it's not already one\n# if n == 1:\n#     axes = [axes]\n\n# # Plot model solution D for each patch\n# for i in range(n):\n#     if deathData[i] != 0 and endTime[i] != 0:\n#         # initial_position = beginTime[i]\n#         # final_position = endTime[i]\n#         # axes[i].plot(initial_position, 0, 'bo')\n#         # axes[i].plot(final_position,\n#         #              deathData[i], 'bo')\n#         axes[i].plot(model_dict['I'][i], color='darkred', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n#         axes[i].legend(loc='lower right')\n       \n#     else:\n#         axes[i].plot(model_dict['I'][i],\n#                      color='darkred', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n#         axes[i].legend(loc='upper right')\n        \n# # Adjust the layout to avoid overlapping\n# plt.tight_layout()\n# plt.show()\n</pre> # %matplotlib inline  # # Set up the data to fit # beginTime = group['BeginDaysPlague'].values # endTime = group['EndDaysPlague'].values # deathData = group['VictimsNumber'].values  # # Number of patches # n = Model_test['n']  # # Set the figsize for each subplot # figsize_single_subplot = (8, 2)  # # Calculate the total figure height based on the number of subplots and their height # fig_height = figsize_single_subplot[1] * n  # # Create a figure and an array of axes with nrows=n and ncols=1 # fig, axes = plt.subplots(nrows=n, ncols=1, figsize=( #     figsize_single_subplot[0], fig_height), sharex=False)  # # Convert axes to a list if it's not already one # if n == 1: #     axes = [axes]  # # Plot model solution D for each patch # for i in range(n): #     if deathData[i] != 0 and endTime[i] != 0: #         # initial_position = beginTime[i] #         # final_position = endTime[i] #         # axes[i].plot(initial_position, 0, 'bo') #         # axes[i].plot(final_position, #         #              deathData[i], 'bo') #         axes[i].plot(model_dict['I'][i], color='darkred', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths') #         axes[i].legend(loc='lower right')         #     else: #         axes[i].plot(model_dict['I'][i], #                      color='darkred', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths') #         axes[i].legend(loc='upper right')          # # Adjust the layout to avoid overlapping # plt.tight_layout() # plt.show() <p>Defining the optimization problem:</p> In\u00a0[471]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters, beginTime, endTime, deathData):\n    beta, p, bump_center, bump_width, bump_height  = parameters\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': group,\n                  # defining the initial values for the model\n                  'beta': beta,\n                  'p': p,\n                  'gamma': 0.06,\n                  'sigma': 0.05,\n                  'mu': 0.2,\n                  'bump_center': bump_center,\n                  'bump_width': bump_width,\n                  'bump_height': bump_height,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n    model_sol = generate_sol(model_info)\n    totalError = 0\n    n = model_info['n']\n\n    # Calculate the error for each patch\n    errors = np.zeros(n)\n    for i in range(n):\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        if (deathData[i] != 0 and final_position != 0):\n            # errors[i] =  abs(model_sol['D'][i][initial_position] - 1.0) + abs(\n            #         model_sol['D'][i][final_position] - deathData[i])\n            errors[i] =  ((model_sol['D'][i][initial_position] - 1.0)**2 + (\n                    model_sol['D'][i][final_position] - deathData[i])**2)\n        # elif (deathData[i] != 0 and final_position == 0):\n        #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (\n        #         (model_sol['D'][i][initial_position + 30] - deathData[i])**2)\n        # elif (deathData[i] == 0 and final_position != 0):\n        #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (\n        #         (model_sol['D'][i][final_position+1] - 0.0)**2)\n        else:\n            #errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)\n\n    # Calculate the total error\n    totalError = np.sum(errors)\n    return totalError\n</pre> # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters, beginTime, endTime, deathData):     beta, p, bump_center, bump_width, bump_height  = parameters     model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': group,                   # defining the initial values for the model                   'beta': beta,                   'p': p,                   'gamma': 0.06,                   'sigma': 0.05,                   'mu': 0.2,                   'bump_center': bump_center,                   'bump_width': bump_width,                   'bump_height': bump_height,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}     model_sol = generate_sol(model_info)     totalError = 0     n = model_info['n']      # Calculate the error for each patch     errors = np.zeros(n)     for i in range(n):         initial_position = beginTime[i]         final_position = endTime[i]         if (deathData[i] != 0 and final_position != 0):             # errors[i] =  abs(model_sol['D'][i][initial_position] - 1.0) + abs(             #         model_sol['D'][i][final_position] - deathData[i])             errors[i] =  ((model_sol['D'][i][initial_position] - 1.0)**2 + (                     model_sol['D'][i][final_position] - deathData[i])**2)         # elif (deathData[i] != 0 and final_position == 0):         #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (         #         (model_sol['D'][i][initial_position + 30] - deathData[i])**2)         # elif (deathData[i] == 0 and final_position != 0):         #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (         #         (model_sol['D'][i][final_position+1] - 0.0)**2)         else:             #errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)      # Calculate the total error     totalError = np.sum(errors)     return totalError  <p>Parameter estimation</p> In\u00a0[472]: Copied! <pre># Set up the data to fit\nbeginTime = group['BeginDaysPlague'].values\nendTime = group['EndDaysPlague'].values\ndeathData = group['VictimsNumber'].values\n\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = 0.5\np_guess = 0.3\nbump_center_guess = 0.1\nbump_width_guess = 180.0\nbump_height_guess = 30.0\n\n\n# Minimize the objective function to obtain estimates for beta and gamma\nresult = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess ), args=(beginTime, endTime, deathData),\n                           method='L-BFGS-B'\n                           , bounds=[(0, 1), (0, 1),(0,1),(0, 365), (0, 100)]\n                           )\n\nbeta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x\n\nprint(\"beta = \", beta_estimated)\nprint(\"p = \", p_estimated)\n# print(\"gamma = \", gamma_estimated)\n# print(\"sigma = \", sigma_estimated)\n# print(\"mu = \", mu_estimated)\nprint(\"bump_center = \", bump_center_estimated)\nprint(\"bump_width = \", bump_width_estimated)\nprint(\"bump_height = \", bump_height_estimated)\n</pre> # Set up the data to fit beginTime = group['BeginDaysPlague'].values endTime = group['EndDaysPlague'].values deathData = group['VictimsNumber'].values   # Choose initial guesses for the parameters to fit beta_guess = 0.5 p_guess = 0.3 bump_center_guess = 0.1 bump_width_guess = 180.0 bump_height_guess = 30.0   # Minimize the objective function to obtain estimates for beta and gamma result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess ), args=(beginTime, endTime, deathData),                            method='L-BFGS-B'                            , bounds=[(0, 1), (0, 1),(0,1),(0, 365), (0, 100)]                            )  beta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x  print(\"beta = \", beta_estimated) print(\"p = \", p_estimated) # print(\"gamma = \", gamma_estimated) # print(\"sigma = \", sigma_estimated) # print(\"mu = \", mu_estimated) print(\"bump_center = \", bump_center_estimated) print(\"bump_width = \", bump_width_estimated) print(\"bump_height = \", bump_height_estimated) <pre>beta =  0.499999980470431\np =  0.2999999882822586\nbump_center =  0.1000000351532242\nbump_width =  179.99999992269966\nbump_height =  30.00000273413966\n</pre> <p>Results from estimations</p> In\u00a0[473]: Copied! <pre># Set up the data to fit\nbeginTime = group['BeginDaysPlague'].values\nendTime = group['EndDaysPlague'].values\ndeathData = group['VictimsNumber'].values\n\n#Estimated parameters for group1 without seasonality and p = 0 or 1 old objective function\n# beta_estimated =  0.2791417475872005\n# p_estimated =  0.0008691648488012702\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n# #Estimated parameters for group1 without seasonality and p = 0 or 1 new objective function\n# beta_estimated =  0.6203556824484048\n# p_estimated =  0.0009204636123439495\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n#Estimated parameters for group3 without seasonality and with p = 0 or 1 \n# beta_estimated =  0.1053868589337042\n# p_estimated =  0.0055115526615037045\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n#Estimated parameters for cluster_6 without seasonality and with p = gravity\n# beta_estimated =  0.03209853286135653\n# p_estimated =  1.0\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n#Estimated parameters for cluster_6 with seasonality and with p = gravity\nbeta_estimated =  0.499999980470431\np_estimated =  0.2999999882822586\nbump_center_estimated =  0.1000000351532242\nbump_width_estimated =  179.99999992269966\nbump_height_estimated =  30.00000273413966\n#Estimated parameters for cluster_6 without seasonality and with p = 0 or 1 \n# beta_estimated =  0.06702189095421028\n# p_estimated =  0.03050604412998896\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n#Estimated parameters for group3 without seasonality and with p = 0 or 1 \n\n# beta_estimated =  1.8259184082184054e-08\n# p_estimated =  0.6558584860995986\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n#Estimated parameters for group2 without seasonality and p = 0 or 1\n# beta_estimated =  1.0\n# p_estimated =  0.05031279957398098\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n</pre> # Set up the data to fit beginTime = group['BeginDaysPlague'].values endTime = group['EndDaysPlague'].values deathData = group['VictimsNumber'].values  #Estimated parameters for group1 without seasonality and p = 0 or 1 old objective function # beta_estimated =  0.2791417475872005 # p_estimated =  0.0008691648488012702 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  # #Estimated parameters for group1 without seasonality and p = 0 or 1 new objective function # beta_estimated =  0.6203556824484048 # p_estimated =  0.0009204636123439495 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  #Estimated parameters for group3 without seasonality and with p = 0 or 1  # beta_estimated =  0.1053868589337042 # p_estimated =  0.0055115526615037045 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  #Estimated parameters for cluster_6 without seasonality and with p = gravity # beta_estimated =  0.03209853286135653 # p_estimated =  1.0 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  #Estimated parameters for cluster_6 with seasonality and with p = gravity beta_estimated =  0.499999980470431 p_estimated =  0.2999999882822586 bump_center_estimated =  0.1000000351532242 bump_width_estimated =  179.99999992269966 bump_height_estimated =  30.00000273413966 #Estimated parameters for cluster_6 without seasonality and with p = 0 or 1  # beta_estimated =  0.06702189095421028 # p_estimated =  0.03050604412998896 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  #Estimated parameters for group3 without seasonality and with p = 0 or 1   # beta_estimated =  1.8259184082184054e-08 # p_estimated =  0.6558584860995986 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  #Estimated parameters for group2 without seasonality and p = 0 or 1 # beta_estimated =  1.0 # p_estimated =  0.05031279957398098 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0 <p>Substituting the estimated values into the model and solving it</p> In\u00a0[474]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                    'init': {\n                        'S': model_input.S0,\n                        'E': model_input.E0,\n                        'I': model_input.I0,\n                        'R': model_input.R0,\n                        'D': model_input.D0,\n                    },\n                    'gdf': group,\n                    # defining the initial values for the model\n                    'beta': beta_estimated,\n                    'p': p_estimated,\n                    'bump_center': bump_center_estimated,\n                    'bump_width': bump_width_estimated,\n                    'bump_height': bump_height_estimated,\n                    'gamma': 0.06,\n                    'sigma': 0.05,\n                    'mu': 0.2,\n                    'N': model_input.patchPop(),\n                    'n': model_input.n,\n                    'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                     'init': {                         'S': model_input.S0,                         'E': model_input.E0,                         'I': model_input.I0,                         'R': model_input.R0,                         'D': model_input.D0,                     },                     'gdf': group,                     # defining the initial values for the model                     'beta': beta_estimated,                     'p': p_estimated,                     'bump_center': bump_center_estimated,                     'bump_width': bump_width_estimated,                     'bump_height': bump_height_estimated,                     'gamma': 0.06,                     'sigma': 0.05,                     'mu': 0.2,                     'N': model_input.patchPop(),                     'n': model_input.n,                     'T': model_input.maxDays()} model_solution = generate_sol(model_estimation)  <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[475]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n#tick_positions = group['BeginDaysPlague'].values\ntick_positions = group['BeginDaysPlague'].values\ntick_labels = group['BeginPlaguePeriod'].values\n#tick_labels = group['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Dictionary that reduces the plotting to those plots with data\n#lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]\n\n# Plot model solution D for each patch\nfor i in range(n):\n    if deathData[i] != 0 and endTime[i] != 0:\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        axes[i].plot(initial_position, 0, 'bo')\n        axes[i].plot(final_position,\n                     deathData[i], 'bo')\n        axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc = 'lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n    else:\n        axes[i].plot(model_solution['D'][i],\n                     color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.savefig('cumdeathsgravityseas_cluster_6.png'.format(), dpi=300)\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  #tick_positions = group['BeginDaysPlague'].values tick_positions = group['BeginDaysPlague'].values tick_labels = group['BeginPlaguePeriod'].values #tick_labels = group['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Dictionary that reduces the plotting to those plots with data #lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]  # Plot model solution D for each patch for i in range(n):     if deathData[i] != 0 and endTime[i] != 0:         initial_position = beginTime[i]         final_position = endTime[i]         axes[i].plot(initial_position, 0, 'bo')         axes[i].plot(final_position,                      deathData[i], 'bo')         axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc = 'lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)     else:         axes[i].plot(model_solution['D'][i],                      color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.savefig('cumdeathsgravityseas_cluster_6.png'.format(), dpi=300) plt.show() <p>Plotting the daily deaths by parish</p> In\u00a0[476]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[477]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\ntick_positions = group['BeginDaysPlague'].values\ntick_labels = group['BeginPlaguePeriod'].values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.savefig('dailydeathsgravseas_cluster_6.png'.format(), dpi=300)\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  tick_positions = group['BeginDaysPlague'].values tick_labels = group['BeginPlaguePeriod'].values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.savefig('dailydeathsgravseas_cluster_6.png'.format(), dpi=300) plt.show() In\u00a0[478]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\ntick_positions = group['BeginDaysPlague'].values\ntick_labels = group['BeginPlaguePeriod'].values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(model_solution['I'][i],\n                 color='red', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Infected')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.savefig('infectedgravseas_cluster_6.png'.format(), dpi=300)\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  tick_positions = group['BeginDaysPlague'].values tick_labels = group['BeginPlaguePeriod'].values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(model_solution['I'][i],                  color='red', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Infected')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.savefig('infectedgravseas_cluster_6.png'.format(), dpi=300) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"PlagueProject/Estimating_initial_conditions_parameters/","title":"Estimating initial conditions parameters","text":"In\u00a0[10]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[11]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[12]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[13]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[14]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) <p>Set the working directory for private files with detail information for some parishes</p> In\u00a0[15]: Copied! <pre># Southeast Scania\nsoutheast_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southeast')\n# Middle Scania\nmiddle_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Middle')\n# Southwest Scania\nsouthwest_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southwest')\n</pre> # Southeast Scania southeast_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southeast') # Middle Scania middle_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Middle') # Southwest Scania southwest_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southwest') <p>Function to call the data by parish and transform the date to an appropiate format</p> In\u00a0[16]: Copied! <pre>def get_parish_data(parish_name, parish_folder):\n    parish_path = os.path.join(parish_folder, parish_name + '.xlsx')\n    parish = pd.read_excel(parish_path, sheet_name='Plague')\n\n    # Convert 'EndDate' to datetime with appropriate format\n    parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%b %Y')\n    parish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')\n    parish['first_day'] = parish['NewEndDate'].dt.to_timestamp()\n    parish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')\n\n    # Add a column with the days since the first date and then cumsum\n    parish['Days'] = parish['last_day'].dt.daysinmonth\n    parish['Days'] = parish['Days'].cumsum()\n    return parish\n</pre> def get_parish_data(parish_name, parish_folder):     parish_path = os.path.join(parish_folder, parish_name + '.xlsx')     parish = pd.read_excel(parish_path, sheet_name='Plague')      # Convert 'EndDate' to datetime with appropriate format     parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%b %Y')     parish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')     parish['first_day'] = parish['NewEndDate'].dt.to_timestamp()     parish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')      # Add a column with the days since the first date and then cumsum     parish['Days'] = parish['last_day'].dt.daysinmonth     parish['Days'] = parish['Days'].cumsum()     return parish <p>Initializing the population size (pop_parish) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[17]: Copied! <pre>def get_parish_info(parish_name, df: pd.DataFrame, column_name='ParishName', column_pop='BEF1699'):\n    pop_df = df[(df[column_name] == parish_name)][column_pop]\n    name_df = df[(df[column_name] == parish_name)][column_name]\n    \n    if not pop_df.empty and not name_df.empty:\n        pop_parish = pop_df.values[0]\n        name_parish = name_df.values[0]\n    else:\n        pop_parish = None\n        name_parish = None\n\n    return pop_parish, name_parish\n</pre> def get_parish_info(parish_name, df: pd.DataFrame, column_name='ParishName', column_pop='BEF1699'):     pop_df = df[(df[column_name] == parish_name)][column_pop]     name_df = df[(df[column_name] == parish_name)][column_name]          if not pop_df.empty and not name_df.empty:         pop_parish = pop_df.values[0]         name_parish = name_df.values[0]     else:         pop_parish = None         name_parish = None      return pop_parish, name_parish In\u00a0[22]: Copied! <pre>parish_file = get_parish_data('Ystad', southeast_parishes_folder)\nparish = southScania[southScania['ParishName'] == 'YSTAD']\nmerged = pd.merge(parish_file, parish, on='ParishName', how='inner')\n\n\n# # Convert WKT (Well-Known Text) geometry to Shapely geometry\n# merged['geometry'] = merged['geometry'].apply(wkt.loads)\n\n# # Create a GeoDataFrame from the DataFrame\n# merged = gpd.GeoDataFrame(merged, geometry='geometry')  \n# merged\n</pre> parish_file = get_parish_data('Ystad', southeast_parishes_folder) parish = southScania[southScania['ParishName'] == 'YSTAD'] merged = pd.merge(parish_file, parish, on='ParishName', how='inner')   # # Convert WKT (Well-Known Text) geometry to Shapely geometry # merged['geometry'] = merged['geometry'].apply(wkt.loads)  # # Create a GeoDataFrame from the DataFrame # merged = gpd.GeoDataFrame(merged, geometry='geometry')   # merged In\u00a0[23]: Copied! <pre>parish_file\n</pre> parish_file Out[23]: ParishName EndDate Deaths CumDeaths NewEndDate first_day last_day Days 0 YSTAD Jun 1712 26 26 1712-06 1712-06-01 1712-06-30 23:59:59.999999999 30 1 YSTAD Jul 1712 80 106 1712-07 1712-07-01 1712-07-31 23:59:59.999999999 61 2 YSTAD Aug 1712 303 409 1712-08 1712-08-01 1712-08-31 23:59:59.999999999 92 3 YSTAD Sep 1712 202 611 1712-09 1712-09-01 1712-09-30 23:59:59.999999999 122 4 YSTAD Oct 1712 84 695 1712-10 1712-10-01 1712-10-31 23:59:59.999999999 153 5 YSTAD Nov 1712 35 730 1712-11 1712-11-01 1712-11-30 23:59:59.999999999 183 6 YSTAD Dec 1712 5 735 1712-12 1712-12-01 1712-12-31 23:59:59.999999999 214 In\u00a0[24]: Copied! <pre>parish\n</pre> parish Out[24]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry 18 SOUTHEAST HERRESTAD YSTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... In\u00a0[25]: Copied! <pre>merged\n</pre> merged Out[25]: ParishName EndDate Deaths CumDeaths NewEndDate first_day last_day Days Region District(H\u00e4rad) G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry 0 YSTAD Jun 1712 26 26 1712-06 1712-06-01 1712-06-30 23:59:59.999999999 30 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... 1 YSTAD Jul 1712 80 106 1712-07 1712-07-01 1712-07-31 23:59:59.999999999 61 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... 2 YSTAD Aug 1712 303 409 1712-08 1712-08-01 1712-08-31 23:59:59.999999999 92 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... 3 YSTAD Sep 1712 202 611 1712-09 1712-09-01 1712-09-30 23:59:59.999999999 122 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... 4 YSTAD Oct 1712 84 695 1712-10 1712-10-01 1712-10-31 23:59:59.999999999 153 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... 5 YSTAD Nov 1712 35 730 1712-11 1712-11-01 1712-11-30 23:59:59.999999999 183 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... 6 YSTAD Dec 1712 5 735 1712-12 1712-12-01 1712-12-31 23:59:59.999999999 214 SOUTHEAST HERRESTAD YSTADS 1782 1156 1469.0 1 4 JUN 1712 DEC 1712 740 POLYGON ((4240506.247557897 3176029.5812564003... In\u00a0[16]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf, beta_guess:float, mu_guess:float, p_guess:float, df2: pd.DataFrame):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        self.beta =np.full(self.n, beta_guess)# dimension = self.n and value for all the components = beta_guess\n        self.mu = np.full(self.n, mu_guess)\n        self.p = np.full(self.n, p_guess)\n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].values\n\n    def numPatches(self):\n        return len(self.patchNames())\n\n    def patchPop(self, column_pop: str = 'BEF1699'):\n        return self.gdf[column_pop].values\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf, beta_guess:float, mu_guess:float, p_guess:float, df2: pd.DataFrame):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         self.beta =np.full(self.n, beta_guess)# dimension = self.n and value for all the components = beta_guess         self.mu = np.full(self.n, mu_guess)         self.p = np.full(self.n, p_guess)         for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].values      def numPatches(self):         return len(self.patchNames())      def patchPop(self, column_pop: str = 'BEF1699'):         return self.gdf[column_pop].values      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()           <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[16]: Copied! <pre>southeastScania = get_centroid(southeastScania)\n</pre> southeastScania = get_centroid(southeastScania) <p>Generating the differential equations</p> In\u00a0[18]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta']\n    p = parameters['p']\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    mu = parameters['mu']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    beta_matrix =  transmission_matrix_beta(gdf)\n    p_matrix = transmission_matrix_p(gdf)\n    seasonal_rate = lambda t : seasonal_transmission_rate(t, bump_center, bump_width, bump_height)\n    matrix = lambda t : (beta + seasonal_rate(t)) * beta_matrix + (p + seasonal_rate(t))  * p_matrix\n\n    sum_transmission = lambda t : np.sum(matrix(t) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = gamma * (1 - mu) * entry[:, 2]\n    dD = gamma * mu * entry[:, 2]\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n\n    \n    # for i in range(n):\n    #     sum_transmission = sum( matrix(t)[i][j] * entry(j, 2) for j in range(n))\n    #     dS[i] = - entry(i, 0) / N[i] * sum_transmission\n    #     dE[i] = entry(i, 0) / N[i] * sum_transmission - sigma * entry(i, 1)\n    #     dI[i] = sigma * entry(i, 1) - gamma * entry(i, 2)\n    #     dR[i] = gamma * (1 - mu) * entry(i, 2)\n    #     dD[i] = gamma * mu * entry(i, 2)\n    # derivates = [val for i in range(n) for val in (\n    #     dS[i], dE[i], dI[i], dR[i], dD[i])]\n    return derivatives\n\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n\n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta']     p = parameters['p']     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     mu = parameters['mu']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      beta_matrix =  transmission_matrix_beta(gdf)     p_matrix = transmission_matrix_p(gdf)     seasonal_rate = lambda t : seasonal_transmission_rate(t, bump_center, bump_width, bump_height)     matrix = lambda t : (beta + seasonal_rate(t)) * beta_matrix + (p + seasonal_rate(t))  * p_matrix      sum_transmission = lambda t : np.sum(matrix(t) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = gamma * (1 - mu) * entry[:, 2]     dD = gamma * mu * entry[:, 2]     derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()           # for i in range(n):     #     sum_transmission = sum( matrix(t)[i][j] * entry(j, 2) for j in range(n))     #     dS[i] = - entry(i, 0) / N[i] * sum_transmission     #     dE[i] = entry(i, 0) / N[i] * sum_transmission - sigma * entry(i, 1)     #     dI[i] = sigma * entry(i, 1) - gamma * entry(i, 2)     #     dR[i] = gamma * (1 - mu) * entry(i, 2)     #     dD[i] = gamma * mu * entry(i, 2)     # derivates = [val for i in range(n) for val in (     #     dS[i], dE[i], dI[i], dR[i], dD[i])]     return derivatives   def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]      T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']     solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}  <p>Trying a small dataframe</p> In\u00a0[16]: Copied! <pre># k = 6\n# example = southeastScania.head(k)\n# example.shape\n# model_input = Initial_Model(example)\n</pre> # k = 6 # example = southeastScania.head(k) # example.shape # model_input = Initial_Model(example)  In\u00a0[17]: Copied! <pre># Model_test = {'model': SEIRD_model,\n#               'init': {\n#                   'S': model_input.S0,\n#                   'E': model_input.E0,\n#                   'I': model_input.I0,\n#                   'R': model_input.R0,\n#                   'D': model_input.D0,\n#               },  # defining the initial values for the model\n#               'gdf': example,  # defining the graph\n#               'beta': 0.3,\n#               'p': 0.1,\n#               'bump_center': 0.1,\n#               'bump_width': 180.0,\n#               'bump_height': 30.0,\n#               'gamma': 0.06,\n#               'sigma': 0.02,\n#               'mu': 0.2,\n#               'N': model_input.patchPop(),\n#               'n': model_input.n,\n#               'T': model_input.maxDays()}\n\n# model_dict = generate_sol(Model_test)\n</pre> # Model_test = {'model': SEIRD_model, #               'init': { #                   'S': model_input.S0, #                   'E': model_input.E0, #                   'I': model_input.I0, #                   'R': model_input.R0, #                   'D': model_input.D0, #               },  # defining the initial values for the model #               'gdf': example,  # defining the graph #               'beta': 0.3, #               'p': 0.1, #               'bump_center': 0.1, #               'bump_width': 180.0, #               'bump_height': 30.0, #               'gamma': 0.06, #               'sigma': 0.02, #               'mu': 0.2, #               'N': model_input.patchPop(), #               'n': model_input.n, #               'T': model_input.maxDays()}  # model_dict = generate_sol(Model_test) In\u00a0[18]: Copied! <pre># %matplotlib inline\n\n# # Set up the data to fit\n# beginTime = southeastScania['BeginDaysPlague'].values\n# endTime = southeastScania['EndDaysPlague'].values\n# deathData = southeastScania['VictimsNumber'].values\n\n# # Number of patches\n# n = Model_test['n']\n\n# # Set the figsize for each subplot\n# figsize_single_subplot = (8, 2)\n\n# # Calculate the total figure height based on the number of subplots and their height\n# fig_height = figsize_single_subplot[1] * n\n\n# # Create a figure and an array of axes with nrows=n and ncols=1\n# fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n#     figsize_single_subplot[0], fig_height), sharex=False)\n\n# # Plot model solution D for each patch\n# for i in range(n):\n#     if deathData[i] != 0 and endTime[i] != 0:\n#         initial_position = beginTime[i]\n#         final_position = endTime[i]\n#         axes[i].plot(initial_position, 0, 'bo')\n#         axes[i].plot(final_position,\n#                      deathData[i], 'bo')\n#         axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n       \n#     else:\n#         axes[i].plot(model_dict['D'][i],\n#                      color='orange', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n#         axes[i].legend(loc='lower right')\n        \n# # Adjust the layout to avoid overlapping\n# plt.tight_layout()\n# plt.show()\n</pre> # %matplotlib inline  # # Set up the data to fit # beginTime = southeastScania['BeginDaysPlague'].values # endTime = southeastScania['EndDaysPlague'].values # deathData = southeastScania['VictimsNumber'].values  # # Number of patches # n = Model_test['n']  # # Set the figsize for each subplot # figsize_single_subplot = (8, 2)  # # Calculate the total figure height based on the number of subplots and their height # fig_height = figsize_single_subplot[1] * n  # # Create a figure and an array of axes with nrows=n and ncols=1 # fig, axes = plt.subplots(nrows=n, ncols=1, figsize=( #     figsize_single_subplot[0], fig_height), sharex=False)  # # Plot model solution D for each patch # for i in range(n): #     if deathData[i] != 0 and endTime[i] != 0: #         initial_position = beginTime[i] #         final_position = endTime[i] #         axes[i].plot(initial_position, 0, 'bo') #         axes[i].plot(final_position, #                      deathData[i], 'bo') #         axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths')         #     else: #         axes[i].plot(model_dict['D'][i], #                      color='orange', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths') #         axes[i].legend(loc='lower right')          # # Adjust the layout to avoid overlapping # plt.tight_layout() # plt.show() <p>Defining the optimization problem:</p> In\u00a0[19]: Copied! <pre>model_input = Initial_Model(southeastScania)\n</pre>  model_input = Initial_Model(southeastScania) In\u00a0[20]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters, beginTime, endTime, deathData):\n    beta, p, bump_center, bump_width, bump_height = parameters\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': southeastScania,\n                  # defining the initial values for the model\n                  'beta': model_input.beta,\n                  'p': p,\n                  'mu': model_input.mu,\n                  'bump_center': bump_center,\n                  'bump_width': bump_width,\n                  'bump_height': bump_height,\n                  'gamma': 0.06,\n                  'sigma': 0.02,                  \n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n    model_sol = generate_sol(model_info)\n    totalError = 0\n    n = model_info['n']\n\n    # Calculate the error for each patch\n    errors = np.zeros(n)\n    for i in range(n):\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        if (deathData[i] != 0 and final_position != 0):\n            try:\n                errors[i] = 0.7 * ((model_sol['D'][i][initial_position] - 1.0)**2 + (\n                    model_sol['D'][i][final_position] - deathData[i])**2)\n            except:\n                print(\n                    f\"Error at: n={n}, i={i}, final_position={final_position}, len(model_sol['D'])= {len(model_sol['D'])}, model_sol['D'][i] = {model_sol['D'][i]}, deathData[i] = {deathData[i]}\")\n        else:\n            errors[i] = 0.3 * ((model_sol['D'][i][initial_position] - 1.0)**2)\n\n    # Calculate the total error\n    totalError = np.sum(errors)\n    return totalError\n</pre> # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters, beginTime, endTime, deathData):     beta, p, bump_center, bump_width, bump_height = parameters     model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': southeastScania,                   # defining the initial values for the model                   'beta': model_input.beta,                   'p': p,                   'mu': model_input.mu,                   'bump_center': bump_center,                   'bump_width': bump_width,                   'bump_height': bump_height,                   'gamma': 0.06,                   'sigma': 0.02,                                     'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}     model_sol = generate_sol(model_info)     totalError = 0     n = model_info['n']      # Calculate the error for each patch     errors = np.zeros(n)     for i in range(n):         initial_position = beginTime[i]         final_position = endTime[i]         if (deathData[i] != 0 and final_position != 0):             try:                 errors[i] = 0.7 * ((model_sol['D'][i][initial_position] - 1.0)**2 + (                     model_sol['D'][i][final_position] - deathData[i])**2)             except:                 print(                     f\"Error at: n={n}, i={i}, final_position={final_position}, len(model_sol['D'])= {len(model_sol['D'])}, model_sol['D'][i] = {model_sol['D'][i]}, deathData[i] = {deathData[i]}\")         else:             errors[i] = 0.3 * ((model_sol['D'][i][initial_position] - 1.0)**2)      # Calculate the total error     totalError = np.sum(errors)     return totalError  <p>Parameter estimation</p> In\u00a0[21]: Copied! <pre># Set up the data to fit\n# beginTime = southeastScania['BeginDaysPlague'].values\n# endTime = southeastScania['EndDaysPlague'].values\n# deathData = southeastScania['VictimsNumber'].values\n\n\n# # Choose initial guesses for the parameters to fit\n# beta_guess = 0.3\n# p_guess = 0.1\n# bump_center_guess = 0.1\n# bump_width_guess = 180.0\n# bump_height_guess = 30.0\n\n\n# # Minimize the objective function to obtain estimates for beta and gamma\n# result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess), args=(beginTime, endTime, deathData),\n#                            method='L-BFGS-B'\n#                            # ,bounds=[(0, 1), (0, 1), (0, 10), (-2, 2), (-10, 10)]\n#                            )\n# beta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x\n\n# print(\"beta = \", beta_estimated)\n# print(\"p = \", p_estimated)\n# print(\"bump_center = \", bump_center_estimated)\n# print(\"bump_width = \", bump_width_estimated)\n# print(\"bump_height = \", bump_height_estimated)\n</pre> # Set up the data to fit # beginTime = southeastScania['BeginDaysPlague'].values # endTime = southeastScania['EndDaysPlague'].values # deathData = southeastScania['VictimsNumber'].values   # # Choose initial guesses for the parameters to fit # beta_guess = 0.3 # p_guess = 0.1 # bump_center_guess = 0.1 # bump_width_guess = 180.0 # bump_height_guess = 30.0   # # Minimize the objective function to obtain estimates for beta and gamma # result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess), args=(beginTime, endTime, deathData), #                            method='L-BFGS-B' #                            # ,bounds=[(0, 1), (0, 1), (0, 10), (-2, 2), (-10, 10)] #                            ) # beta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x  # print(\"beta = \", beta_estimated) # print(\"p = \", p_estimated) # print(\"bump_center = \", bump_center_estimated) # print(\"bump_width = \", bump_width_estimated) # print(\"bump_height = \", bump_height_estimated) <p>Results from estimations</p> In\u00a0[21]: Copied! <pre># # Set up the data to fit\nbeginTime = southeastScania['BeginDaysPlague'].values\nendTime = southeastScania['EndDaysPlague'].values\ndeathData = southeastScania['VictimsNumber'].values\n\n# #result estimation considering seasonality in p and beta, considering (p+seasonality) and (beta+seasonality)\nbeta_estimated =  0.29999990413218025\np_estimated =  0.10000015278925331\nbump_center_estimated =  0.1000005503832051\nbump_width_estimated =  180.00000012694866\nbump_height_estimated =  30.00000035610224\n\n# #result estimation considering seasonality only (beta + seasonality)\n# beta_estimated =  0.3574870151324586\n# p_estimated =  0.07419604016603554\n# bump_center_estimated =  0.10388586284456384\n# bump_width_estimated =  180.00730607111754\n# bump_height_estimated =  30.01454878377533\n\n# #result estimation without considering seasonality\n# beta_estimated =  0.6159244597903136\n# p_estimated =  -0.062116989387247884\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n</pre> # # Set up the data to fit beginTime = southeastScania['BeginDaysPlague'].values endTime = southeastScania['EndDaysPlague'].values deathData = southeastScania['VictimsNumber'].values  # #result estimation considering seasonality in p and beta, considering (p+seasonality) and (beta+seasonality) beta_estimated =  0.29999990413218025 p_estimated =  0.10000015278925331 bump_center_estimated =  0.1000005503832051 bump_width_estimated =  180.00000012694866 bump_height_estimated =  30.00000035610224  # #result estimation considering seasonality only (beta + seasonality) # beta_estimated =  0.3574870151324586 # p_estimated =  0.07419604016603554 # bump_center_estimated =  0.10388586284456384 # bump_width_estimated =  180.00730607111754 # bump_height_estimated =  30.01454878377533  # #result estimation without considering seasonality # beta_estimated =  0.6159244597903136 # p_estimated =  -0.062116989387247884 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  <p>Substituting the estimated values into the model and solving it</p> In\u00a0[22]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                    'init': {\n                        'S': model_input.S0,\n                        'E': model_input.E0,\n                        'I': model_input.I0,\n                        'R': model_input.R0,\n                        'D': model_input.D0,\n                    },\n                    'gdf': southeastScania,\n                    # defining the initial values for the model\n                    'beta': beta_estimated,\n                    'p': p_estimated,\n                    'bump_center': bump_center_estimated,\n                    'bump_width': bump_width_estimated,\n                    'bump_height': bump_height_estimated,\n                    'gamma': 0.06,\n                    'sigma': 0.02,\n                    'mu': 0.2,\n                    'N': model_input.patchPop(),\n                    'n': model_input.n,\n                    'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                     'init': {                         'S': model_input.S0,                         'E': model_input.E0,                         'I': model_input.I0,                         'R': model_input.R0,                         'D': model_input.D0,                     },                     'gdf': southeastScania,                     # defining the initial values for the model                     'beta': beta_estimated,                     'p': p_estimated,                     'bump_center': bump_center_estimated,                     'bump_width': bump_width_estimated,                     'bump_height': bump_height_estimated,                     'gamma': 0.06,                     'sigma': 0.02,                     'mu': 0.2,                     'N': model_input.patchPop(),                     'n': model_input.n,                     'T': model_input.maxDays()} model_solution = generate_sol(model_estimation)  <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[19]: Copied! <pre>southeastScania['BeginPlaguePeriod'][1]\n</pre> southeastScania['BeginPlaguePeriod'][1]  Out[19]: <pre>'1711-10-01 00:00:00'</pre> In\u00a0[24]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ntick_positions = southeastScania['BeginDaysPlague'].values\ntick_labels = southeastScania['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Dictionary that reduces the plotting to those plots with data\n#lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]\n\n# Plot model solution D for each patch\nfor i in range(n):\n    if deathData[i] != 0 and endTime[i] != 0:\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        axes[i].plot(initial_position, 0, 'bo')\n        axes[i].plot(final_position,\n                     deathData[i], 'bo')\n        axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc = 'lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n    else:\n        axes[i].plot(model_solution['D'][i],\n                     color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  tick_positions = southeastScania['BeginDaysPlague'].values tick_labels = southeastScania['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Dictionary that reduces the plotting to those plots with data #lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]  # Plot model solution D for each patch for i in range(n):     if deathData[i] != 0 and endTime[i] != 0:         initial_position = beginTime[i]         final_position = endTime[i]         axes[i].plot(initial_position, 0, 'bo')         axes[i].plot(final_position,                      deathData[i], 'bo')         axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc = 'lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)     else:         axes[i].plot(model_solution['D'][i],                      color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Plotting the daily deaths by parish</p> In\u00a0[25]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[28]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\n# tick_positions = southeastScania['BeginDaysPlague'].values\n# tick_labels = southeastScania['BeginPlaguePeriod'].apply(\n#     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  # tick_positions = southeastScania['BeginDaysPlague'].values # tick_labels = southeastScania['BeginPlaguePeriod'].apply( #     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()"},{"location":"PlagueProject/FittingModelObFunct2/","title":"FittingModelObFunct2","text":"In\u00a0[63]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[64]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[65]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[66]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[67]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[68]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[69]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n\ntype(southScania)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]]  type(southScania) Out[69]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[70]: Copied! <pre>southScania = get_centroid(southScania)\nsouthScania = southScania.replace(['UNDEFINED', '?'], np.nan)\nsouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    southScania['BeginPlaguePeriod'], format='%b %Y')\nsouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    southScania['EndPlaguePeriod'], format='%b %Y')\n\nlen(southScania)\n</pre> southScania = get_centroid(southScania) southScania = southScania.replace(['UNDEFINED', '?'], np.nan) southScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     southScania['BeginPlaguePeriod'], format='%b %Y') southScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     southScania['EndPlaguePeriod'], format='%b %Y')  len(southScania) Out[70]: <pre>235</pre> <p>Defining a group to work with</p> In\u00a0[71]: Copied! <pre>group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n                 | (southScania['ParishName'] == '\u00d6JA')\n                 | (southScania['ParishName'] == 'BROMMA')\n                 | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (southScania['ParishName'] == 'VALLEBERGA')\n                 | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (southScania['ParishName'] == 'INGELSTORP')\n                 | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (southScania['ParishName'] == 'HEDESKOGA')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> group1 = southScania[(southScania['ParishName'] == 'YSTAD')                  | (southScania['ParishName'] == '\u00d6JA')                  | (southScania['ParishName'] == 'BROMMA')                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (southScania['ParishName'] == 'STORA K\u00d6PINGE')                  | (southScania['ParishName'] == 'VALLEBERGA')                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))                  | (southScania['ParishName'] == 'INGELSTORP')                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))                  | (southScania['ParishName'] == 'HEDESKOGA')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) ]      group1 = group1.reset_index(drop=True) group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED' <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[72]: Copied! <pre>group = group1\nlen(group)\n</pre> group = group1 len(group) Out[72]: <pre>13</pre> In\u00a0[73]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\ngroup = group.replace(['UNDEFINED', '?'], np.nan)\ngroup['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    group['BeginPlaguePeriod'], format='%b %Y')\ngroup['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    group['EndPlaguePeriod'], format='%b %Y')\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe group = group.replace(['UNDEFINED', '?'], np.nan) group['new_format_BeginPlaguePeriod'] = pd.to_datetime(     group['BeginPlaguePeriod'], format='%b %Y') group['new_format_EndPlaguePeriod'] = pd.to_datetime(     group['EndPlaguePeriod'], format='%b %Y') In\u00a0[120]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n# # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0\n# cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']\ncluster1\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) # # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0 # cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699'] cluster1 Out[120]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry centroid new_format_BeginPlaguePeriod new_format_EndPlaguePeriod BeginDaysPlague EndDaysPlague 0 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740.0 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 0 213 1 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40.0 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 0 303 2 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 NaN NaN POLYGON ((4228840.232 3178726.042, 4228969.528... POINT (4230600.699862956 3177291.2599278623) 1712-07-01 NaT 30 0 3 SOUTHEAST H\u00d6RUP 289 JUL 1712 NaN 60.0 POLYGON ((4252154.708 3177178.545, 4251691.166... POINT (4252336.724268622 3179160.5725134797) 1712-07-01 NaT 30 0 4 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80.0 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 30 244 5 SOUTHEAST VALLEBERGA 391 JUL 1712 NaN NaN POLYGON ((4249981.772 3179064.746, 4250047.408... POINT (4250097.829593883 3174762.056364127) 1712-07-01 NaT 30 0 6 SOUTHEAST BROMMA 154 AUG 1712 NaN NaN POLYGON ((4231996.049 3179728.504, 4232042.002... POINT (4234027.027249504 3179705.7627846766) 1712-08-01 NaT 61 0 7 SOUTHEAST GLEMMINGE 362 AUG 1712 AUG 1712 NaN POLYGON ((4246778.053 3180955.654, 4246924.466... POINT (4247370.639288611 3179086.3361054286) 1712-08-01 1712-08-01 61 91 8 SOUTHEAST HAMMENH\u00d6G 169 AUG 1712 NaN NaN POLYGON ((4255090.885 3182137.710, 4254726.251... POINT (4253884.1828368595 3183416.9898560927) 1712-08-01 NaT 61 0 9 SOUTHEAST INGELSTORP 358 AUG 1712 NaN 1.0 POLYGON ((4246833.197 3177120.892, 4246902.701... POINT (4247499.939231565 3174682.3176888516) 1712-08-01 NaT 61 0 10 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5.0 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-01 92 152 11 SOUTHEAST \u00d6VRABY 208 SEP 1712 SEP 1712 NaN POLYGON ((4242987.654 3183129.802, 4242831.234... POINT (4242196.192299193 3183903.5739921844) 1712-09-01 1712-09-01 92 121 12 SOUTHEAST \u00d6VRABY 208 NOV 1712 NOV 1712 NaN POLYGON ((4242987.654 3183129.802, 4242831.234... POINT (4242196.192299193 3183903.5739921844) 1712-11-01 1712-11-01 153 182 In\u00a0[126]: Copied! <pre># define a matrix of transmission. 1 means that the parish i is connected with the parish j\n# 0 means that the parish i is not connected with the parish j\n# The matrix is defined by the order of the parishes in the GeoDataFrame\n# The matrix is a symmetric matrix\n# consider only the parishes with different names\ndef connection_matrix(cluster, column_name='ParishName'):\n    # consider only the parishes with different names\n    cluster = cluster.drop_duplicates(subset=column_name)\n    matrix = np.zeros((len(cluster), len(cluster)))\n    for i in range(len(cluster)):\n        for j in range(len(cluster)):\n            if i != j:\n                if cluster.iloc[i].geometry.touches(cluster.iloc[j].geometry):\n                    matrix[i, j] = 1\n            else:\n                matrix[i, j] = 0\n\n    # Divide each row by the total number of non-zero elements in the same row\n    for i in range(len(matrix)):\n        non_zero_count = np.count_nonzero(matrix[i])\n        if non_zero_count &gt; 0:  # Avoid division by zero\n            matrix[i] /= non_zero_count\n\n    return matrix\n\n\nconnection_Yellow_group = connection_matrix(cluster1)\nconnection_Yellow_group\n</pre> # define a matrix of transmission. 1 means that the parish i is connected with the parish j # 0 means that the parish i is not connected with the parish j # The matrix is defined by the order of the parishes in the GeoDataFrame # The matrix is a symmetric matrix # consider only the parishes with different names def connection_matrix(cluster, column_name='ParishName'):     # consider only the parishes with different names     cluster = cluster.drop_duplicates(subset=column_name)     matrix = np.zeros((len(cluster), len(cluster)))     for i in range(len(cluster)):         for j in range(len(cluster)):             if i != j:                 if cluster.iloc[i].geometry.touches(cluster.iloc[j].geometry):                     matrix[i, j] = 1             else:                 matrix[i, j] = 0      # Divide each row by the total number of non-zero elements in the same row     for i in range(len(matrix)):         non_zero_count = np.count_nonzero(matrix[i])         if non_zero_count &gt; 0:  # Avoid division by zero             matrix[i] /= non_zero_count      return matrix   connection_Yellow_group = connection_matrix(cluster1) connection_Yellow_group  Out[126]: <pre>array([[0.        , 0.33333333, 0.        , 0.        , 0.33333333,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.33333333, 0.        ],\n       [0.33333333, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.33333333, 0.        , 0.        , 0.        ,\n        0.33333333, 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.5       , 0.        , 0.        , 0.        ,\n        0.5       , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.33333333, 0.        , 0.33333333, 0.33333333, 0.        ,\n        0.        , 0.        ],\n       [0.25      , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.25      , 0.        , 0.25      ,\n        0.        , 0.25      ],\n       [0.        , 0.        , 0.        , 0.33333333, 0.        ,\n        0.        , 0.        , 0.33333333, 0.        , 0.33333333,\n        0.        , 0.        ],\n       [0.        , 0.33333333, 0.33333333, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.33333333, 0.        ],\n       [0.        , 0.        , 0.        , 0.25      , 0.25      ,\n        0.25      , 0.        , 0.        , 0.        , 0.25      ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.33333333,\n        0.33333333, 0.        , 0.33333333, 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.25      , 0.25      , 0.25      , 0.        , 0.        ,\n        0.        , 0.25      , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 1.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ]])</pre> In\u00a0[135]: Copied! <pre>import pandas as pd\n\ndef connection_matrix(cluster, column_name='ParishName'):\n    # consider only the parishes with different names\n    cluster = cluster.drop_duplicates(subset=column_name)\n    matrix = np.zeros((len(cluster), len(cluster)))\n    for i in range(len(cluster)):\n        for j in range(len(cluster)):\n            if i != j:\n                if cluster.iloc[i].geometry.touches(cluster.iloc[j].geometry):\n                    matrix[i, j] = 1\n            else:\n                matrix[i, j] = 0\n\n    # Divide each row by the total number of non-zero elements in the same row\n    for i in range(len(matrix)):\n        non_zero_count = np.count_nonzero(matrix[i])\n        if non_zero_count &gt; 0:  # Avoid division by zero\n            matrix[i] /= non_zero_count\n\n    # Convert the matrix to a DataFrame and add column labels\n    df = pd.DataFrame(matrix, columns=cluster[column_name].values)\n\n    return df\nconnection_Yellow_group = connection_matrix(cluster1)\n</pre> import pandas as pd  def connection_matrix(cluster, column_name='ParishName'):     # consider only the parishes with different names     cluster = cluster.drop_duplicates(subset=column_name)     matrix = np.zeros((len(cluster), len(cluster)))     for i in range(len(cluster)):         for j in range(len(cluster)):             if i != j:                 if cluster.iloc[i].geometry.touches(cluster.iloc[j].geometry):                     matrix[i, j] = 1             else:                 matrix[i, j] = 0      # Divide each row by the total number of non-zero elements in the same row     for i in range(len(matrix)):         non_zero_count = np.count_nonzero(matrix[i])         if non_zero_count &gt; 0:  # Avoid division by zero             matrix[i] /= non_zero_count      # Convert the matrix to a DataFrame and add column labels     df = pd.DataFrame(matrix, columns=cluster[column_name].values)      return df connection_Yellow_group = connection_matrix(cluster1)  In\u00a0[138]: Copied! <pre># export the matrix to a csv file with numerical values to use in R\nconnection_Yellow_group.to_csv('connection_Yellow_group.csv', index=False)\n</pre> # export the matrix to a csv file with numerical values to use in R connection_Yellow_group.to_csv('connection_Yellow_group.csv', index=False) <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[75]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf, beta_guess:float, mu_guess:float):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        self.mu = np.full(self.n, mu_guess)\n        self.beta = np.full(self.n, beta_guess)\n\n        self.S0 = np.zeros(self.n)       \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].unique()\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()  \n\n    def p_coeff(self, p_guess:float):\n        p_coeff = np.full((self.n, self.n), p_guess)\n        np.fill_diagonal(p_coeff, 0)\n        return p_coeff\n</pre> class Initial_Model:     def __init__(self, gdf, beta_guess:float, mu_guess:float):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         self.mu = np.full(self.n, mu_guess)         self.beta = np.full(self.n, beta_guess)          self.S0 = np.zeros(self.n)                for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].unique()      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()        def p_coeff(self, p_guess:float):         p_coeff = np.full((self.n, self.n), p_guess)         np.fill_diagonal(p_coeff, 0)         return p_coeff       <p>Generating the differential equations</p> In\u00a0[76]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[77]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # ensure beta is a numpy array of shape n\n    mu = parameters['mu'] # ensure mu is a numpy array of shape n\n    p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    # Ensure p_coeff is symmetric\n    #p_coeff = np.triu(p_coeff,1) + np.tril(p_coeff, -1)\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n\n    beta_matrix =  transmission_matrix_beta(gdf, beta)\n    p_matrix = transmission_matrix2_p(gdf, p_coeff)\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + (beta_matrix + p_matrix) \n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # ensure beta is a numpy array of shape n     mu = parameters['mu'] # ensure mu is a numpy array of shape n     p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      # Ensure p_coeff is symmetric     #p_coeff = np.triu(p_coeff,1) + np.tril(p_coeff, -1)      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])       beta_matrix =  transmission_matrix_beta(gdf, beta)     p_matrix = transmission_matrix2_p(gdf, p_coeff)      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + (beta_matrix + p_matrix)       sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)       dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]      derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Trying a small dataframe</p> In\u00a0[78]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\nexample1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n</pre> # Getting the centroid of each polygon for defining the transmission matrix example1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         )  In\u00a0[79]: Copied! <pre>k = 3\n# Selecting specific rows from the dataframe reseting the index\nexample = southScania[(southScania['ParishName'] == 'YSTAD')\n                   | (southScania['ParishName'] == '\u00d6JA')\n                   | (southScania['ParishName'] == 'HEDESKOGA')].reset_index(drop=True)\n# Getting the centroid of each polygon for defining the transmission matrix\nexample = get_centroid(add_Begin_End_days(sort_by_date(example)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\nmodel_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5)\n</pre> k = 3 # Selecting specific rows from the dataframe reseting the index example = southScania[(southScania['ParishName'] == 'YSTAD')                    | (southScania['ParishName'] == '\u00d6JA')                    | (southScania['ParishName'] == 'HEDESKOGA')].reset_index(drop=True) # Getting the centroid of each polygon for defining the transmission matrix example = get_centroid(add_Begin_End_days(sort_by_date(example)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5) In\u00a0[80]: Copied! <pre># # Selecting two parishes to test the model\n# #example = cluster1[cluster1['ParishName']=='\u00d6JA']\n# example = cluster1[cluster1['ParishName'].isin(['YSTAD','\u00d6JA'])]\n# example = cluster1\n# model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5)\n</pre> # # Selecting two parishes to test the model # #example = cluster1[cluster1['ParishName']=='\u00d6JA'] # example = cluster1[cluster1['ParishName'].isin(['YSTAD','\u00d6JA'])] # example = cluster1 # model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5) In\u00a0[81]: Copied! <pre>Model_test = {'model': SEIRD_model,\n              'init': {\n                  'S': model_input.S0,\n                  'E': model_input.E0,\n                  'I': model_input.I0,\n                  'R': model_input.R0,\n                  'D': model_input.D0,\n              },  # defining the initial values for the model\n              'gdf': example,  # defining the graph\n              'beta': model_input.beta,\n              'p_coeff': model_input.p_coeff(p_guess=0.5),\n              'bump_center': 0.0,\n              'bump_width': 0.0,\n              'bump_height': 0.0,\n              'gamma': 0.4,\n              'sigma': 0.17,\n              'mu': model_input.mu,\n              'N': model_input.patchPop(),\n              'n': model_input.n,\n              'T': model_input.maxDays()}\n\nmodel_dict = generate_sol(Model_test)\nmodel_dict['D'][0][120]\n</pre> Model_test = {'model': SEIRD_model,               'init': {                   'S': model_input.S0,                   'E': model_input.E0,                   'I': model_input.I0,                   'R': model_input.R0,                   'D': model_input.D0,               },  # defining the initial values for the model               'gdf': example,  # defining the graph               'beta': model_input.beta,               'p_coeff': model_input.p_coeff(p_guess=0.5),               'bump_center': 0.0,               'bump_width': 0.0,               'bump_height': 0.0,               'gamma': 0.4,               'sigma': 0.17,               'mu': model_input.mu,               'N': model_input.patchPop(),               'n': model_input.n,               'T': model_input.maxDays()}  model_dict = generate_sol(Model_test) model_dict['D'][0][120] Out[81]: <pre>67.2106847671744</pre> In\u00a0[82]: Copied! <pre>%matplotlib inline\n\n# Set up the data to fit\nbeginTime = example['BeginDaysPlague'].values\nendTime = example['EndDaysPlague'].values\ndeathData = example['VictimsNumber'].values\n\n# Number of patches\nn = Model_test['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ngato = count_infected_parishes_by_month(example,'JUN 1712',0 )\ntick_positions = gato['DaysFromInitialDate'].values\ntick_labels = gato['date'].apply(\n     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Plot model solution D for each patch\nfor i in range(n):\n    axes[i].plot(model_dict['D'][i],\n                color='orange', label=(model_input.patchNames()[i]))\n    axes[i].axhline(y=1, color='blue', linestyle='--')\n    axes[i].set_ylabel('Cumulative Deaths')\n    axes[i].legend(loc='lower right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n    \n    # if deathData[i] != 0 and endTime[i] != 0:\n    #     initial_position = beginTime[i]\n    #     final_position = endTime[i]\n    #     axes[i].plot(initial_position, 0, 'bo')\n    #     axes[i].plot(final_position,\n    #                  deathData[i], 'bo')\n    #     axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))\n    #     axes[i].set_ylabel('Cumulative Deaths')\n       \n    # else:\n    #     axes[i].plot(model_dict['D'][i],\n    #                  color='orange', label=(model_input.patchNames()[i]))\n    #     axes[i].set_ylabel('Cumulative Deaths')\n    #     axes[i].legend(loc='lower right')\n        \n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Set up the data to fit beginTime = example['BeginDaysPlague'].values endTime = example['EndDaysPlague'].values deathData = example['VictimsNumber'].values  # Number of patches n = Model_test['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  gato = count_infected_parishes_by_month(example,'JUN 1712',0 ) tick_positions = gato['DaysFromInitialDate'].values tick_labels = gato['date'].apply(      lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Plot model solution D for each patch for i in range(n):     axes[i].plot(model_dict['D'][i],                 color='orange', label=(model_input.patchNames()[i]))     axes[i].axhline(y=1, color='blue', linestyle='--')     axes[i].set_ylabel('Cumulative Deaths')     axes[i].legend(loc='lower right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)          # if deathData[i] != 0 and endTime[i] != 0:     #     initial_position = beginTime[i]     #     final_position = endTime[i]     #     axes[i].plot(initial_position, 0, 'bo')     #     axes[i].plot(final_position,     #                  deathData[i], 'bo')     #     axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))     #     axes[i].set_ylabel('Cumulative Deaths')             # else:     #     axes[i].plot(model_dict['D'][i],     #                  color='orange', label=(model_input.patchNames()[i]))     #     axes[i].set_ylabel('Cumulative Deaths')     #     axes[i].legend(loc='lower right')          # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Defining the objective functions</p> In\u00a0[111]: Copied! <pre># Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n\ndef objectiveFunction_2 (model_sol: dict\n                         , gdf: gpd.GeoDataFrame = example\n                         , column_name: str = 'ParishName'\n                         , n: int = 0\n                         ):\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Initializing the number of infected parishes per month for the model's output\n    infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_infected_parishes_by_month))\n    # Computing the total number of parishes in the dataframe without repetitions\n    total_parishes = len(grouped_by_parish)\n\n    # Computing the number of infected parishes per month from the model's output\n    for i in range(len(cum_infected_parishes_by_month)):\n        init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n        final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n        \n        for k in range(len(grouped_by_parish)):\n            for day in range(init_days, final_days):\n                if model_sol['I'][k][day] &gt;= 1:\n                    infected_parishes[i] += 1\n                    break # Breaks the innermost loop when the condition is met\n        error[i] = (infected_parishes[i] \n                           - cum_infected_parishes_by_month['CumInfectParishes'][i])**2\n    \n    # Computing the error between the model's output and the data\n    total_error = 1/len(cum_infected_parishes_by_month)*(1/total_parishes) * (np.sum(error))\n          \n    return (total_error)\n</pre> # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data  def objectiveFunction_2 (model_sol: dict                          , gdf: gpd.GeoDataFrame = example                          , column_name: str = 'ParishName'                          , n: int = 0                          ):     #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Initializing the number of infected parishes per month for the model's output     infected_parishes = np.zeros(len(cum_infected_parishes_by_month))     # Initializing the error between the model's output and the data     error = np.zeros(len(cum_infected_parishes_by_month))     # Computing the total number of parishes in the dataframe without repetitions     total_parishes = len(grouped_by_parish)      # Computing the number of infected parishes per month from the model's output     for i in range(len(cum_infected_parishes_by_month)):         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']                  for k in range(len(grouped_by_parish)):             for day in range(init_days, final_days):                 if model_sol['I'][k][day] &gt;= 1:                     infected_parishes[i] += 1                     break # Breaks the innermost loop when the condition is met         error[i] = (infected_parishes[i]                             - cum_infected_parishes_by_month['CumInfectParishes'][i])**2          # Computing the error between the model's output and the data     total_error = 1/len(cum_infected_parishes_by_month)*(1/total_parishes) * (np.sum(error))                return (total_error)  <p>Defining the optimization problem:</p> In\u00a0[112]: Copied! <pre>def objectiveFunction(parameters\n                      , gdf: gpd.GeoDataFrame = example\n                      , column_name: str = 'ParishName'\n                      , n: int = 0):\n    parameters = np.array(parameters)\n\n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n].reshape(n,)\n    mu:  np.array = parameters[n:2*n].reshape(n,)\n    p_coeff: np.array = parameters[2*n:].reshape(n, n)\n\n    # Penalize if p_coeff is not symmetric or has non-zero diagonal elements\n    if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):\n        return 1e50\n\n    # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)\n    # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p_coeff': p_coeff,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    return objectiveFunction_2(model_sol, gdf, column_name, n)\n</pre> def objectiveFunction(parameters                       , gdf: gpd.GeoDataFrame = example                       , column_name: str = 'ParishName'                       , n: int = 0):     parameters = np.array(parameters)      n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n].reshape(n,)     mu:  np.array = parameters[n:2*n].reshape(n,)     p_coeff: np.array = parameters[2*n:].reshape(n, n)      # Penalize if p_coeff is not symmetric or has non-zero diagonal elements     if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):         return 1e50      # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p_coeff': p_coeff,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      return objectiveFunction_2(model_sol, gdf, column_name, n) In\u00a0[113]: Copied! <pre># Set up the data to fit\nn = model_input.n\n\n# # Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p_coeff(p_guess=0.3)\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]*len(beta_guess.flatten())\nmu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p\n\n# Concatenate the bounds\nbounds = beta_bounds + mu_bounds + p_bounds\n\n# Minimize the objective function to obtain beta, mu, and p\nresult = optimize.minimize(objectiveFunction, x0=initial_parameters\n                           , args=(example, 'ParishName', 0)\n                           , bounds=bounds\n                           )\n\nbeta_estimated = result.x[:n].reshape(n,)\nmu_estimated = result.x[n:2*n].reshape(n,)\np_estimated = result.x[2*n:].reshape(n, n)\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre>  # Set up the data to fit n = model_input.n  # # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p_coeff(p_guess=0.3) initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)  # Define the bounds for beta, mu and p beta_bounds = [(0,1)]*len(beta_guess.flatten()) mu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p  # Concatenate the bounds bounds = beta_bounds + mu_bounds + p_bounds  # Minimize the objective function to obtain beta, mu, and p result = optimize.minimize(objectiveFunction, x0=initial_parameters                            , args=(example, 'ParishName', 0)                            , bounds=bounds                            )  beta_estimated = result.x[:n].reshape(n,) mu_estimated = result.x[n:2*n].reshape(n,) p_estimated = result.x[2*n:].reshape(n, n)  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  0.4666666666666667\nbeta =  [0.5 0.5 0.5]\nmu =  [0.5 0.5 0.5]\np =  [[0.  0.3 0.3]\n [0.3 0.  0.3]\n [0.3 0.3 0. ]]\n</pre> <p>Substituting the estimated values into the model and solving it</p> In\u00a0[116]: Copied! <pre># Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n\ndef plot_infected_parishes (beta_estimated\n                            , mu_estimated\n                            , p_estimated\n                            , gdf: gpd.GeoDataFrame = example\n                            , column_name: str = 'ParishName'\n                            , n: int = 0\n                            ):\n    model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n    \n    model_solution = generate_sol(model_estimation)\n        \n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Initializing the number of infected parishes per month for the model's output\n    infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n    days = np.zeros(len(cum_infected_parishes_by_month))\n    \n    # Computing the number of infected parishes per month from the model's output\n    for i in range(len(cum_infected_parishes_by_month)):\n        init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n        final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n        \n        for k in range(len(grouped_by_parish)):\n            for day in range(init_days, final_days):\n                if model_solution['I'][k][day] &gt;= 1:\n                    days[i] = day\n                    infected_parishes[i] += 1\n                    break # Breaks the innermost loop when the condition is met\n    plt.plot(days,infected_parishes) \n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of infected parishes')\n    plt.title('South Scania')\n    plt.show()         \n    return (infected_parishes, days)\n</pre> # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data  def plot_infected_parishes (beta_estimated                             , mu_estimated                             , p_estimated                             , gdf: gpd.GeoDataFrame = example                             , column_name: str = 'ParishName'                             , n: int = 0                             ):     model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}          model_solution = generate_sol(model_estimation)              #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Initializing the number of infected parishes per month for the model's output     infected_parishes = np.zeros(len(cum_infected_parishes_by_month))     days = np.zeros(len(cum_infected_parishes_by_month))          # Computing the number of infected parishes per month from the model's output     for i in range(len(cum_infected_parishes_by_month)):         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']                  for k in range(len(grouped_by_parish)):             for day in range(init_days, final_days):                 if model_solution['I'][k][day] &gt;= 1:                     days[i] = day                     infected_parishes[i] += 1                     break # Breaks the innermost loop when the condition is met     plt.plot(days,infected_parishes)      plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of infected parishes')     plt.title('South Scania')     plt.show()              return (infected_parishes, days)  In\u00a0[117]: Copied! <pre>plot_infected_parishes(beta_estimated, mu_estimated, p_estimated, example, 'ParishName', 0)\n</pre> plot_infected_parishes(beta_estimated, mu_estimated, p_estimated, example, 'ParishName', 0) Out[117]: <pre>(array([1., 1., 2., 3., 3., 3., 3., 1., 1., 1.]),\n array([  0.,  41.,  83., 104., 122., 153., 183., 214., 245., 273.]))</pre> In\u00a0[110]: Copied! <pre>def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):\n    results = count_infected_parishes_by_month(df, date, n, column_name, start_date, end_date)\n    plt.plot(results['DaysToEndOfMonth'], results['CumInfectParishes'],\n              label='Number of infected parishes', color='blue')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of infected parishes')\n    plt.title('South Scania')\n    plt.show()\nplot_parishes_by_month(example, 'JUN 1712', 0)\n</pre> def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):     results = count_infected_parishes_by_month(df, date, n, column_name, start_date, end_date)     plt.plot(results['DaysToEndOfMonth'], results['CumInfectParishes'],               label='Number of infected parishes', color='blue')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of infected parishes')     plt.title('South Scania')     plt.show() plot_parishes_by_month(example, 'JUN 1712', 0) In\u00a0[86]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[87]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            # axes[i].plot(initial_position, 0, 'bo')\n            # axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            # axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             # axes[i].plot(initial_position, 0, 'bo')             # axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             # axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Plotting the daily deaths by parish</p> In\u00a0[618]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[619]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\n# tick_positions = southeastScania['BeginDaysPlague'].values\n# tick_labels = southeastScania['BeginPlaguePeriod'].apply(\n#     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  # tick_positions = southeastScania['BeginDaysPlague'].values # tick_labels = southeastScania['BeginPlaguePeriod'].apply( #     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[267]: Copied! <pre># Set up the data to fit\nn = model_input.n\n\n# # Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p_coeff(p_guess=0.3)\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]*len(beta_guess.flatten())\nmu_bounds = [(0.1,0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p\n\n# Concatenate the bounds\nbounds = beta_bounds + mu_bounds + p_bounds\n# def set_constraint(x):\n#     p = x[2*n:].reshape(n, n)\n#     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n#     #[p[i,j] - p[j,i] == 0 for i in range(n) for j in range(n)] \n#     for i in range(n):\n#         for j in range(n):\n#             if p[i,j] - p[j,i] != 0:\n#                return 1\n#     return 0\n\n# def set_constraint(x):\n#     p = x[2*n:].reshape(n, n)\n#     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n#     diff_matrix = np.abs(p - p.T)\n#     if np.any(diff_matrix != 0):\n#         return 1\n#     return 0\n\n# Minimize the objective function to obtain beta, mu, and p\nresult = optimize.minimize(objectiveFunction, x0=initial_parameters\n                           , args=(example, 0, 'BeginPlaguePeriod', 'ParishName', 'BeginDaysPlague', 'EndDaysPlague', 'VictimsNumber')\n                           , bounds=bounds\n                           # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n                           #, constraints = ({'type': 'eq', 'fun': set_constraint})\n\n\n                           # Constraint for checking that the vector p after a transformation gives a symmetric matrix \n                           #, constraints = ({'type': 'eq', 'fun': lambda p: np.triu(p,1) + np.tril(p, -1)})\n                           )\n\nbeta_estimated = result.x[:n].reshape(n,)\nmu_estimated = result.x[n:2*n].reshape(n,)\np_estimated = result.x[2*n:].reshape(n, n)\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> # Set up the data to fit n = model_input.n  # # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p_coeff(p_guess=0.3) initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)  # Define the bounds for beta, mu and p beta_bounds = [(0,1)]*len(beta_guess.flatten()) mu_bounds = [(0.1,0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p  # Concatenate the bounds bounds = beta_bounds + mu_bounds + p_bounds # def set_constraint(x): #     p = x[2*n:].reshape(n, n) #     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other #     #[p[i,j] - p[j,i] == 0 for i in range(n) for j in range(n)]  #     for i in range(n): #         for j in range(n): #             if p[i,j] - p[j,i] != 0: #                return 1 #     return 0  # def set_constraint(x): #     p = x[2*n:].reshape(n, n) #     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other #     diff_matrix = np.abs(p - p.T) #     if np.any(diff_matrix != 0): #         return 1 #     return 0  # Minimize the objective function to obtain beta, mu, and p result = optimize.minimize(objectiveFunction, x0=initial_parameters                            , args=(example, 0, 'BeginPlaguePeriod', 'ParishName', 'BeginDaysPlague', 'EndDaysPlague', 'VictimsNumber')                            , bounds=bounds                            # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other                            #, constraints = ({'type': 'eq', 'fun': set_constraint})                              # Constraint for checking that the vector p after a transformation gives a symmetric matrix                             #, constraints = ({'type': 'eq', 'fun': lambda p: np.triu(p,1) + np.tril(p, -1)})                            )  beta_estimated = result.x[:n].reshape(n,) mu_estimated = result.x[n:2*n].reshape(n,) p_estimated = result.x[2*n:].reshape(n, n)  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  0.3333333333333333\nbeta =  [0.5 0.5 0.5]\nmu =  [0.5 0.5 0.5]\np =  [[0.  0.3 0.3]\n [0.3 0.  0.3]\n [0.3 0.3 0. ]]\n</pre> In\u00a0[268]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) In\u00a0[269]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            # axes[i].plot(initial_position, 0, 'bo')\n            # axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            # axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             # axes[i].plot(initial_position, 0, 'bo')             # axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             # axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()"},{"location":"PlagueProject/FittingObFunct2_one_parameter/","title":"FittingObFunct2 one parameter","text":"In\u00a0[1]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 In\u00a0[3]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[4]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      <pre>/opt/homebrew/lib/python3.11/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n  warnings.warn(\n</pre> In\u00a0[5]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[6]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[7]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n\ntype(southScania)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]]  type(southScania) Out[7]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Getting the centroid of each polygon for defining the transmission matrix. First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[8]: Copied! <pre>southScania = get_centroid(southScania)\nsouthScania = southScania.replace(['UNDEFINED', '?'], np.nan)\nsouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    southScania['BeginPlaguePeriod'], format='%b %Y')\nsouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    southScania['EndPlaguePeriod'], format='%b %Y')\n\nlen(southScania)\n</pre> southScania = get_centroid(southScania) southScania = southScania.replace(['UNDEFINED', '?'], np.nan) southScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     southScania['BeginPlaguePeriod'], format='%b %Y') southScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     southScania['EndPlaguePeriod'], format='%b %Y')  len(southScania) Out[8]: <pre>235</pre> <p>Defining a group to work with</p> In\u00a0[9]: Copied! <pre># Filter the data to get only the infected parishes\nsouthScania = southScania[southScania['new_format_BeginPlaguePeriod'].notna()]\n</pre> # Filter the data to get only the infected parishes southScania = southScania[southScania['new_format_BeginPlaguePeriod'].notna()] In\u00a0[10]: Copied! <pre># group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n#                  | (southScania['ParishName'] == '\u00d6JA')\n#                  | (southScania['ParishName'] == 'BROMMA')\n#                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n#                  | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n#                  | (southScania['ParishName'] == 'VALLEBERGA')\n#                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n#                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n#                  | (southScania['ParishName'] == 'INGELSTORP')\n#                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n#                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n#                  | (southScania['ParishName'] == 'HEDESKOGA')\n#                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n# ]     \n# group1 = group1.reset_index(drop=True)\n# group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\n# group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> # group1 = southScania[(southScania['ParishName'] == 'YSTAD') #                  | (southScania['ParishName'] == '\u00d6JA') #                  | (southScania['ParishName'] == 'BROMMA') #                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')  #                  | (southScania['ParishName'] == 'STORA K\u00d6PINGE') #                  | (southScania['ParishName'] == 'VALLEBERGA') #                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712')) #                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712')) #                  | (southScania['ParishName'] == 'INGELSTORP') #                  | (southScania['ParishName'] == 'HAMMENH\u00d6G') #                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712')) #                  | (southScania['ParishName'] == 'HEDESKOGA') #                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) # ]      # group1 = group1.reset_index(drop=True) # group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' # group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED' In\u00a0[11]: Copied! <pre># group = group1\n</pre> # group = group1 In\u00a0[12]: Copied! <pre># #replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\n# group = group.replace(['UNDEFINED', '?'], np.nan)\n# group['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n#     group['BeginPlaguePeriod'], format='%b %Y')\n# group['new_format_EndPlaguePeriod'] = pd.to_datetime(\n#     group['EndPlaguePeriod'], format='%b %Y')\n</pre> # #replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe # group = group.replace(['UNDEFINED', '?'], np.nan) # group['new_format_BeginPlaguePeriod'] = pd.to_datetime( #     group['BeginPlaguePeriod'], format='%b %Y') # group['new_format_EndPlaguePeriod'] = pd.to_datetime( #     group['EndPlaguePeriod'], format='%b %Y') In\u00a0[13]: Copied! <pre># # Getting the centroid of each polygon for defining the transmission matrix\n# cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n#                                          , 'new_format_BeginPlaguePeriod'\n#                                          , 'new_format_EndPlaguePeriod'\n#                                          )\n#                         )\n# # Fix the tYpe of Victims number to numeric\n# cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n</pre> # # Getting the centroid of each polygon for defining the transmission matrix # cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group) #                                          , 'new_format_BeginPlaguePeriod' #                                          , 'new_format_EndPlaguePeriod' #                                          ) #                         ) # # Fix the tYpe of Victims number to numeric # cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) In\u00a0[14]: Copied! <pre># # Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n# # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0\n# cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']\n</pre> # # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) # # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0 # cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']  <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[15]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        \n        self.S0 = np.zeros(self.n)       \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name]\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)                  self.S0 = np.zeros(self.n)                for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name]      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()           <p>Generating the differential equations</p> In\u00a0[16]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[17]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] \n    mu = parameters['mu'] \n    p_coeff = parameters['p_coeff'] \n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    beta = beta * identity_matrix(gdf)\n    p_matrix = p_coeff * gravitational_matrix(gdf)\n \n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + (beta + p_matrix) \n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T'] \n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta']      mu = parameters['mu']      p_coeff = parameters['p_coeff']      gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      beta = beta * identity_matrix(gdf)     p_matrix = p_coeff * gravitational_matrix(gdf)        # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + (beta + p_matrix)       sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)       dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]      derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']      t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} In\u00a0[18]: Copied! <pre># Selecting specific rows from the dataframe reseting the index\nexample = cluster1\nmodel_input = Initial_Model(example)\n</pre> # Selecting specific rows from the dataframe reseting the index example = cluster1 model_input = Initial_Model(example) <p>Defining the optimization problem:</p> In\u00a0[20]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0):\n    parameters = np.array(parameters)\n\n    n = model_input.n\n\n    # Asign parameters to beta, mu and p_coeff\n    beta: float = parameters[0]\n    mu: float = parameters[1]\n    p_coeff: float = parameters[2]\n   \n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p_coeff': p_coeff,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Make a list of the days to iterate over \n    days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values\n    # Initializing the number of infected parishes per month for the model's output\n    model_infected_parishes = np.zeros(len(days))\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(days))\n\n    # Initializing a matrix where the rows represents the number of parishes and the columns the number of days\n    matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n    matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n\n    for k in range(len(grouped_by_parish)):\n        for i, day in enumerate(days):\n            if day &lt; len(model_sol['D'][k]):\n                if model_sol['D'][k][day] &gt;= 1.0 :\n                    matrix_death_parishes_month[k, i] = model_sol['D'][k][day]\n     \n    for i in range(matrix_death_parishes_month.shape[0]):\n        for j in range(matrix_death_parishes_month.shape[1]):\n            if j == 0: # For the first day, there's no previous day to compare\n                matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1\n            else:\n                diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]\n                if diff &gt;= 1.0:\n                    matrix_infected_parishes_month[i,j] = 1\n                else:\n                    matrix_infected_parishes_month[i,j] = 0\n    \n    # Computing the number of infected parishes per month from the matrix\n    for j in range(matrix_infected_parishes_month.shape[1]):\n        # Sum up all the values in the column and store it \n        model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])\n        error[j] = (model_infected_parishes[j] \n                           - cum_infected_parishes_by_month['NumberInfectedParishes'][j])**2\n    \n    max_error = np.max(error)\n    # Computing the error between the model's output and the data\n    total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))\n          \n    return (total_error)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0):     parameters = np.array(parameters)      n = model_input.n      # Asign parameters to beta, mu and p_coeff     beta: float = parameters[0]     mu: float = parameters[1]     p_coeff: float = parameters[2]         model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p_coeff': p_coeff,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Make a list of the days to iterate over      days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values     # Initializing the number of infected parishes per month for the model's output     model_infected_parishes = np.zeros(len(days))     # Initializing the error between the model's output and the data     error = np.zeros(len(days))      # Initializing a matrix where the rows represents the number of parishes and the columns the number of days     matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))     matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))      for k in range(len(grouped_by_parish)):         for i, day in enumerate(days):             if day &lt; len(model_sol['D'][k]):                 if model_sol['D'][k][day] &gt;= 1.0 :                     matrix_death_parishes_month[k, i] = model_sol['D'][k][day]           for i in range(matrix_death_parishes_month.shape[0]):         for j in range(matrix_death_parishes_month.shape[1]):             if j == 0: # For the first day, there's no previous day to compare                 matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1             else:                 diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]                 if diff &gt;= 1.0:                     matrix_infected_parishes_month[i,j] = 1                 else:                     matrix_infected_parishes_month[i,j] = 0          # Computing the number of infected parishes per month from the matrix     for j in range(matrix_infected_parishes_month.shape[1]):         # Sum up all the values in the column and store it          model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])         error[j] = (model_infected_parishes[j]                             - cum_infected_parishes_by_month['NumberInfectedParishes'][j])**2          max_error = np.max(error)     # Computing the error between the model's output and the data     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))                return (total_error) In\u00a0[21]: Copied! <pre>from pyDOE import lhs\n\n# Define a function to generate parameters using Latin Hypercube Sampling\ndef generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, size):\n    beta = lhs(n=1, samples=size)\n    beta = beta * (beta_bounds[0][1] - beta_bounds[0][0]) + beta_bounds[0][0]\n    \n    mu = lhs(n=1, samples=size)\n    mu = mu * (mu_bounds[0][1] - mu_bounds[0][0]) + mu_bounds[0][0]\n    \n    p = lhs(n=1, samples=size)\n    p = p * (p_bounds[0][1] - p_bounds[0][0]) + p_bounds[0][0]\n    \n    return np.concatenate((beta, mu, p), axis=None)\n\n# Set up the data to fit\nn = model_input.n\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]\nmu_bounds = [(0,1)]\np_bounds = [(0,1)]   \n\nnum_iterations = 30\n\n# Generate parameters using LHS\nparameters_samples = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, num_iterations)\n\n# Initialize variables to store the best parameters and minimum error\nmin_error = np.inf\nbest_parameters = None\n\n# Run Monte Carlo simulation for a specified number of iterations\nfor i in range(num_iterations):\n    # Generate random parameters\n    parameters = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds,5)\n    \n    # Calculate the objective function with these parameters\n    error = objectiveFunction(parameters, example, 'ParishName', 0)\n    \n    # If this error is less than the current minimum, update the minimum and best parameters\n    if error &lt; min_error:\n        min_error = error\n        best_parameters = parameters\n    \n    # Store the parameters\n    parameters_samples = np.append(parameters_samples, parameters, axis=0)\n\n# Extract estimated parameters\nbeta_estimated = best_parameters[0]\nmu_estimated = best_parameters[1]\np_estimated = best_parameters[2]\n\nprint(\"Minimum error = \", min_error)\nprint(\"Estimated beta = \", beta_estimated)\nprint(\"Estimated mu = \", mu_estimated)\nprint(\"Estimated p = \", p_estimated)\n\nfrom scipy import stats\n\n# Reshape parameters_samples to have 3 columns (beta, mu, p)\nparameters_samples = parameters_samples.reshape(-1, 3)\n\n# Compute means\nbeta_mean = np.mean(parameters_samples[:, 0])\nmu_mean = np.mean(parameters_samples[:, 1])\np_mean = np.mean(parameters_samples[:, 2])\n\n# Compute standard deviations\nbeta_std = np.std(parameters_samples[:, 0])\nmu_std = np.std(parameters_samples[:, 1])\np_std = np.std(parameters_samples[:, 2])\n\n# Compute 95% confidence intervals\nbeta_ci = stats.norm.interval(0.95, loc=beta_mean, scale=beta_std)\nmu_ci = stats.norm.interval(0.95, loc=mu_mean, scale=mu_std)\np_ci = stats.norm.interval(0.95, loc=p_mean, scale=p_std)\n\nprint(\"Beta: Mean = \", beta_mean, \", Std = \", beta_std, \", CI = \", beta_ci)\nprint(\"Mu: Mean = \", mu_mean, \", Std = \", mu_std, \", CI = \", mu_ci)\nprint(\"P: Mean = \", p_mean, \", Std = \", p_std, \", CI = \", p_ci)\n</pre> from pyDOE import lhs  # Define a function to generate parameters using Latin Hypercube Sampling def generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, size):     beta = lhs(n=1, samples=size)     beta = beta * (beta_bounds[0][1] - beta_bounds[0][0]) + beta_bounds[0][0]          mu = lhs(n=1, samples=size)     mu = mu * (mu_bounds[0][1] - mu_bounds[0][0]) + mu_bounds[0][0]          p = lhs(n=1, samples=size)     p = p * (p_bounds[0][1] - p_bounds[0][0]) + p_bounds[0][0]          return np.concatenate((beta, mu, p), axis=None)  # Set up the data to fit n = model_input.n  # Define the bounds for beta, mu and p beta_bounds = [(0,1)] mu_bounds = [(0,1)] p_bounds = [(0,1)]     num_iterations = 30  # Generate parameters using LHS parameters_samples = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, num_iterations)  # Initialize variables to store the best parameters and minimum error min_error = np.inf best_parameters = None  # Run Monte Carlo simulation for a specified number of iterations for i in range(num_iterations):     # Generate random parameters     parameters = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds,5)          # Calculate the objective function with these parameters     error = objectiveFunction(parameters, example, 'ParishName', 0)          # If this error is less than the current minimum, update the minimum and best parameters     if error &lt; min_error:         min_error = error         best_parameters = parameters          # Store the parameters     parameters_samples = np.append(parameters_samples, parameters, axis=0)  # Extract estimated parameters beta_estimated = best_parameters[0] mu_estimated = best_parameters[1] p_estimated = best_parameters[2]  print(\"Minimum error = \", min_error) print(\"Estimated beta = \", beta_estimated) print(\"Estimated mu = \", mu_estimated) print(\"Estimated p = \", p_estimated)  from scipy import stats  # Reshape parameters_samples to have 3 columns (beta, mu, p) parameters_samples = parameters_samples.reshape(-1, 3)  # Compute means beta_mean = np.mean(parameters_samples[:, 0]) mu_mean = np.mean(parameters_samples[:, 1]) p_mean = np.mean(parameters_samples[:, 2])  # Compute standard deviations beta_std = np.std(parameters_samples[:, 0]) mu_std = np.std(parameters_samples[:, 1]) p_std = np.std(parameters_samples[:, 2])  # Compute 95% confidence intervals beta_ci = stats.norm.interval(0.95, loc=beta_mean, scale=beta_std) mu_ci = stats.norm.interval(0.95, loc=mu_mean, scale=mu_std) p_ci = stats.norm.interval(0.95, loc=p_mean, scale=p_std)  print(\"Beta: Mean = \", beta_mean, \", Std = \", beta_std, \", CI = \", beta_ci) print(\"Mu: Mean = \", mu_mean, \", Std = \", mu_std, \", CI = \", mu_ci) print(\"P: Mean = \", p_mean, \", Std = \", p_std, \", CI = \", p_ci)  <pre>Minimum error =  0.34732510288065843\nEstimated beta =  0.652328794059636\nEstimated mu =  0.3495510086319139\nEstimated p =  0.9294677550963119\nBeta: Mean =  0.5171644760915517 , Std =  0.27712935775507785 , CI =  (-0.025999084167116826, 1.0603280363502203)\nMu: Mean =  0.46228752982480903 , Std =  0.29659141175023734 , CI =  (-0.11902095532954593, 1.043596014979164)\nP: Mean =  0.5009635200876775 , Std =  0.29350412097858747 , CI =  (-0.07429398634444084, 1.076221026519796)\n</pre> <p>Substituting the estimated values into the model and solving it</p> In\u00a0[25]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plot to check how the objective function works</p> In\u00a0[26]: Copied! <pre>plot_infected_parishes(model_solution, example, 'ParishName', 0)\n</pre> plot_infected_parishes(model_solution, example, 'ParishName', 0) Out[26]: <pre>array([ 1., 10., 12., 12., 12.,  9.,  1.,  0.,  0.,  0.])</pre> <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[27]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>############################################################################################################\n</pre> ############################################################################################################ <p>Defining the objective functions</p> In\u00a0[\u00a0]: Copied! <pre># # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n\n# def objectiveFunction_2 (model_sol: dict\n#                          , gdf: gpd.GeoDataFrame = example\n#                          , column_name: str = 'ParishName'\n#                          , n: int = 0\n#                          ):\n#     #Group the dataframe by parish name without repetitions\n#     grouped_by_parish = gdf.groupby(column_name)\n#     # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n#     date = gdf.loc[0, 'BeginPlaguePeriod']\n#     # Getting the number of infected parishes per month from the data\n#     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n#     # Initializing the number of infected parishes per month for the model's output\n#     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n#     # Initializing the error between the model's output and the data\n#     error = np.zeros(len(cum_infected_parishes_by_month))\n    \n#     # Computing the number of infected parishes per month from the model's output\n#     for i in range(len(cum_infected_parishes_by_month)):\n#         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n#         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n        \n#         for k in range(len(grouped_by_parish)):\n#             for day in range(init_days, final_days):\n#                 if model_sol['I'][k][day] &gt; 1:\n#                     model_infected_parishes[i] += 1\n#                     break # Breaks the innermost loop when the condition is met\n#         error[i] = (model_infected_parishes[i] \n#                            - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2\n    \n#     max_error = np.max(error)\n#     # Computing the error between the model's output and the data\n#     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))\n          \n#     return (total_error)\n</pre> # # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data  # def objectiveFunction_2 (model_sol: dict #                          , gdf: gpd.GeoDataFrame = example #                          , column_name: str = 'ParishName' #                          , n: int = 0 #                          ): #     #Group the dataframe by parish name without repetitions #     grouped_by_parish = gdf.groupby(column_name) #     # Defining the initial date of the dataframe to start counting the number of infected parishes per month #     date = gdf.loc[0, 'BeginPlaguePeriod'] #     # Getting the number of infected parishes per month from the data #     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n) #     # Initializing the number of infected parishes per month for the model's output #     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month)) #     # Initializing the error between the model's output and the data #     error = np.zeros(len(cum_infected_parishes_by_month))      #     # Computing the number of infected parishes per month from the model's output #     for i in range(len(cum_infected_parishes_by_month)): #         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate'] #         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']          #         for k in range(len(grouped_by_parish)): #             for day in range(init_days, final_days): #                 if model_sol['I'][k][day] &gt; 1: #                     model_infected_parishes[i] += 1 #                     break # Breaks the innermost loop when the condition is met #         error[i] = (model_infected_parishes[i]  #                            - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2      #     max_error = np.max(error) #     # Computing the error between the model's output and the data #     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))            #     return (total_error)  In\u00a0[\u00a0]: Copied! <pre># def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0):\n#     parameters = np.array(parameters)\n\n#     n = model_input.n\n#     # Reshape parameters back to their original shapes\n#     beta: np.array = parameters[:n].reshape(n,)\n#     mu:  np.array = parameters[n:2*n].reshape(n,)\n#     p_coeff: float = parameters[2*n]\n#     # p_coeff: np.array = parameters[2*n:].reshape(n, n)\n\n#     # # Penalize if p_coeff is not symmetric or has non-zero diagonal elements\n#     # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):\n#     #     return 1e50\n\n#     # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)\n#     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T\n\n#     model_info = {'model': SEIRD_model,\n#                   'init': {\n#                       'S': model_input.S0,\n#                       'E': model_input.E0,\n#                       'I': model_input.I0,\n#                       'R': model_input.R0,\n#                       'D': model_input.D0,\n#                   },\n#                   'gdf': gdf,\n#                   'beta': beta,\n#                   'p_coeff': p_coeff,\n#                   'mu': mu,\n#                   'gamma': 0.4,\n#                   'sigma': 0.17,\n#                   'bump_center': 0.0,\n#                   'bump_width': 0.0,\n#                   'bump_height': 0.0,\n#                   'N': model_input.patchPop(),\n#                   'n': model_input.n,\n#                   'T': model_input.maxDays()}\n\n#     model_sol = generate_sol(model_info)\n\n#     # Group the dataframe by parish name without repetitions\n#     grouped_by_parish = gdf.groupby(column_name)\n#     # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n#     date = gdf.loc[0, 'BeginPlaguePeriod']\n#     # Getting the number of infected parishes per month from the data\n#     cum_infected_parishes_by_month = count_infected_parishes_by_month(\n#         gdf, date, n)\n#     # Make a list of the days to iterate over we took initial date because we are using infected humans\n#     days = cum_infected_parishes_by_month['DaysFromInitialDate'].values\n#     # Initializing the number of infected parishes per month for the model's output\n#     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n#     # Initializing the error between the model's output and the data\n#     error = np.zeros(len(cum_infected_parishes_by_month))\n\n#     # Initializing a matrix where the rows represents the number of parishes and the columns the number of days\n#     matrix_infected_parishes = np.zeros((len(grouped_by_parish), len(days)))\n\n#     # Computing the number of infected parishes per month from the model's output\n#     # for i in range(len(cum_infected_parishes_by_month)):\n#     #     init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n#     #     final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n\n#     for i, day in enumerate(days):\n#         for k in range(len(grouped_by_parish)):\n#             # for day in range(init_days, min(final_days, len(model_sol['I'][k]))):\n#             if model_sol['I'][k][day] &gt;= 1 :\n#                 model_infected_parishes[i] += 1\n#                 break  # Breaks the innermost loop when the condition is met\n#         error[i] = (model_infected_parishes[i]\n#                     - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2\n\n#     max_error = np.max(error)\n#     # Computing the error between the model's output and the data\n#     total_error = (np.sum(error))/(max_error *\n#                                    len(cum_infected_parishes_by_month))\n\n#     return (total_error)\n</pre> # def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0): #     parameters = np.array(parameters)  #     n = model_input.n #     # Reshape parameters back to their original shapes #     beta: np.array = parameters[:n].reshape(n,) #     mu:  np.array = parameters[n:2*n].reshape(n,) #     p_coeff: float = parameters[2*n] #     # p_coeff: np.array = parameters[2*n:].reshape(n, n)  #     # # Penalize if p_coeff is not symmetric or has non-zero diagonal elements #     # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0): #     #     return 1e50  #     # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1) #     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T  #     model_info = {'model': SEIRD_model, #                   'init': { #                       'S': model_input.S0, #                       'E': model_input.E0, #                       'I': model_input.I0, #                       'R': model_input.R0, #                       'D': model_input.D0, #                   }, #                   'gdf': gdf, #                   'beta': beta, #                   'p_coeff': p_coeff, #                   'mu': mu, #                   'gamma': 0.4, #                   'sigma': 0.17, #                   'bump_center': 0.0, #                   'bump_width': 0.0, #                   'bump_height': 0.0, #                   'N': model_input.patchPop(), #                   'n': model_input.n, #                   'T': model_input.maxDays()}  #     model_sol = generate_sol(model_info)  #     # Group the dataframe by parish name without repetitions #     grouped_by_parish = gdf.groupby(column_name) #     # Defining the initial date of the dataframe to start counting the number of infected parishes per month #     date = gdf.loc[0, 'BeginPlaguePeriod'] #     # Getting the number of infected parishes per month from the data #     cum_infected_parishes_by_month = count_infected_parishes_by_month( #         gdf, date, n) #     # Make a list of the days to iterate over we took initial date because we are using infected humans #     days = cum_infected_parishes_by_month['DaysFromInitialDate'].values #     # Initializing the number of infected parishes per month for the model's output #     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month)) #     # Initializing the error between the model's output and the data #     error = np.zeros(len(cum_infected_parishes_by_month))  #     # Initializing a matrix where the rows represents the number of parishes and the columns the number of days #     matrix_infected_parishes = np.zeros((len(grouped_by_parish), len(days)))  #     # Computing the number of infected parishes per month from the model's output #     # for i in range(len(cum_infected_parishes_by_month)): #     #     init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate'] #     #     final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']  #     for i, day in enumerate(days): #         for k in range(len(grouped_by_parish)): #             # for day in range(init_days, min(final_days, len(model_sol['I'][k]))): #             if model_sol['I'][k][day] &gt;= 1 : #                 model_infected_parishes[i] += 1 #                 break  # Breaks the innermost loop when the condition is met #         error[i] = (model_infected_parishes[i] #                     - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2  #     max_error = np.max(error) #     # Computing the error between the model's output and the data #     total_error = (np.sum(error))/(max_error * #                                    len(cum_infected_parishes_by_month))  #     return (total_error)  In\u00a0[\u00a0]: Copied! <pre>### Testing the objective function                         \nobjectiveFunction_2(model_dict, example, 'ParishName', 0)\n</pre> ### Testing the objective function                          objectiveFunction_2(model_dict, example, 'ParishName', 0) In\u00a0[\u00a0]: Copied! <pre>def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):\n    results = count_infected_parishes_by_month(df, date, n, column_name, start_date, end_date)\n    plt.plot(results['DaysFromInitialDate'], results['NumberInfectedParishes'],\n              label='Number of infected parishes', color='blue')\n    plt.plot(results['DaysToEndOfMonth'], results['NumberInfectedParishes'],\n              label='Number of infected parishes', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of infected parishes')\n    plt.title('South Scania')\n    plt.show()\n    return results\nplot_parishes_by_month(example, 'JUN 1712', 0)\n</pre> def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):     results = count_infected_parishes_by_month(df, date, n, column_name, start_date, end_date)     plt.plot(results['DaysFromInitialDate'], results['NumberInfectedParishes'],               label='Number of infected parishes', color='blue')     plt.plot(results['DaysToEndOfMonth'], results['NumberInfectedParishes'],               label='Number of infected parishes', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of infected parishes')     plt.title('South Scania')     plt.show()     return results plot_parishes_by_month(example, 'JUN 1712', 0) Out[\u00a0]: date DaysFromInitialDate NumberInfectedParishes CumInfectParishes EndOfMonth DaysToEndOfMonth InfectedParishes 0 1712-06-01 0 2 2 1712-06-30 29 {\u00d6JA, YSTAD} 1 1712-07-01 30 6 6 1712-07-31 60 {YSTAD, VALLEBERGA, H\u00d6RUP, BJ\u00c4RESJ\u00d6, STORA K\u00d6P... 2 1712-08-01 61 7 10 1712-08-31 91 {GLEMMINGE, YSTAD, INGELSTORP, BROMMA, STORA K... 3 1712-09-01 92 5 12 1712-09-30 121 {YSTAD, HEDESKOGA, STORA K\u00d6PINGE, \u00d6VRABY, \u00d6JA} 4 1712-10-01 122 4 12 1712-10-31 152 {\u00d6JA, YSTAD, HEDESKOGA, STORA K\u00d6PINGE} 5 1712-11-01 153 3 12 1712-11-30 182 {\u00d6JA, YSTAD, STORA K\u00d6PINGE} 6 1712-12-01 183 3 12 1712-12-31 213 {\u00d6JA, YSTAD, STORA K\u00d6PINGE} 7 1713-01-01 214 2 12 1713-01-31 244 {\u00d6JA, STORA K\u00d6PINGE} 8 1713-02-01 245 1 12 1713-02-28 272 {\u00d6JA} 9 1713-03-01 273 1 12 1713-03-31 303 {\u00d6JA} <p>Plotting the daily deaths by parish</p> In\u00a0[\u00a0]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\n# tick_positions = southeastScania['BeginDaysPlague'].values\n# tick_labels = southeastScania['BeginPlaguePeriod'].apply(\n#     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  # tick_positions = southeastScania['BeginDaysPlague'].values # tick_labels = southeastScania['BeginPlaguePeriod'].apply( #     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Runnning the classic optimization algorithm\n\n # Set up the data to fit\nn = model_input.n\n\n# # Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess), axis=None)\n\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0, 1)]*len(beta_guess.flatten())\nmu_bounds = [(0, 0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0, 1)]   # example bounds for p\n\n# Concatenate the bounds\nbounds = beta_bounds + mu_bounds + p_bounds\n\n# Minimize the objective function to obtain beta, mu, and p\nresult = optimize.minimize(objectiveFunction, x0=initial_parameters, args=(example, 'ParishName', 0), bounds=bounds\n                           )\n\nbeta_estimated = result.x[:n].reshape(n,)\nmu_estimated = result.x[n:2*n].reshape(n,)\np_estimated = result.x[2*n]\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre>  # Runnning the classic optimization algorithm   # Set up the data to fit n = model_input.n  # # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess), axis=None)   # Define the bounds for beta, mu and p beta_bounds = [(0, 1)]*len(beta_guess.flatten()) mu_bounds = [(0, 0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0, 1)]   # example bounds for p  # Concatenate the bounds bounds = beta_bounds + mu_bounds + p_bounds  # Minimize the objective function to obtain beta, mu, and p result = optimize.minimize(objectiveFunction, x0=initial_parameters, args=(example, 'ParishName', 0), bounds=bounds                            )  beta_estimated = result.x[:n].reshape(n,) mu_estimated = result.x[n:2*n].reshape(n,) p_estimated = result.x[2*n]  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) In\u00a0[\u00a0]: Copied! <pre>group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n                 | (southScania['ParishName'] == '\u00d6JA')\n                 | (southScania['ParishName'] == 'BROMMA')\n                 | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (southScania['ParishName'] == 'VALLEBERGA')\n                 | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (southScania['ParishName'] == 'INGELSTORP')\n                 | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (southScania['ParishName'] == 'HEDESKOGA')\n                 #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> group1 = southScania[(southScania['ParishName'] == 'YSTAD')                  | (southScania['ParishName'] == '\u00d6JA')                  | (southScania['ParishName'] == 'BROMMA')                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (southScania['ParishName'] == 'STORA K\u00d6PINGE')                  | (southScania['ParishName'] == 'VALLEBERGA')                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))                  | (southScania['ParishName'] == 'INGELSTORP')                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))                  | (southScania['ParishName'] == 'HEDESKOGA')                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) ]      group1 = group1.reset_index(drop=True) group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'"},{"location":"PlagueProject/FittingObFunct3_one_parameter/","title":"FittingObFunct3 one parameter","text":"In\u00a0[429]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[430]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[431]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[432]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[433]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[434]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[435]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n\ntype(southScania)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]]  type(southScania) Out[435]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[436]: Copied! <pre>southScania = get_centroid(southScania)\nsouthScania = southScania.replace(['UNDEFINED', '?'], np.nan)\nsouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    southScania['BeginPlaguePeriod'], format='%b %Y')\nsouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    southScania['EndPlaguePeriod'], format='%b %Y')\n\nlen(southScania)\n</pre> southScania = get_centroid(southScania) southScania = southScania.replace(['UNDEFINED', '?'], np.nan) southScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     southScania['BeginPlaguePeriod'], format='%b %Y') southScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     southScania['EndPlaguePeriod'], format='%b %Y')  len(southScania) Out[436]: <pre>235</pre> <p>Defining a group to work with</p> In\u00a0[437]: Copied! <pre># Filter the data to get only the infected parishes\nsouthScania = southScania[southScania['new_format_BeginPlaguePeriod'].notna()]\n</pre> # Filter the data to get only the infected parishes southScania = southScania[southScania['new_format_BeginPlaguePeriod'].notna()] In\u00a0[438]: Copied! <pre># group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n#                  | (southScania['ParishName'] == '\u00d6JA')\n#                  | (southScania['ParishName'] == 'BROMMA')\n#                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n#                  | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n#                  | (southScania['ParishName'] == 'VALLEBERGA')\n#                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n#                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n#                  | (southScania['ParishName'] == 'INGELSTORP')\n#                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n#                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n#                  | (southScania['ParishName'] == 'HEDESKOGA')\n#                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n# ]     \n# group1 = group1.reset_index(drop=True)\n# group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\n# group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> # group1 = southScania[(southScania['ParishName'] == 'YSTAD') #                  | (southScania['ParishName'] == '\u00d6JA') #                  | (southScania['ParishName'] == 'BROMMA') #                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')  #                  | (southScania['ParishName'] == 'STORA K\u00d6PINGE') #                  | (southScania['ParishName'] == 'VALLEBERGA') #                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712')) #                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712')) #                  | (southScania['ParishName'] == 'INGELSTORP') #                  | (southScania['ParishName'] == 'HAMMENH\u00d6G') #                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712')) #                  | (southScania['ParishName'] == 'HEDESKOGA') #                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) # ]      # group1 = group1.reset_index(drop=True) # group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' # group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED' <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[439]: Copied! <pre># group = group1\n</pre> # group = group1 In\u00a0[440]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\n# group = group.replace(['UNDEFINED', '?'], np.nan)\n# group['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n#     group['BeginPlaguePeriod'], format='%b %Y')\n# group['new_format_EndPlaguePeriod'] = pd.to_datetime(\n#     group['EndPlaguePeriod'], format='%b %Y')\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe # group = group.replace(['UNDEFINED', '?'], np.nan) # group['new_format_BeginPlaguePeriod'] = pd.to_datetime( #     group['BeginPlaguePeriod'], format='%b %Y') # group['new_format_EndPlaguePeriod'] = pd.to_datetime( #     group['EndPlaguePeriod'], format='%b %Y') In\u00a0[441]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n# # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0\n# cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) # # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0 # cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']  In\u00a0[442]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\n# cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n#                                          , 'new_format_BeginPlaguePeriod'\n#                                          , 'new_format_EndPlaguePeriod'\n#                                          )\n#                         )\n# # Fix the tYpe of Victims number to numeric\n# cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n</pre> # Getting the centroid of each polygon for defining the transmission matrix # cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group) #                                          , 'new_format_BeginPlaguePeriod' #                                          , 'new_format_EndPlaguePeriod' #                                          ) #                         ) # # Fix the tYpe of Victims number to numeric # cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[443]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        \n        self.S0 = np.zeros(self.n)       \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name]\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)                  self.S0 = np.zeros(self.n)                for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name]      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()        <p>Generating the differential equations</p> In\u00a0[446]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[423]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # ensure beta is a numpy array of shape n\n    mu = parameters['mu'] # ensure mu is a numpy array of shape n\n    p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    beta = beta * identity_matrix(gdf)\n    p_matrix = p_coeff * gravitational_matrix(gdf)\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + (beta + p_matrix)\n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # ensure beta is a numpy array of shape n     mu = parameters['mu'] # ensure mu is a numpy array of shape n     p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      beta = beta * identity_matrix(gdf)     p_matrix = p_coeff * gravitational_matrix(gdf)      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + (beta + p_matrix)      sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)       dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]      derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Trying a small dataframe</p> In\u00a0[\u00a0]: Copied! <pre># Selecting specific rows from the dataframe reseting the index\nexample = cluster1\nmodel_input = Initial_Model(example)\n</pre> # Selecting specific rows from the dataframe reseting the index example = cluster1 model_input = Initial_Model(example) <p>Defining the optimization problem:</p> In\u00a0[241]: Copied! <pre>def objectiveFunction(parameters\n                      , gdf: gpd.GeoDataFrame = example\n                      , column_name: str = 'ParishName'\n                      ):\n    parameters = np.array(parameters)\n\n      \n    beta: float = parameters[0]\n    mu:  float = parameters[1]\n    p_coeff: float = parameters[2]\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p_coeff': p_coeff,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    \n    # Getting the number of deaths per month from the data\n    cum_deaths_by_month = count_victims_by_month(gdf)\n\n    # Remove rows where 'EndMonth' is null\n    cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])\n\n    # Initializing the cum. number of deaths per month for the model's output\n    model_deaths_month = np.zeros(len(cum_deaths_by_month))\n    #model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))\n\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_deaths_by_month))\n    \n    # Computing the number of cum. deaths per month from the model's output\n    for i in range(len(cum_deaths_by_month)):\n        day = cum_deaths_by_month['CumDays'][i]\n        data = cum_deaths_by_month['CumDeaths'][i]\n\n        model_deaths_month[i] = np.sum([model_sol['D'][k][day] for k in range(len(grouped_by_parish))])    \n        # for k in range(len(grouped_by_parish)):\n        #     model_deaths_month[i] += model_sol['D'][k][day]\n        \n        # model_cum_deaths_month[i] = model_deaths_month[i]   \n\n        # if i &gt; 0:\n        #     model_cum_deaths_month[i] += model_cum_deaths_month[i-1] \n        \n        error[i] = (model_deaths_month[i] - data)**2\n        \n    max_error = np.max(error)    \n    # Computing the error between the model's output and the data\n    total_error = (np.sum(error))/(len(error)* max_error)\n          \n    return (total_error)\n</pre> def objectiveFunction(parameters                       , gdf: gpd.GeoDataFrame = example                       , column_name: str = 'ParishName'                       ):     parameters = np.array(parameters)             beta: float = parameters[0]     mu:  float = parameters[1]     p_coeff: float = parameters[2]      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p_coeff': p_coeff,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)          # Getting the number of deaths per month from the data     cum_deaths_by_month = count_victims_by_month(gdf)      # Remove rows where 'EndMonth' is null     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])      # Initializing the cum. number of deaths per month for the model's output     model_deaths_month = np.zeros(len(cum_deaths_by_month))     #model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))      # Initializing the error between the model's output and the data     error = np.zeros(len(cum_deaths_by_month))          # Computing the number of cum. deaths per month from the model's output     for i in range(len(cum_deaths_by_month)):         day = cum_deaths_by_month['CumDays'][i]         data = cum_deaths_by_month['CumDeaths'][i]          model_deaths_month[i] = np.sum([model_sol['D'][k][day] for k in range(len(grouped_by_parish))])             # for k in range(len(grouped_by_parish)):         #     model_deaths_month[i] += model_sol['D'][k][day]                  # model_cum_deaths_month[i] = model_deaths_month[i]             # if i &gt; 0:         #     model_cum_deaths_month[i] += model_cum_deaths_month[i-1]                   error[i] = (model_deaths_month[i] - data)**2              max_error = np.max(error)         # Computing the error between the model's output and the data     total_error = (np.sum(error))/(len(error)* max_error)                return (total_error)         In\u00a0[178]: Copied! <pre>from pyDOE import lhs\n\n# Define a function to generate parameters using Latin Hypercube Sampling\ndef generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, size):\n    beta = lhs(n=1, samples=size)\n    beta = beta * (beta_bounds[0][1] - beta_bounds[0][0]) + beta_bounds[0][0]\n    \n    mu = lhs(n=1, samples=size)\n    mu = mu * (mu_bounds[0][1] - mu_bounds[0][0]) + mu_bounds[0][0]\n    \n    p = lhs(n=1, samples=size)\n    p = p * (p_bounds[0][1] - p_bounds[0][0]) + p_bounds[0][0]\n    \n    return np.concatenate((beta, mu, p), axis=None)\n\n# Set up the data to fit\nn = model_input.n\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]\nmu_bounds = [(0,1)]\np_bounds = [(0,1)]   \n\nnum_iterations = 1\n\n# Generate parameters using LHS\nparameters_samples = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, num_iterations)\n\n# Initialize variables to store the best parameters and minimum error\nmin_error = np.inf\nbest_parameters = None\n\n# Run Monte Carlo simulation for a specified number of iterations\nfor i in range(num_iterations):\n    # Generate random parameters\n    parameters = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds,5)\n    \n    # Calculate the objective function with these parameters\n    error = objectiveFunction(parameters, example, 'ParishName')\n    \n    # If this error is less than the current minimum, update the minimum and best parameters\n    if error &lt; min_error:\n        min_error = error\n        best_parameters = parameters\n    \n    # Store the parameters\n    parameters_samples = np.append(parameters_samples, parameters, axis=0)\n\n# Extract estimated parameters\nbeta_estimated = best_parameters[0]\nmu_estimated = best_parameters[1]\np_estimated = best_parameters[2]\n\nprint(\"Minimum error = \", min_error)\nprint(\"Estimated beta = \", beta_estimated)\nprint(\"Estimated mu = \", mu_estimated)\nprint(\"Estimated p = \", p_estimated)\n\nfrom scipy import stats\n\n# Reshape parameters_samples to have 3 columns (beta, mu, p)\nparameters_samples = parameters_samples.reshape(-1, 3)\n\n# Compute means\nbeta_mean = np.mean(parameters_samples[:, 0])\nmu_mean = np.mean(parameters_samples[:, 1])\np_mean = np.mean(parameters_samples[:, 2])\n\n# Compute standard deviations\nbeta_std = np.std(parameters_samples[:, 0])\nmu_std = np.std(parameters_samples[:, 1])\np_std = np.std(parameters_samples[:, 2])\n\n# Compute 95% confidence intervals\nbeta_ci = stats.norm.interval(0.95, loc=beta_mean, scale=beta_std)\nmu_ci = stats.norm.interval(0.95, loc=mu_mean, scale=mu_std)\np_ci = stats.norm.interval(0.95, loc=p_mean, scale=p_std)\n\nprint(\"Beta: Mean = \", beta_mean, \", Std = \", beta_std, \", CI = \", beta_ci)\nprint(\"Mu: Mean = \", mu_mean, \", Std = \", mu_std, \", CI = \", mu_ci)\nprint(\"P: Mean = \", p_mean, \", Std = \", p_std, \", CI = \", p_ci)\n</pre> from pyDOE import lhs  # Define a function to generate parameters using Latin Hypercube Sampling def generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, size):     beta = lhs(n=1, samples=size)     beta = beta * (beta_bounds[0][1] - beta_bounds[0][0]) + beta_bounds[0][0]          mu = lhs(n=1, samples=size)     mu = mu * (mu_bounds[0][1] - mu_bounds[0][0]) + mu_bounds[0][0]          p = lhs(n=1, samples=size)     p = p * (p_bounds[0][1] - p_bounds[0][0]) + p_bounds[0][0]          return np.concatenate((beta, mu, p), axis=None)  # Set up the data to fit n = model_input.n  # Define the bounds for beta, mu and p beta_bounds = [(0,1)] mu_bounds = [(0,1)] p_bounds = [(0,1)]     num_iterations = 1  # Generate parameters using LHS parameters_samples = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, num_iterations)  # Initialize variables to store the best parameters and minimum error min_error = np.inf best_parameters = None  # Run Monte Carlo simulation for a specified number of iterations for i in range(num_iterations):     # Generate random parameters     parameters = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds,5)          # Calculate the objective function with these parameters     error = objectiveFunction(parameters, example, 'ParishName')          # If this error is less than the current minimum, update the minimum and best parameters     if error &lt; min_error:         min_error = error         best_parameters = parameters          # Store the parameters     parameters_samples = np.append(parameters_samples, parameters, axis=0)  # Extract estimated parameters beta_estimated = best_parameters[0] mu_estimated = best_parameters[1] p_estimated = best_parameters[2]  print(\"Minimum error = \", min_error) print(\"Estimated beta = \", beta_estimated) print(\"Estimated mu = \", mu_estimated) print(\"Estimated p = \", p_estimated)  from scipy import stats  # Reshape parameters_samples to have 3 columns (beta, mu, p) parameters_samples = parameters_samples.reshape(-1, 3)  # Compute means beta_mean = np.mean(parameters_samples[:, 0]) mu_mean = np.mean(parameters_samples[:, 1]) p_mean = np.mean(parameters_samples[:, 2])  # Compute standard deviations beta_std = np.std(parameters_samples[:, 0]) mu_std = np.std(parameters_samples[:, 1]) p_std = np.std(parameters_samples[:, 2])  # Compute 95% confidence intervals beta_ci = stats.norm.interval(0.95, loc=beta_mean, scale=beta_std) mu_ci = stats.norm.interval(0.95, loc=mu_mean, scale=mu_std) p_ci = stats.norm.interval(0.95, loc=p_mean, scale=p_std)  print(\"Beta: Mean = \", beta_mean, \", Std = \", beta_std, \", CI = \", beta_ci) print(\"Mu: Mean = \", mu_mean, \", Std = \", mu_std, \", CI = \", mu_ci) print(\"P: Mean = \", p_mean, \", Std = \", p_std, \", CI = \", p_ci)  <pre>Minimum error =  0.6894332199971802\nEstimated beta =  0.5885990523053602\nEstimated mu =  0.3003762199759821\nEstimated p =  0.8344640197883185\nBeta: Mean =  0.4371674691799002 , Std =  0.24736337064652625 , CI =  (-0.047655828381723664, 0.921990766741524)\nMu: Mean =  0.5822388242033397 , Std =  0.26777430504968947 , CI =  (0.057410830320706485, 1.1070668180859728)\nP: Mean =  0.6284184197170484 , Std =  0.2618710147725645 , CI =  (0.11516066216786547, 1.1416761772662314)\n</pre> <p>Substituting the estimated values into the model and solving it</p> In\u00a0[279]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[280]: Copied! <pre>plot_cum_deaths_model(model_solution, example, 'ParishName')\n</pre> plot_cum_deaths_model(model_solution, example, 'ParishName') Out[280]: <pre>array([219.76104471, 554.99732764, 806.83845061, 906.37218614,\n       910.1194666 , 910.97972505])</pre> In\u00a0[281]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n# def plot_cum_deaths_model(model_solution: dict\n#                             , gdf: gpd.GeoDataFrame = example\n#                             , column_name: str = 'ParishName'\n#                             ):\n      \n#     #Group the dataframe by parish name without repetitions\n#     grouped_by_parish = gdf.groupby(column_name)\n\n#     # Getting the number of deaths per month from the data\n#     cum_deaths_by_month = count_victims_by_month(gdf)\n\n#     # Remove rows where 'EndMonth' is null\n#     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])\n\n#     # Now, you can directly get the 'CumDays' and 'CumDeaths' without looping and checking for nulls\n#     days = cum_deaths_by_month['CumDays'].values\n#     cum_deaths = cum_deaths_by_month['CumDeaths'].values\n\n#     # Initializing the cum. number of deaths per month for the model's output\n#     model_deaths_month = np.zeros(len(cum_deaths_by_month))\n        \n#     # Computing the number of cum. deaths per month from the model's output\n#     for i in range(len(cum_deaths_by_month)):\n#         day = cum_deaths_by_month['CumDays'][i]\n                   \n#         for k in range(len(grouped_by_parish)):\n#             model_deaths_month[i] += model_solution['D'][k][day]\n          \n#     plt.plot(days, model_deaths_month, color='blue') \n#     plt.plot(days, cum_deaths, label='Number of infected parishes', color='orange')\n#     plt.xlabel('Month')\n#     plt.xticks( rotation=45)\n#     plt.ylabel('Cumulative Deaths')\n#     plt.title('South Scania')\n#     plt.show()         \n#     return (model_deaths_month, days, cum_deaths)\n</pre> # # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data # def plot_cum_deaths_model(model_solution: dict #                             , gdf: gpd.GeoDataFrame = example #                             , column_name: str = 'ParishName' #                             ):        #     #Group the dataframe by parish name without repetitions #     grouped_by_parish = gdf.groupby(column_name)  #     # Getting the number of deaths per month from the data #     cum_deaths_by_month = count_victims_by_month(gdf)  #     # Remove rows where 'EndMonth' is null #     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])  #     # Now, you can directly get the 'CumDays' and 'CumDeaths' without looping and checking for nulls #     days = cum_deaths_by_month['CumDays'].values #     cum_deaths = cum_deaths_by_month['CumDeaths'].values  #     # Initializing the cum. number of deaths per month for the model's output #     model_deaths_month = np.zeros(len(cum_deaths_by_month))          #     # Computing the number of cum. deaths per month from the model's output #     for i in range(len(cum_deaths_by_month)): #         day = cum_deaths_by_month['CumDays'][i]                     #         for k in range(len(grouped_by_parish)): #             model_deaths_month[i] += model_solution['D'][k][day]            #     plt.plot(days, model_deaths_month, color='blue')  #     plt.plot(days, cum_deaths, label='Number of infected parishes', color='orange') #     plt.xlabel('Month') #     plt.xticks( rotation=45) #     plt.ylabel('Cumulative Deaths') #     plt.title('South Scania') #     plt.show()          #     return (model_deaths_month, days, cum_deaths)"},{"location":"PlagueProject/FittingSeasonMetapopModelPlague/","title":"FittingSeasonMetapopModelPlague","text":"In\u00a0[31]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[32]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[33]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[34]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[35]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[36]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[37]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n\ntype(southScania)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]]  type(southScania) Out[37]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Fixing the dataframe to work in R</p> In\u00a0[38]: Copied! <pre># southeastScania['geometry'] = southeastScania['geometry'].apply(wkt.loads)\n# southeastScania = gpd.GeoDataFrame(southeastScania, geometry='geometry')\n# southeastScania = southeastScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n#                            , 'EndPlaguePeriod', 'VictimsNumber', 'plague', 'geometry'\n#                            ]]\n\n# southeastScania = get_centroid(southeastScania)\n</pre> # southeastScania['geometry'] = southeastScania['geometry'].apply(wkt.loads) # southeastScania = gpd.GeoDataFrame(southeastScania, geometry='geometry') # southeastScania = southeastScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod' #                            , 'EndPlaguePeriod', 'VictimsNumber', 'plague', 'geometry' #                            ]]  # southeastScania = get_centroid(southeastScania) <p>Selecting the parishes to prove in R</p> In\u00a0[39]: Copied! <pre># Ystad_group = southeastScania[(southeastScania['ParishName'] == 'YSTAD')\n#                  | (southeastScania['ParishName'] == '\u00d6JA')\n#                  | (southeastScania['ParishName'] == 'HEDESKOGA')\n#                  | (southeastScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n#                  | (southeastScania['ParishName'] == 'BROMMA')\n#                  | (southeastScania['ParishName'] == 'STORA HERRESTA')\n#                  | (southeastScania['ParishName'] == 'BORRIE')\n#                  | (southeastScania['ParishName'] == 'STORA K\u00d6PINGE')\n# ]\n# Ystad_group = Ystad_group.reset_index(drop=True)\n# Ystad_group.at[2, 'BeginPlaguePeriod'] = 'AUG 1712'\n# Ystad_group.at[2, 'EndPlaguePeriod'] = 'UNDEFINED'\n\n# Ystad_group\n</pre> # Ystad_group = southeastScania[(southeastScania['ParishName'] == 'YSTAD') #                  | (southeastScania['ParishName'] == '\u00d6JA') #                  | (southeastScania['ParishName'] == 'HEDESKOGA') #                  | (southeastScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')  #                  | (southeastScania['ParishName'] == 'BROMMA') #                  | (southeastScania['ParishName'] == 'STORA HERRESTA') #                  | (southeastScania['ParishName'] == 'BORRIE') #                  | (southeastScania['ParishName'] == 'STORA K\u00d6PINGE') # ] # Ystad_group = Ystad_group.reset_index(drop=True) # Ystad_group.at[2, 'BeginPlaguePeriod'] = 'AUG 1712' # Ystad_group.at[2, 'EndPlaguePeriod'] = 'UNDEFINED'  # Ystad_group In\u00a0[40]: Copied! <pre># # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\n# Ystad_group = Ystad_group.replace(['UNDEFINED', '?'], np.nan)\n# Ystad_group['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n#     Ystad_group['BeginPlaguePeriod'], format='%b %Y')\n# Ystad_group['new_format_EndPlaguePeriod'] = pd.to_datetime(\n#     Ystad_group['EndPlaguePeriod'], format='%b %Y')\n</pre> # # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe # Ystad_group = Ystad_group.replace(['UNDEFINED', '?'], np.nan) # Ystad_group['new_format_BeginPlaguePeriod'] = pd.to_datetime( #     Ystad_group['BeginPlaguePeriod'], format='%b %Y') # Ystad_group['new_format_EndPlaguePeriod'] = pd.to_datetime( #     Ystad_group['EndPlaguePeriod'], format='%b %Y') In\u00a0[41]: Copied! <pre>#cluster.to_csv('Ystad_group.csv', index=False)\n</pre> #cluster.to_csv('Ystad_group.csv', index=False) <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[42]: Copied! <pre>southScania = get_centroid(southScania)\nsouthScania = southScania.replace(['UNDEFINED', '?'], np.nan)\nsouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    southScania['BeginPlaguePeriod'], format='%b %Y')\nsouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    southScania['EndPlaguePeriod'], format='%b %Y')\n\nlen(southScania)\n</pre> southScania = get_centroid(southScania) southScania = southScania.replace(['UNDEFINED', '?'], np.nan) southScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     southScania['BeginPlaguePeriod'], format='%b %Y') southScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     southScania['EndPlaguePeriod'], format='%b %Y')  len(southScania) Out[42]: <pre>235</pre> <p>Defining a group to work with</p> In\u00a0[43]: Copied! <pre>group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n                 | (southScania['ParishName'] == '\u00d6JA')\n                 | (southScania['ParishName'] == 'BROMMA')\n                 | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (southScania['ParishName'] == 'VALLEBERGA')\n                 | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (southScania['ParishName'] == 'INGELSTORP')\n                 | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (southScania['ParishName'] == 'HEDESKOGA')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> group1 = southScania[(southScania['ParishName'] == 'YSTAD')                  | (southScania['ParishName'] == '\u00d6JA')                  | (southScania['ParishName'] == 'BROMMA')                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (southScania['ParishName'] == 'STORA K\u00d6PINGE')                  | (southScania['ParishName'] == 'VALLEBERGA')                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))                  | (southScania['ParishName'] == 'INGELSTORP')                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))                  | (southScania['ParishName'] == 'HEDESKOGA')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) ]      group1 = group1.reset_index(drop=True) group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED' In\u00a0[44]: Copied! <pre>group1\n</pre> group1 Out[44]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry centroid new_format_BeginPlaguePeriod new_format_EndPlaguePeriod 0 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 NaN NaN POLYGON ((4228840.232 3178726.042, 4228969.528... POINT (4230600.699862956 3177291.2599278623) 1712-07-01 NaT 1 SOUTHEAST BROMMA 154 AUG 1712 UNDEFINED NaN POLYGON ((4231996.049 3179728.504, 4232042.002... POINT (4234027.027249504 3179705.7627846766) 1712-04-01 1712-05-01 2 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-01 3 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 4 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 5 SOUTHEAST GLEMMINGE 362 AUG 1712 AUG 1712 NaN POLYGON ((4246778.053 3180955.654, 4246924.466... POINT (4247370.639288611 3179086.3361054286) 1712-08-01 1712-08-01 6 SOUTHEAST HAMMENH\u00d6G 169 AUG 1712 NaN NaN POLYGON ((4255090.885 3182137.710, 4254726.251... POINT (4253884.1828368595 3183416.9898560927) 1712-08-01 NaT 7 SOUTHEAST H\u00d6RUP 289 JUL 1712 NaN 60 POLYGON ((4252154.708 3177178.545, 4251691.166... POINT (4252336.724268622 3179160.5725134797) 1712-07-01 NaT 8 SOUTHEAST INGELSTORP 358 AUG 1712 NaN 1 POLYGON ((4246833.197 3177120.892, 4246902.701... POINT (4247499.939231565 3174682.3176888516) 1712-08-01 NaT 9 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 10 SOUTHEAST VALLEBERGA 391 JUL 1712 NaN NaN POLYGON ((4249981.772 3179064.746, 4250047.408... POINT (4250097.829593883 3174762.056364127) 1712-07-01 NaT 11 SOUTHEAST \u00d6VRABY 208 SEP 1712 SEP 1712 NaN POLYGON ((4242987.654 3183129.802, 4242831.234... POINT (4242196.192299193 3183903.5739921844) 1712-09-01 1712-09-01 12 SOUTHEAST \u00d6VRABY 208 NOV 1712 NOV 1712 NaN POLYGON ((4242987.654 3183129.802, 4242831.234... POINT (4242196.192299193 3183903.5739921844) 1712-11-01 1712-11-01 <p>Set the working directory for private files with monthly data for some parishes</p> In\u00a0[45]: Copied! <pre># Southeast Scania\nsoutheast_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southeast')\n# Middle Scania\nmiddle_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Middle')\n# Southwest Scania\nsouthwest_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southwest')\n</pre> # Southeast Scania southeast_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southeast') # Middle Scania middle_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Middle') # Southwest Scania southwest_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southwest') <p>Function to call the data by parish and transform the date to an appropiate format</p> In\u00a0[46]: Copied! <pre>def get_parish_data(parish_name, parish_folder):\n    parish_path = os.path.join(parish_folder, parish_name + '.xlsx')\n    parish = pd.read_excel(parish_path, sheet_name='Plague')\n    parish = parish[['ParishName', 'EndDate', 'CumDeaths']]\n    return parish\n</pre> def get_parish_data(parish_name, parish_folder):     parish_path = os.path.join(parish_folder, parish_name + '.xlsx')     parish = pd.read_excel(parish_path, sheet_name='Plague')     parish = parish[['ParishName', 'EndDate', 'CumDeaths']]     return parish <p>Fixing information and selecting only the columns the model needs</p> In\u00a0[47]: Copied! <pre>addparish = pd.concat([get_parish_data('YSTAD', southeast_parishes_folder)\n                       ,get_parish_data('OJA', southeast_parishes_folder)\n                       ,get_parish_data('HEDESKOGA', southeast_parishes_folder)\n                       ,get_parish_data('STORAKOPINGE', southeast_parishes_folder)]\n                       , ignore_index=True\n                        )\n#addparish = get_parish_data('YSTAD', southeast_parishes_folder)\naddparish\n</pre> addparish = pd.concat([get_parish_data('YSTAD', southeast_parishes_folder)                        ,get_parish_data('OJA', southeast_parishes_folder)                        ,get_parish_data('HEDESKOGA', southeast_parishes_folder)                        ,get_parish_data('STORAKOPINGE', southeast_parishes_folder)]                        , ignore_index=True                         ) #addparish = get_parish_data('YSTAD', southeast_parishes_folder) addparish Out[47]: ParishName EndDate CumDeaths 0 YSTAD Jun 1712 26 1 YSTAD Jul 1712 106 2 YSTAD Aug 1712 409 3 YSTAD Sep 1712 611 4 YSTAD Oct 1712 695 5 YSTAD Nov 1712 730 6 YSTAD Dec 1712 735 7 \u00d6JA May 1712 0 8 \u00d6JA Jun 1712 1 9 \u00d6JA Oct 1712 21 10 \u00d6JA Mar 1713 40 11 \u00d6JA Apr 1713 40 12 HEDESKOGA Aug 1712 0 13 HEDESKOGA Sep 1712 1 14 HEDESKOGA Oct 1712 5 15 HEDESKOGA Nov 1712 5 16 STORA K\u00d6PINGE Jun 1712 0 17 STORA K\u00d6PINGE Jul 1712 1 18 STORA K\u00d6PINGE Oct 1712 40 19 STORA K\u00d6PINGE Jan 1713 80 20 STORA K\u00d6PINGE Feb 1713 80 In\u00a0[48]: Copied! <pre>group = pd.merge(group1, addparish, on='ParishName', how='left')\ngroup\n</pre> group = pd.merge(group1, addparish, on='ParishName', how='left') group Out[48]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry centroid new_format_BeginPlaguePeriod new_format_EndPlaguePeriod EndDate CumDeaths 0 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 NaN NaN POLYGON ((4228840.232 3178726.042, 4228969.528... POINT (4230600.699862956 3177291.2599278623) 1712-07-01 NaT NaN NaN 1 SOUTHEAST BROMMA 154 AUG 1712 UNDEFINED NaN POLYGON ((4231996.049 3179728.504, 4232042.002... POINT (4234027.027249504 3179705.7627846766) 1712-04-01 1712-05-01 NaN NaN 2 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-01 Aug 1712 0.0 3 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-01 Sep 1712 1.0 4 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-01 Oct 1712 5.0 5 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-01 Nov 1712 5.0 6 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Jun 1712 26.0 7 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Jul 1712 106.0 8 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Aug 1712 409.0 9 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Sep 1712 611.0 10 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Oct 1712 695.0 11 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Nov 1712 730.0 12 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-01 Dec 1712 735.0 13 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 May 1712 0.0 14 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 Jun 1712 1.0 15 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 Oct 1712 21.0 16 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 Mar 1713 40.0 17 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-01 Apr 1713 40.0 18 SOUTHEAST GLEMMINGE 362 AUG 1712 AUG 1712 NaN POLYGON ((4246778.053 3180955.654, 4246924.466... POINT (4247370.639288611 3179086.3361054286) 1712-08-01 1712-08-01 NaN NaN 19 SOUTHEAST HAMMENH\u00d6G 169 AUG 1712 NaN NaN POLYGON ((4255090.885 3182137.710, 4254726.251... POINT (4253884.1828368595 3183416.9898560927) 1712-08-01 NaT NaN NaN 20 SOUTHEAST H\u00d6RUP 289 JUL 1712 NaN 60 POLYGON ((4252154.708 3177178.545, 4251691.166... POINT (4252336.724268622 3179160.5725134797) 1712-07-01 NaT NaN NaN 21 SOUTHEAST INGELSTORP 358 AUG 1712 NaN 1 POLYGON ((4246833.197 3177120.892, 4246902.701... POINT (4247499.939231565 3174682.3176888516) 1712-08-01 NaT NaN NaN 22 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 Jun 1712 0.0 23 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 Jul 1712 1.0 24 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 Oct 1712 40.0 25 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 Jan 1713 80.0 26 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... POINT (4242671.863855316 3178171.7113459506) 1712-07-01 1713-01-01 Feb 1713 80.0 27 SOUTHEAST VALLEBERGA 391 JUL 1712 NaN NaN POLYGON ((4249981.772 3179064.746, 4250047.408... POINT (4250097.829593883 3174762.056364127) 1712-07-01 NaT NaN NaN 28 SOUTHEAST \u00d6VRABY 208 SEP 1712 SEP 1712 NaN POLYGON ((4242987.654 3183129.802, 4242831.234... POINT (4242196.192299193 3183903.5739921844) 1712-09-01 1712-09-01 NaN NaN 29 SOUTHEAST \u00d6VRABY 208 NOV 1712 NOV 1712 NaN POLYGON ((4242987.654 3183129.802, 4242831.234... POINT (4242196.192299193 3183903.5739921844) 1712-11-01 1712-11-01 NaN NaN In\u00a0[49]: Copied! <pre>group = pd.merge(group1, addparish, on='ParishName', how='left')\n\n# # Fix the data for YSTAD\n# group.at[3,'BeginPlaguePeriod'] = 'JUN 1712'\n# group.at[3,'EndPlaguePeriod'] = 'JUN 1712'\n# group.at[3,'VictimsNumber'] = 26\n# group.at[4,'BeginPlaguePeriod'] = 'JUL 1712'\n# group.at[4,'EndPlaguePeriod'] = 'JUL 1712'\n# group.at[4,'VictimsNumber'] = 106\n# group.at[5,'BeginPlaguePeriod'] = 'AUG 1712'\n# group.at[5,'EndPlaguePeriod'] = 'AUG 1712'\n# group.at[5,'VictimsNumber'] = 409\n# group.at[6,'BeginPlaguePeriod'] = 'SEP 1712'\n# group.at[6,'EndPlaguePeriod'] = 'SEP 1712'\n# group.at[6,'VictimsNumber'] = 611\n# group.at[7,'BeginPlaguePeriod'] = 'OCT 1712'\n# group.at[7,'EndPlaguePeriod'] = 'OCT 1712'\n# group.at[7,'VictimsNumber'] = 695\n# group.at[8,'BeginPlaguePeriod'] = 'NOV 1712'\n# group.at[8,'EndPlaguePeriod'] = 'NOV 1712'\n# group.at[8,'VictimsNumber'] = 730\n# group.at[9,'BeginPlaguePeriod'] = 'DEC 1712'\n# group.at[9,'EndPlaguePeriod'] = 'DEC 1712'\n# group.at[9,'VictimsNumber'] = 735\n\n\n# Fixing the data for four parishes\n# Fix the data for HEDESKOGA\ngroup.at[2,'BeginPlaguePeriod'] = 'AUG 1712'\ngroup.at[2,'EndPlaguePeriod'] = 'AUG 1712'\ngroup.at[2,'VictimsNumber'] = 0\ngroup.at[3,'BeginPlaguePeriod'] = 'SEP 1712'\ngroup.at[3,'EndPlaguePeriod'] = 'SEP 1712'\ngroup.at[3,'VictimsNumber'] = 1\ngroup.at[4,'BeginPlaguePeriod'] = 'OCT 1712'\ngroup.at[4,'EndPlaguePeriod'] = 'OCT 1712'\ngroup.at[4,'VictimsNumber'] = 5\ngroup.at[5,'BeginPlaguePeriod'] = 'NOV 1712'\ngroup.at[5,'EndPlaguePeriod'] = 'NOV 1712'\ngroup.at[5,'VictimsNumber'] = 5\n\n# Fix the data for YSTAD\ngroup.at[6,'BeginPlaguePeriod'] = 'JUN 1712'\ngroup.at[6,'EndPlaguePeriod'] = 'JUN 1712'\ngroup.at[6,'VictimsNumber'] = 26\ngroup.at[7,'BeginPlaguePeriod'] = 'JUL 1712'\ngroup.at[7,'EndPlaguePeriod'] = 'JUL 1712'\ngroup.at[7,'VictimsNumber'] = 106\ngroup.at[8,'BeginPlaguePeriod'] = 'AUG 1712'\ngroup.at[8,'EndPlaguePeriod'] = 'AUG 1712'\ngroup.at[8,'VictimsNumber'] = 409\ngroup.at[9,'BeginPlaguePeriod'] = 'SEP 1712'\ngroup.at[9,'EndPlaguePeriod'] = 'SEP 1712'\ngroup.at[9,'VictimsNumber'] = 611\ngroup.at[10,'BeginPlaguePeriod'] = 'OCT 1712'\ngroup.at[10,'EndPlaguePeriod'] = 'OCT 1712'\ngroup.at[10,'VictimsNumber'] = 695\ngroup.at[11,'BeginPlaguePeriod'] = 'NOV 1712'\ngroup.at[11,'EndPlaguePeriod'] = 'NOV 1712'\ngroup.at[11,'VictimsNumber'] = 730\ngroup.at[12,'BeginPlaguePeriod'] = 'DEC 1712'\ngroup.at[12,'EndPlaguePeriod'] = 'DEC 1712'\ngroup.at[12,'VictimsNumber'] = 735\n\n# Fix the data for \u00d6JA\ngroup.at[13,'BeginPlaguePeriod'] = 'MAY 1712'\ngroup.at[13,'EndPlaguePeriod'] = 'MAY 1712'\ngroup.at[13,'VictimsNumber'] = 0\ngroup.at[14,'BeginPlaguePeriod'] = 'JUN 1712'\ngroup.at[14,'EndPlaguePeriod'] = 'JUN 1712'\ngroup.at[14,'VictimsNumber'] = 1\ngroup.at[15,'BeginPlaguePeriod'] = 'OCT 1712'\ngroup.at[15,'EndPlaguePeriod'] = 'OCT 1712'\ngroup.at[15,'VictimsNumber'] = 20\ngroup.at[16,'BeginPlaguePeriod'] = 'MAR 1713'\ngroup.at[16,'EndPlaguePeriod'] = 'MAR 1713'\ngroup.at[16,'VictimsNumber'] = 40\ngroup.at[17,'BeginPlaguePeriod'] = 'APR 1713'\ngroup.at[17,'EndPlaguePeriod'] = 'APR 1713'\ngroup.at[17,'VictimsNumber'] = 40\n\n# Fix the data for STORA K\u00d6PINGE\ngroup.at[22,'BeginPlaguePeriod'] = 'JUN 1712'\ngroup.at[22,'EndPlaguePeriod'] = 'JUN 1712'\ngroup.at[22,'VictimsNumber'] = 0\ngroup.at[23,'BeginPlaguePeriod'] = 'JUL 1712'\ngroup.at[23,'EndPlaguePeriod'] = 'JUL 1712'\ngroup.at[23,'VictimsNumber'] = 1\ngroup.at[24,'BeginPlaguePeriod'] = 'OCT 1712'\ngroup.at[24,'EndPlaguePeriod'] = 'OCT 1712'\ngroup.at[24,'VictimsNumber'] = 40\ngroup.at[25,'BeginPlaguePeriod'] = 'JAN 1713'\ngroup.at[25,'EndPlaguePeriod'] = 'JAN 1713'\ngroup.at[25,'VictimsNumber'] = 80\ngroup.at[26,'BeginPlaguePeriod'] = 'FEB 1713'\ngroup.at[26,'EndPlaguePeriod'] = 'FEB 1713'\ngroup.at[26,'VictimsNumber'] = 80\n\ngroup = group[['Region', 'ParishName', 'BEF1699', 'BeginPlaguePeriod'\n               , 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n</pre>           group = pd.merge(group1, addparish, on='ParishName', how='left')  # # Fix the data for YSTAD # group.at[3,'BeginPlaguePeriod'] = 'JUN 1712' # group.at[3,'EndPlaguePeriod'] = 'JUN 1712' # group.at[3,'VictimsNumber'] = 26 # group.at[4,'BeginPlaguePeriod'] = 'JUL 1712' # group.at[4,'EndPlaguePeriod'] = 'JUL 1712' # group.at[4,'VictimsNumber'] = 106 # group.at[5,'BeginPlaguePeriod'] = 'AUG 1712' # group.at[5,'EndPlaguePeriod'] = 'AUG 1712' # group.at[5,'VictimsNumber'] = 409 # group.at[6,'BeginPlaguePeriod'] = 'SEP 1712' # group.at[6,'EndPlaguePeriod'] = 'SEP 1712' # group.at[6,'VictimsNumber'] = 611 # group.at[7,'BeginPlaguePeriod'] = 'OCT 1712' # group.at[7,'EndPlaguePeriod'] = 'OCT 1712' # group.at[7,'VictimsNumber'] = 695 # group.at[8,'BeginPlaguePeriod'] = 'NOV 1712' # group.at[8,'EndPlaguePeriod'] = 'NOV 1712' # group.at[8,'VictimsNumber'] = 730 # group.at[9,'BeginPlaguePeriod'] = 'DEC 1712' # group.at[9,'EndPlaguePeriod'] = 'DEC 1712' # group.at[9,'VictimsNumber'] = 735   # Fixing the data for four parishes # Fix the data for HEDESKOGA group.at[2,'BeginPlaguePeriod'] = 'AUG 1712' group.at[2,'EndPlaguePeriod'] = 'AUG 1712' group.at[2,'VictimsNumber'] = 0 group.at[3,'BeginPlaguePeriod'] = 'SEP 1712' group.at[3,'EndPlaguePeriod'] = 'SEP 1712' group.at[3,'VictimsNumber'] = 1 group.at[4,'BeginPlaguePeriod'] = 'OCT 1712' group.at[4,'EndPlaguePeriod'] = 'OCT 1712' group.at[4,'VictimsNumber'] = 5 group.at[5,'BeginPlaguePeriod'] = 'NOV 1712' group.at[5,'EndPlaguePeriod'] = 'NOV 1712' group.at[5,'VictimsNumber'] = 5  # Fix the data for YSTAD group.at[6,'BeginPlaguePeriod'] = 'JUN 1712' group.at[6,'EndPlaguePeriod'] = 'JUN 1712' group.at[6,'VictimsNumber'] = 26 group.at[7,'BeginPlaguePeriod'] = 'JUL 1712' group.at[7,'EndPlaguePeriod'] = 'JUL 1712' group.at[7,'VictimsNumber'] = 106 group.at[8,'BeginPlaguePeriod'] = 'AUG 1712' group.at[8,'EndPlaguePeriod'] = 'AUG 1712' group.at[8,'VictimsNumber'] = 409 group.at[9,'BeginPlaguePeriod'] = 'SEP 1712' group.at[9,'EndPlaguePeriod'] = 'SEP 1712' group.at[9,'VictimsNumber'] = 611 group.at[10,'BeginPlaguePeriod'] = 'OCT 1712' group.at[10,'EndPlaguePeriod'] = 'OCT 1712' group.at[10,'VictimsNumber'] = 695 group.at[11,'BeginPlaguePeriod'] = 'NOV 1712' group.at[11,'EndPlaguePeriod'] = 'NOV 1712' group.at[11,'VictimsNumber'] = 730 group.at[12,'BeginPlaguePeriod'] = 'DEC 1712' group.at[12,'EndPlaguePeriod'] = 'DEC 1712' group.at[12,'VictimsNumber'] = 735  # Fix the data for \u00d6JA group.at[13,'BeginPlaguePeriod'] = 'MAY 1712' group.at[13,'EndPlaguePeriod'] = 'MAY 1712' group.at[13,'VictimsNumber'] = 0 group.at[14,'BeginPlaguePeriod'] = 'JUN 1712' group.at[14,'EndPlaguePeriod'] = 'JUN 1712' group.at[14,'VictimsNumber'] = 1 group.at[15,'BeginPlaguePeriod'] = 'OCT 1712' group.at[15,'EndPlaguePeriod'] = 'OCT 1712' group.at[15,'VictimsNumber'] = 20 group.at[16,'BeginPlaguePeriod'] = 'MAR 1713' group.at[16,'EndPlaguePeriod'] = 'MAR 1713' group.at[16,'VictimsNumber'] = 40 group.at[17,'BeginPlaguePeriod'] = 'APR 1713' group.at[17,'EndPlaguePeriod'] = 'APR 1713' group.at[17,'VictimsNumber'] = 40  # Fix the data for STORA K\u00d6PINGE group.at[22,'BeginPlaguePeriod'] = 'JUN 1712' group.at[22,'EndPlaguePeriod'] = 'JUN 1712' group.at[22,'VictimsNumber'] = 0 group.at[23,'BeginPlaguePeriod'] = 'JUL 1712' group.at[23,'EndPlaguePeriod'] = 'JUL 1712' group.at[23,'VictimsNumber'] = 1 group.at[24,'BeginPlaguePeriod'] = 'OCT 1712' group.at[24,'EndPlaguePeriod'] = 'OCT 1712' group.at[24,'VictimsNumber'] = 40 group.at[25,'BeginPlaguePeriod'] = 'JAN 1713' group.at[25,'EndPlaguePeriod'] = 'JAN 1713' group.at[25,'VictimsNumber'] = 80 group.at[26,'BeginPlaguePeriod'] = 'FEB 1713' group.at[26,'EndPlaguePeriod'] = 'FEB 1713' group.at[26,'VictimsNumber'] = 80  group = group[['Region', 'ParishName', 'BEF1699', 'BeginPlaguePeriod'                , 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]  <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[50]: Copied! <pre>group = group1\nlen(group)\n</pre> group = group1 len(group) Out[50]: <pre>13</pre> In\u00a0[51]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\ngroup = group.replace(['UNDEFINED', '?'], np.nan)\ngroup['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    group['BeginPlaguePeriod'], format='%b %Y')\ngroup['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    group['EndPlaguePeriod'], format='%b %Y')\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe group = group.replace(['UNDEFINED', '?'], np.nan) group['new_format_BeginPlaguePeriod'] = pd.to_datetime(     group['BeginPlaguePeriod'], format='%b %Y') group['new_format_EndPlaguePeriod'] = pd.to_datetime(     group['EndPlaguePeriod'], format='%b %Y') In\u00a0[52]: Copied! <pre>parishes_complete_data =group[(group['EndPlaguePeriod'].notna()) &amp; (group['VictimsNumber'].notna())]\n</pre> parishes_complete_data =group[(group['EndPlaguePeriod'].notna()) &amp; (group['VictimsNumber'].notna())] In\u00a0[53]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n# # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0\n# cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']\n# cluster1\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) # # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0 # cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699'] # cluster1  In\u00a0[54]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n# # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0\n# cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']\ncluster1.columns\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) # # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0 # cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699'] cluster1.columns Out[54]: <pre>Index(['Region', 'ParishName', 'BEF1699', 'BeginPlaguePeriod',\n       'EndPlaguePeriod', 'VictimsNumber', 'geometry', 'centroid',\n       'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod',\n       'BeginDaysPlague', 'EndDaysPlague'],\n      dtype='object')</pre> In\u00a0[55]: Copied! <pre>cluster1 = cluster1[['Region'\n                     ,'ParishName'\n                     , 'BEF1699'\n                     , 'BeginPlaguePeriod'\n                     ,'EndPlaguePeriod'\n                     , 'VictimsNumber'\n                     ,'new_format_BeginPlaguePeriod'\n                     , 'new_format_EndPlaguePeriod'\n                     , 'BeginDaysPlague', 'EndDaysPlague']]\ncluster1\n</pre> cluster1 = cluster1[['Region'                      ,'ParishName'                      , 'BEF1699'                      , 'BeginPlaguePeriod'                      ,'EndPlaguePeriod'                      , 'VictimsNumber'                      ,'new_format_BeginPlaguePeriod'                      , 'new_format_EndPlaguePeriod'                      , 'BeginDaysPlague', 'EndDaysPlague']] cluster1 Out[55]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber new_format_BeginPlaguePeriod new_format_EndPlaguePeriod BeginDaysPlague EndDaysPlague 0 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740.0 1712-06-01 1712-12-01 0 213 1 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40.0 1712-06-01 1713-03-01 0 303 2 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 NaN NaN 1712-07-01 NaT 30 0 3 SOUTHEAST H\u00d6RUP 289 JUL 1712 NaN 60.0 1712-07-01 NaT 30 0 4 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80.0 1712-07-01 1713-01-01 30 244 5 SOUTHEAST VALLEBERGA 391 JUL 1712 NaN NaN 1712-07-01 NaT 30 0 6 SOUTHEAST BROMMA 154 AUG 1712 NaN NaN 1712-08-01 NaT 61 0 7 SOUTHEAST GLEMMINGE 362 AUG 1712 AUG 1712 NaN 1712-08-01 1712-08-01 61 91 8 SOUTHEAST HAMMENH\u00d6G 169 AUG 1712 NaN NaN 1712-08-01 NaT 61 0 9 SOUTHEAST INGELSTORP 358 AUG 1712 NaN 1.0 1712-08-01 NaT 61 0 10 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5.0 1712-09-01 1712-10-01 92 152 11 SOUTHEAST \u00d6VRABY 208 SEP 1712 SEP 1712 NaN 1712-09-01 1712-09-01 92 121 12 SOUTHEAST \u00d6VRABY 208 NOV 1712 NOV 1712 NaN 1712-11-01 1712-11-01 153 182 <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[56]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf, beta_guess:float, mu_guess:float):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        self.mu = np.full(self.n, mu_guess)\n        self.beta = np.full(self.n, beta_guess)\n\n        self.S0 = np.zeros(self.n)       \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].unique()\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()  \n\n    def p_coeff(self, p_guess:float):\n        p_coeff = np.full((self.n, self.n), p_guess)\n        np.fill_diagonal(p_coeff, 0)\n        return p_coeff\n</pre> class Initial_Model:     def __init__(self, gdf, beta_guess:float, mu_guess:float):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         self.mu = np.full(self.n, mu_guess)         self.beta = np.full(self.n, beta_guess)          self.S0 = np.zeros(self.n)                for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].unique()      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()        def p_coeff(self, p_guess:float):         p_coeff = np.full((self.n, self.n), p_guess)         np.fill_diagonal(p_coeff, 0)         return p_coeff       <p>Generating the differential equations</p> In\u00a0[57]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[58]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # ensure beta is a numpy array of shape n\n    mu = parameters['mu'] # ensure mu is a numpy array of shape n\n    p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    # Ensure p_coeff is symmetric\n    #p_coeff = np.triu(p_coeff,1) + np.tril(p_coeff, -1)\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n\n    beta_matrix =  transmission_matrix_beta(gdf, beta)\n    p_matrix = transmission_matrix2_p(gdf, p_coeff)\n\n    # trans_matrix = total_transmission_matrix(gdf, beta, p_coeff)\n\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + (beta_matrix + p_matrix) \n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n\n    # print('shape entry', entry.shape)\n    # print('shape entry2', entry[:, 2].shape)\n    # print('shape mu', mu.shape)\n    # x = (gamma * (1 - mu))\n    # print('shape x', x.shape)\n    # # PRINT THE SHAPES FOR EACH DERIVATIVE\n    # print('dS', dS.shape, dS)\n    # print('dE', dE.shape, dE)\n    # print('dI', dI.shape, dI)\n    # print('dR', dR.shape, dR)\n    # print('dD', dD.shape, dD)\n\n    \n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # ensure beta is a numpy array of shape n     mu = parameters['mu'] # ensure mu is a numpy array of shape n     p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      # Ensure p_coeff is symmetric     #p_coeff = np.triu(p_coeff,1) + np.tril(p_coeff, -1)      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])       beta_matrix =  transmission_matrix_beta(gdf, beta)     p_matrix = transmission_matrix2_p(gdf, p_coeff)      # trans_matrix = total_transmission_matrix(gdf, beta, p_coeff)       # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + (beta_matrix + p_matrix)       sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)       dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]      # print('shape entry', entry.shape)     # print('shape entry2', entry[:, 2].shape)     # print('shape mu', mu.shape)     # x = (gamma * (1 - mu))     # print('shape x', x.shape)     # # PRINT THE SHAPES FOR EACH DERIVATIVE     # print('dS', dS.shape, dS)     # print('dE', dE.shape, dE)     # print('dI', dI.shape, dI)     # print('dR', dR.shape, dR)     # print('dD', dD.shape, dD)           derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Trying a small dataframe</p> In\u00a0[59]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\nexample1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n</pre> # Getting the centroid of each polygon for defining the transmission matrix example1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         )  In\u00a0[61]: Copied! <pre>k = 3\n# Selecting specific rows from the dataframe reseting the index\nexample = southScania[(southScania['ParishName'] == 'YSTAD')\n                   | (southScania['ParishName'] == '\u00d6JA')\n                   | (southScania['ParishName'] == 'HEDESKOGA')].reset_index(drop=True)\n# Getting the centroid of each polygon for defining the transmission matrix\nexample = get_centroid(add_Begin_End_days(sort_by_date(example)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\nmodel_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5)\n</pre> k = 3 # Selecting specific rows from the dataframe reseting the index example = southScania[(southScania['ParishName'] == 'YSTAD')                    | (southScania['ParishName'] == '\u00d6JA')                    | (southScania['ParishName'] == 'HEDESKOGA')].reset_index(drop=True) # Getting the centroid of each polygon for defining the transmission matrix example = get_centroid(add_Begin_End_days(sort_by_date(example)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5) In\u00a0[162]: Copied! <pre>gato = add_Begin_End_days(sort_by_date(southScania)\n                                 , 'new_format_BeginPlaguePeriod'\n                                 , 'new_format_EndPlaguePeriod'\n                  )\ngato.loc[90]\n</pre> gato = add_Begin_End_days(sort_by_date(southScania)                                  , 'new_format_BeginPlaguePeriod'                                  , 'new_format_EndPlaguePeriod'                   ) gato.loc[90] Out[162]: <pre>Region                                                                  SOUTHEAST\nParishName                                                             F\u00c5GELTOFTA\nBEF1699                                                                       454\nBeginPlaguePeriod                                                             NaN\nEndPlaguePeriod                                                               NaN\nVictimsNumber                                                                 NaN\ngeometry                        POLYGON ((4244692.077621438 3203779.020550321,...\ncentroid                             POINT (4245058.011230637 3200505.8082386227)\nnew_format_BeginPlaguePeriod                                                  NaT\nnew_format_EndPlaguePeriod                                                    NaT\nBeginDaysPlague                                                                 0\nEndDaysPlague                                                                   0\nName: 90, dtype: object</pre> In\u00a0[207]: Copied! <pre># Get only the rows with a valid date in BeginPlaguePeriod. NaN values are ignored\ngato = gato[gato['new_format_BeginPlaguePeriod'].notna()]\n</pre> # Get only the rows with a valid date in BeginPlaguePeriod. NaN values are ignored gato = gato[gato['new_format_BeginPlaguePeriod'].notna()]   In\u00a0[228]: Copied! <pre>maxDays(cluster1, 'EndDaysPlague')\n</pre> maxDays(cluster1, 'EndDaysPlague') Out[228]: <pre>303</pre> In\u00a0[185]: Copied! <pre># Get only the names of the parishes that appear more than once in the dataframe\n# and store them in a list\nparish_names = gato['ParishName'].value_counts()[gato['ParishName'].value_counts() &gt; 1].index.tolist()\nparish_names\n</pre> # Get only the names of the parishes that appear more than once in the dataframe # and store them in a list parish_names = gato['ParishName'].value_counts()[gato['ParishName'].value_counts() &gt; 1].index.tolist() parish_names Out[185]: <pre>['\u00d6VRABY', 'R\u00d6DDINGE', 'GLEMMINGE']</pre> In\u00a0[229]: Copied! <pre>matrix = gravitational_matrix(cluster1)\nmatrix = pd.DataFrame(matrix)\nmatrix.to_csv('matrix_yellow_group.csv', index=False)\n</pre> matrix = gravitational_matrix(cluster1) matrix = pd.DataFrame(matrix) matrix.to_csv('matrix_yellow_group.csv', index=False) In\u00a0[231]: Copied! <pre>matrix\n</pre> matrix Out[231]: 0 1 2 3 4 5 6 7 8 9 10 11 12 0 0.000000 0.053325 0.022200 0.001825 0.012918 0.003430 0.014524 0.004511 0.000787 0.004684 0.025426 0.003498 0.003498 1 0.053325 0.000000 0.001857 0.000172 0.001388 0.000298 0.003256 0.000446 0.000077 0.000400 0.002267 0.000459 0.000459 2 0.022200 0.001857 0.000000 0.000228 0.000965 0.000380 0.003296 0.000479 0.000110 0.000460 0.009195 0.000439 0.000439 3 0.001825 0.000172 0.000228 0.000000 0.001151 0.004639 0.000133 0.004241 0.002381 0.002381 0.000114 0.000480 0.000480 4 0.012918 0.001388 0.000965 0.001151 0.000000 0.002202 0.000751 0.005940 0.000415 0.003793 0.000598 0.002364 0.002364 5 0.003430 0.000298 0.000380 0.004639 0.002202 0.000000 0.000213 0.005415 0.000740 0.020721 0.000195 0.000557 0.000557 6 0.014524 0.003256 0.003296 0.000133 0.000751 0.000213 0.000000 0.000312 0.000064 0.000267 0.003463 0.000380 0.000380 7 0.004511 0.000446 0.000479 0.004241 0.005940 0.005415 0.000312 0.000000 0.001000 0.006676 0.000258 0.001507 0.001507 8 0.000787 0.000077 0.000110 0.002381 0.000415 0.000740 0.000064 0.001000 0.000000 0.000517 0.000053 0.000257 0.000257 9 0.004684 0.000400 0.000460 0.002381 0.003793 0.020721 0.000267 0.006676 0.000517 0.000000 0.000246 0.000658 0.000658 10 0.025426 0.002267 0.009195 0.000114 0.000598 0.000195 0.003463 0.000258 0.000053 0.000246 0.000000 0.000243 0.000243 11 0.003498 0.000459 0.000439 0.000480 0.002364 0.000557 0.000380 0.001507 0.000257 0.000658 0.000243 0.000000 0.000000 12 0.003498 0.000459 0.000439 0.000480 0.002364 0.000557 0.000380 0.001507 0.000257 0.000658 0.000243 0.000000 0.000000 In\u00a0[167]: Copied! <pre>gato.to_csv('allSouthScania.csv', index=False)\n</pre> gato.to_csv('allSouthScania.csv', index=False) In\u00a0[131]: Copied! <pre>grav_matrix = gravitational_coeff(gato)\n</pre> grav_matrix = gravitational_coeff(gato)   In\u00a0[172]: Copied! <pre>grav_matrix = pd.DataFrame(grav_matrix)\ngrav_matrix.head(5)\n</pre> grav_matrix = pd.DataFrame(grav_matrix) grav_matrix.head(5)  Out[172]: 0 1 2 3 4 5 6 7 8 9 ... 73 74 75 76 77 78 79 80 81 82 0 0.000000 0.000015 0.000052 0.000395 0.000024 0.000061 0.000011 0.000047 0.000081 0.000012 ... 0.000010 0.000034 0.000039 0.000027 0.000009 0.000027 0.000081 0.000042 0.000014 0.000158 1 0.000015 0.000000 0.000242 0.000100 0.000007 0.000017 0.000380 0.000016 0.000144 0.000029 ... 0.000007 0.000494 0.000148 0.000473 0.000009 0.000473 0.000463 0.000498 0.004833 0.000123 2 0.000052 0.000242 0.000000 0.000342 0.000016 0.000040 0.000095 0.000036 0.000193 0.000036 ... 0.000012 0.000903 0.001495 0.000466 0.000016 0.000654 0.001152 0.000498 0.000249 0.000324 3 0.000395 0.000100 0.000342 0.000000 0.000058 0.000429 0.000070 0.000122 0.000492 0.000062 ... 0.000039 0.000286 0.000356 0.000147 0.000032 0.000154 0.000855 0.000390 0.000093 0.004883 4 0.000024 0.000007 0.000016 0.000058 0.000000 0.000028 0.000006 0.004292 0.000081 0.000014 ... 0.000031 0.000012 0.000011 0.000013 0.000016 0.000012 0.000027 0.000017 0.000007 0.000044 <p>5 rows \u00d7 83 columns</p> In\u00a0[177]: Copied! <pre># check the size of the matrix\ngrav_matrix.shape\n</pre> # check the size of the matrix grav_matrix.shape Out[177]: <pre>(83, 83)</pre> In\u00a0[133]: Copied! <pre>grav_matrix.to_csv('grav_matrix_infect_SouthScania.csv', index=False)\n</pre> grav_matrix.to_csv('grav_matrix_infect_SouthScania.csv', index=False) In\u00a0[30]: Copied! <pre># # Selecting two parishes to test the model\n# #example = cluster1[cluster1['ParishName']=='\u00d6JA']\n# example = cluster1[cluster1['ParishName'].isin(['YSTAD','\u00d6JA'])]\n# example = cluster1\n# model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5)\n</pre> # # Selecting two parishes to test the model # #example = cluster1[cluster1['ParishName']=='\u00d6JA'] # example = cluster1[cluster1['ParishName'].isin(['YSTAD','\u00d6JA'])] # example = cluster1 # model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5) In\u00a0[62]: Copied! <pre>Model_test = {'model': SEIRD_model,\n              'init': {\n                  'S': model_input.S0,\n                  'E': model_input.E0,\n                  'I': model_input.I0,\n                  'R': model_input.R0,\n                  'D': model_input.D0,\n              },  # defining the initial values for the model\n              'gdf': example,  # defining the graph\n              'beta': model_input.beta,\n              'p_coeff': model_input.p_coeff(p_guess=0.5),\n              'bump_center': 0.0,\n              'bump_width': 0.0,\n              'bump_height': 0.0,\n              'gamma': 0.4,\n              'sigma': 0.17,\n              'mu': model_input.mu,\n              'N': model_input.patchPop(),\n              'n': model_input.n,\n              'T': model_input.maxDays()}\n\nmodel_dict = generate_sol(Model_test)\nmodel_dict['D'][0][120]\n</pre> Model_test = {'model': SEIRD_model,               'init': {                   'S': model_input.S0,                   'E': model_input.E0,                   'I': model_input.I0,                   'R': model_input.R0,                   'D': model_input.D0,               },  # defining the initial values for the model               'gdf': example,  # defining the graph               'beta': model_input.beta,               'p_coeff': model_input.p_coeff(p_guess=0.5),               'bump_center': 0.0,               'bump_width': 0.0,               'bump_height': 0.0,               'gamma': 0.4,               'sigma': 0.17,               'mu': model_input.mu,               'N': model_input.patchPop(),               'n': model_input.n,               'T': model_input.maxDays()}  model_dict = generate_sol(Model_test) model_dict['D'][0][120] Out[62]: <pre>67.2106847671744</pre> In\u00a0[63]: Copied! <pre>%matplotlib inline\n\n# Set up the data to fit\nbeginTime = example['BeginDaysPlague'].values\nendTime = example['EndDaysPlague'].values\ndeathData = example['VictimsNumber'].values\n\n# Number of patches\nn = Model_test['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ngato = count_infected_parishes_by_month(example,'JUN 1712',0 )\ntick_positions = gato['DaysFromInitialDate'].values\ntick_labels = gato['date'].apply(\n     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Plot model solution D for each patch\nfor i in range(n):\n    axes[i].plot(model_dict['D'][i],\n                color='orange', label=(model_input.patchNames()[i]))\n    axes[i].axhline(y=1, color='blue', linestyle='--')\n    axes[i].set_ylabel('Cumulative Deaths')\n    axes[i].legend(loc='lower right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n    \n    # if deathData[i] != 0 and endTime[i] != 0:\n    #     initial_position = beginTime[i]\n    #     final_position = endTime[i]\n    #     axes[i].plot(initial_position, 0, 'bo')\n    #     axes[i].plot(final_position,\n    #                  deathData[i], 'bo')\n    #     axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))\n    #     axes[i].set_ylabel('Cumulative Deaths')\n       \n    # else:\n    #     axes[i].plot(model_dict['D'][i],\n    #                  color='orange', label=(model_input.patchNames()[i]))\n    #     axes[i].set_ylabel('Cumulative Deaths')\n    #     axes[i].legend(loc='lower right')\n        \n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Set up the data to fit beginTime = example['BeginDaysPlague'].values endTime = example['EndDaysPlague'].values deathData = example['VictimsNumber'].values  # Number of patches n = Model_test['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  gato = count_infected_parishes_by_month(example,'JUN 1712',0 ) tick_positions = gato['DaysFromInitialDate'].values tick_labels = gato['date'].apply(      lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Plot model solution D for each patch for i in range(n):     axes[i].plot(model_dict['D'][i],                 color='orange', label=(model_input.patchNames()[i]))     axes[i].axhline(y=1, color='blue', linestyle='--')     axes[i].set_ylabel('Cumulative Deaths')     axes[i].legend(loc='lower right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)          # if deathData[i] != 0 and endTime[i] != 0:     #     initial_position = beginTime[i]     #     final_position = endTime[i]     #     axes[i].plot(initial_position, 0, 'bo')     #     axes[i].plot(final_position,     #                  deathData[i], 'bo')     #     axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))     #     axes[i].set_ylabel('Cumulative Deaths')             # else:     #     axes[i].plot(model_dict['D'][i],     #                  color='orange', label=(model_input.patchNames()[i]))     #     axes[i].set_ylabel('Cumulative Deaths')     #     axes[i].legend(loc='lower right')          # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Defining the objective functions</p> In\u00a0[65]: Copied! <pre># Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n\ndef objectiveFunction_2 (gdf: gpd.GeoDataFrame = example\n                         , column_name: str = 'ParishName'\n                         , n: int = 0):\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Initializing the number of infected parishes per month for the model's output\n    infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_infected_parishes_by_month))\n    # Computing the total number of parishes in the dataframe without repetitions\n    total_parishes = len(grouped_by_parish)\n\n    # Computing the number of infected parishes per month from the model's output\n    for i in range(len(cum_infected_parishes_by_month)):\n        init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n        final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n        \n        for k in range(len(grouped_by_parish)):\n            for day in range(init_days, final_days):\n                if model_dict['D'][k][day] &gt;= 1:\n                    infected_parishes[i] += 1\n                    break # Breaks the innermost loop when the condition is met\n        error[i] = (infected_parishes[i] \n                           - cum_infected_parishes_by_month['CumInfectParishes'][i])**2\n    \n    # Computing the error between the model's output and the data\n    total_error = 1/len(cum_infected_parishes_by_month)*(1/total_parishes) * (np.sum(error))\n          \n    return (total_error)\n</pre> # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data  def objectiveFunction_2 (gdf: gpd.GeoDataFrame = example                          , column_name: str = 'ParishName'                          , n: int = 0):     #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Initializing the number of infected parishes per month for the model's output     infected_parishes = np.zeros(len(cum_infected_parishes_by_month))     # Initializing the error between the model's output and the data     error = np.zeros(len(cum_infected_parishes_by_month))     # Computing the total number of parishes in the dataframe without repetitions     total_parishes = len(grouped_by_parish)      # Computing the number of infected parishes per month from the model's output     for i in range(len(cum_infected_parishes_by_month)):         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']                  for k in range(len(grouped_by_parish)):             for day in range(init_days, final_days):                 if model_dict['D'][k][day] &gt;= 1:                     infected_parishes[i] += 1                     break # Breaks the innermost loop when the condition is met         error[i] = (infected_parishes[i]                             - cum_infected_parishes_by_month['CumInfectParishes'][i])**2          # Computing the error between the model's output and the data     total_error = 1/len(cum_infected_parishes_by_month)*(1/total_parishes) * (np.sum(error))                return (total_error)  In\u00a0[66]: Copied! <pre>objectiveFunction_2(example, 'ParishName', 0)\n</pre> objectiveFunction_2(example, 'ParishName', 0) Out[66]: <pre>0.1</pre> In\u00a0[811]: Copied! <pre># Function to calculate the abs error between the cumulative number of deaths per month between the model and the data\n\ndef objectiveFunction_3 (gdf: gpd.GeoDataFrame = example\n                         , column_name: str = 'ParishName'):\n\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    \n    # Getting the number of deaths per month from the data\n    cum_deaths_by_month = count_victims_by_month(gdf)\n\n    # Initializing the cum. number of deaths per month for the model's output\n    cum_deaths_month = np.zeros(len(cum_deaths_by_month))\n\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_deaths_by_month))\n    \n    # Computing the number of cum. deaths per month from the model's output\n    for i in range(len(cum_deaths_by_month)):\n        day = cum_deaths_by_month['CumDays'][i]\n        data = cum_deaths_by_month['CumDeaths'][i]/cum_deaths_by_month['CumPop'][i]\n\n        for k in range(len(grouped_by_parish)):\n            pop_k = gdf.loc[k, 'BEF1699']\n            cum_deaths_month[i] += model_dict['D'][k][day]/pop_k\n\n        error[i] = np.abs(cum_deaths_month[i] \n                           - data)\n        \n    # Computing the error between the model's output and the data\n    total_error = 1/len(error)*(np.sum(error))\n          \n    return ( cum_deaths_month\n            , error\n            ,np.sum(error)\n            , total_error)\n</pre> # Function to calculate the abs error between the cumulative number of deaths per month between the model and the data  def objectiveFunction_3 (gdf: gpd.GeoDataFrame = example                          , column_name: str = 'ParishName'):      #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)          # Getting the number of deaths per month from the data     cum_deaths_by_month = count_victims_by_month(gdf)      # Initializing the cum. number of deaths per month for the model's output     cum_deaths_month = np.zeros(len(cum_deaths_by_month))      # Initializing the error between the model's output and the data     error = np.zeros(len(cum_deaths_by_month))          # Computing the number of cum. deaths per month from the model's output     for i in range(len(cum_deaths_by_month)):         day = cum_deaths_by_month['CumDays'][i]         data = cum_deaths_by_month['CumDeaths'][i]/cum_deaths_by_month['CumPop'][i]          for k in range(len(grouped_by_parish)):             pop_k = gdf.loc[k, 'BEF1699']             cum_deaths_month[i] += model_dict['D'][k][day]/pop_k          error[i] = np.abs(cum_deaths_month[i]                             - data)              # Computing the error between the model's output and the data     total_error = 1/len(error)*(np.sum(error))                return ( cum_deaths_month             , error             ,np.sum(error)             , total_error)          <p>Defining the optimization problem:</p> In\u00a0[814]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example\n                      , init_date_column: str = 'BeginPlaguePeriod'\n                      , column_name: str = 'ParishName'\n                      , beginTime: str = 'BeginDaysPlague'\n                      , endTime: str = 'EndDaysPlague'\n                      , deathData: str = 'VictimsNumber'\n                      #\n                      ):\n    parameters = np.array(parameters)\n\n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n].reshape(n,)\n    mu:  np.array = parameters[n:2*n].reshape(n,)\n    p_coeff: np.array = parameters[2*n:].reshape(n, n)\n\n    # Penalize if p_coeff is not symmetric or has non-zero diagonal elements\n    if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):\n        return 1e50\n\n    # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)\n    # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p_coeff': p_coeff,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Groupby operation\n    grouped_by_parish = gdf.groupby(column_name)\n    errors = np.zeros(n)\n\n    for i, (current_df) in enumerate(grouped_by_parish):\n        len_data_parish = len(current_df)\n        if len_data_parish &lt; 2:\n            #pop_size = current_df[popData].values[0]\n            initial_position = current_df[beginTime].values[0]\n            final_position = current_df[endTime].values[0]\n            deaths = current_df[deathData].values[0]\n            # Defining the error function using the mean absolute error\n            errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            # errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)\n            if (final_position != 0 and deaths != 0):\n                errors[i] = (errors[i] + abs(model_sol['D'][i]\n                             [final_position] - deaths))\n        else:\n            errors[i] = 1000000\n            #pop_size = current_df[popData].values\n            # position = current_df[endTime].values\n            # monthly_deaths = current_df[deathData].values\n            # point_error = abs(\n            #     model_sol['D'][i][position] - monthly_deaths)\n            # errors[i] = np.sum(point_error)\n    totalError = np.sum(errors)/len(grouped_by_parish)\n    return totalError\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example                       , init_date_column: str = 'BeginPlaguePeriod'                       , column_name: str = 'ParishName'                       , beginTime: str = 'BeginDaysPlague'                       , endTime: str = 'EndDaysPlague'                       , deathData: str = 'VictimsNumber'                       #                       ):     parameters = np.array(parameters)      n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n].reshape(n,)     mu:  np.array = parameters[n:2*n].reshape(n,)     p_coeff: np.array = parameters[2*n:].reshape(n, n)      # Penalize if p_coeff is not symmetric or has non-zero diagonal elements     if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):         return 1e50      # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p_coeff': p_coeff,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Groupby operation     grouped_by_parish = gdf.groupby(column_name)     errors = np.zeros(n)      for i, (current_df) in enumerate(grouped_by_parish):         len_data_parish = len(current_df)         if len_data_parish &lt; 2:             #pop_size = current_df[popData].values[0]             initial_position = current_df[beginTime].values[0]             final_position = current_df[endTime].values[0]             deaths = current_df[deathData].values[0]             # Defining the error function using the mean absolute error             errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             # errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)             if (final_position != 0 and deaths != 0):                 errors[i] = (errors[i] + abs(model_sol['D'][i]                              [final_position] - deaths))         else:             errors[i] = 1000000             #pop_size = current_df[popData].values             # position = current_df[endTime].values             # monthly_deaths = current_df[deathData].values             # point_error = abs(             #     model_sol['D'][i][position] - monthly_deaths)             # errors[i] = np.sum(point_error)     totalError = np.sum(errors)/len(grouped_by_parish)     return totalError   In\u00a0[\u00a0]: Copied! <pre>n = model_input.n\n\n# bounds = [(0.0, 1.0)]*(2*n + n**2)\nbeta_bounds = [(0.0, 1.0)]*n\nmu_bounds = [(0.1, 0.8)]*n\np_bounds = [(0.0, 0.1)]*n**2\nbounds = beta_bounds + mu_bounds + p_bounds\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p_coeff(p_guess=0.3)\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)\ninitial_parameters_list = initial_parameters.tolist()\n\nresult = gp_minimize(objectiveFunction, bounds, n_calls=40, random_state=0)\nparameters_estimated = np.array(result.x)\nbeta_estimated = parameters_estimated[:n].reshape(n,)\nmu_estimated = parameters_estimated[n:2*n].reshape(n,)\np_estimated = parameters_estimated[2*n:].reshape(n, n)\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre>  n = model_input.n  # bounds = [(0.0, 1.0)]*(2*n + n**2) beta_bounds = [(0.0, 1.0)]*n mu_bounds = [(0.1, 0.8)]*n p_bounds = [(0.0, 0.1)]*n**2 bounds = beta_bounds + mu_bounds + p_bounds beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p_coeff(p_guess=0.3) initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None) initial_parameters_list = initial_parameters.tolist()  result = gp_minimize(objectiveFunction, bounds, n_calls=40, random_state=0) parameters_estimated = np.array(result.x) beta_estimated = parameters_estimated[:n].reshape(n,) mu_estimated = parameters_estimated[n:2*n].reshape(n,) p_estimated = parameters_estimated[2*n:].reshape(n, n)  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <p>Substituting the estimated values into the model and solving it</p> In\u00a0[265]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[266]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            # axes[i].plot(initial_position, 0, 'bo')\n            # axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            # axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             # axes[i].plot(initial_position, 0, 'bo')             # axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             # axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Plotting the daily deaths by parish</p> In\u00a0[618]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[619]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\n# tick_positions = southeastScania['BeginDaysPlague'].values\n# tick_labels = southeastScania['BeginPlaguePeriod'].apply(\n#     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  # tick_positions = southeastScania['BeginDaysPlague'].values # tick_labels = southeastScania['BeginPlaguePeriod'].apply( #     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[267]: Copied! <pre># Set up the data to fit\nn = model_input.n\n\n# # Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p_coeff(p_guess=0.3)\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]*len(beta_guess.flatten())\nmu_bounds = [(0.1,0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p\n\n# Concatenate the bounds\nbounds = beta_bounds + mu_bounds + p_bounds\n# def set_constraint(x):\n#     p = x[2*n:].reshape(n, n)\n#     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n#     #[p[i,j] - p[j,i] == 0 for i in range(n) for j in range(n)] \n#     for i in range(n):\n#         for j in range(n):\n#             if p[i,j] - p[j,i] != 0:\n#                return 1\n#     return 0\n\n# def set_constraint(x):\n#     p = x[2*n:].reshape(n, n)\n#     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n#     diff_matrix = np.abs(p - p.T)\n#     if np.any(diff_matrix != 0):\n#         return 1\n#     return 0\n\n# Minimize the objective function to obtain beta, mu, and p\nresult = optimize.minimize(objectiveFunction, x0=initial_parameters\n                           , args=(example, 0, 'BeginPlaguePeriod', 'ParishName', 'BeginDaysPlague', 'EndDaysPlague', 'VictimsNumber')\n                           , bounds=bounds\n                           # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n                           #, constraints = ({'type': 'eq', 'fun': set_constraint})\n\n\n                           # Constraint for checking that the vector p after a transformation gives a symmetric matrix \n                           #, constraints = ({'type': 'eq', 'fun': lambda p: np.triu(p,1) + np.tril(p, -1)})\n                           )\n\nbeta_estimated = result.x[:n].reshape(n,)\nmu_estimated = result.x[n:2*n].reshape(n,)\np_estimated = result.x[2*n:].reshape(n, n)\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> # Set up the data to fit n = model_input.n  # # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p_coeff(p_guess=0.3) initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)  # Define the bounds for beta, mu and p beta_bounds = [(0,1)]*len(beta_guess.flatten()) mu_bounds = [(0.1,0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p  # Concatenate the bounds bounds = beta_bounds + mu_bounds + p_bounds # def set_constraint(x): #     p = x[2*n:].reshape(n, n) #     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other #     #[p[i,j] - p[j,i] == 0 for i in range(n) for j in range(n)]  #     for i in range(n): #         for j in range(n): #             if p[i,j] - p[j,i] != 0: #                return 1 #     return 0  # def set_constraint(x): #     p = x[2*n:].reshape(n, n) #     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other #     diff_matrix = np.abs(p - p.T) #     if np.any(diff_matrix != 0): #         return 1 #     return 0  # Minimize the objective function to obtain beta, mu, and p result = optimize.minimize(objectiveFunction, x0=initial_parameters                            , args=(example, 0, 'BeginPlaguePeriod', 'ParishName', 'BeginDaysPlague', 'EndDaysPlague', 'VictimsNumber')                            , bounds=bounds                            # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other                            #, constraints = ({'type': 'eq', 'fun': set_constraint})                              # Constraint for checking that the vector p after a transformation gives a symmetric matrix                             #, constraints = ({'type': 'eq', 'fun': lambda p: np.triu(p,1) + np.tril(p, -1)})                            )  beta_estimated = result.x[:n].reshape(n,) mu_estimated = result.x[n:2*n].reshape(n,) p_estimated = result.x[2*n:].reshape(n, n)  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  0.3333333333333333\nbeta =  [0.5 0.5 0.5]\nmu =  [0.5 0.5 0.5]\np =  [[0.  0.3 0.3]\n [0.3 0.  0.3]\n [0.3 0.3 0. ]]\n</pre> In\u00a0[268]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) In\u00a0[269]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            # axes[i].plot(initial_position, 0, 'bo')\n            # axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            # axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             # axes[i].plot(initial_position, 0, 'bo')             # axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             # axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()"},{"location":"PlagueProject/FittingSouthScania/","title":"FittingSouthScania","text":"In\u00a0[108]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[109]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[110]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[111]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[112]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[113]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] In\u00a0[114]: Copied! <pre># Set the working directory for private files\n\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n</pre> # Set the working directory for private files  # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) In\u00a0[115]: Copied! <pre># Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n# Create a GeoDataFrame from the DataFrame\nsouthScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry')\n</pre> # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads) # Create a GeoDataFrame from the DataFrame southScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry') <p>Plotting the parishes from South Scania</p> In\u00a0[116]: Copied! <pre># Assigning the coordinate reference system (CRS) to the GeoDataFrame\nsouthScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")\n\n# For checking the CRS\n# southScaniaMap.crs\n</pre> # Assigning the coordinate reference system (CRS) to the GeoDataFrame southScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")  # For checking the CRS # southScaniaMap.crs In\u00a0[117]: Copied! <pre>colorByColumn(southScaniaMap, 'EndPlaguePeriod')\nsouthMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nsouthScaniaMap.explore(\n    m = southMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"South Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    southMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(southMap)  # use folium to add layer control\n\nsouthMap  # show map\n</pre> colorByColumn(southScaniaMap, 'EndPlaguePeriod') southMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  southScaniaMap.explore(     m = southMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"South Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     southMap )  # use folium to add alternative tiles folium.LayerControl().add_to(southMap)  # use folium to add layer control  southMap  # show map Out[117]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[118]: Copied! <pre>colorByColumn(southScaniaMap, 'EndPlaguePeriod')\nfig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel2')\nsouthScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> colorByColumn(southScaniaMap, 'EndPlaguePeriod') fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel2') southScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() <p>We will focus only in the parishes affected by the plague. To do so, first we filter the data frame.</p> In\u00a0[119]: Copied! <pre>plagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red']\nlen(plagueSouthScania)\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, color = 'lightgray', edgecolor='black',\n              legend=False)\nplagueSouthScania.plot(ax=ax, color = 'darkred',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> plagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red'] len(plagueSouthScania)   fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, color = 'lightgray', edgecolor='black',               legend=False) plagueSouthScania.plot(ax=ax, color = 'darkred',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() In\u00a0[120]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe\nplagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan)\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan) In\u00a0[121]: Copied! <pre>plagueSouthScania['BeginPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['BeginPlaguePeriod'], format='%b %Y')\nplagueSouthScania['EndPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['EndPlaguePeriod'], format='%b %Y')\nplagueSouthScania.sort_values(\n    by=['BeginPlaguePeriod'],   # Row or columns names to sort by\n    axis=0,       # Sort Rows axis = 0\n    ascending=True,  # Sort ascending or descending?\n    # Modify the DataFrame in place (do not create a new object)\n    inplace=True\n)\n\nplagueSouthScania.reset_index(drop=True, inplace=True)\n</pre> plagueSouthScania['BeginPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['BeginPlaguePeriod'], format='%b %Y') plagueSouthScania['EndPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['EndPlaguePeriod'], format='%b %Y') plagueSouthScania.sort_values(     by=['BeginPlaguePeriod'],   # Row or columns names to sort by     axis=0,       # Sort Rows axis = 0     ascending=True,  # Sort ascending or descending?     # Modify the DataFrame in place (do not create a new object)     inplace=True )  plagueSouthScania.reset_index(drop=True, inplace=True) In\u00a0[122]: Copied! <pre># Create a new column called \"BeginDaysPlague\"\nplagueSouthScania[\"BeginDaysPlague\"] = plagueSouthScania.apply(\n    # axis = 1 means apply function to each row\n    lambda row: begin_days_between(plagueSouthScania[\"BeginPlaguePeriod\"].iloc[0], row[\"BeginPlaguePeriod\"]), axis=1\n)\n\n# Create a new column called \"EndDaysPlague\"\nplagueSouthScania['EndDaysPlague'] = plagueSouthScania.apply(lambda row: end_days_between(\n    plagueSouthScania['BeginPlaguePeriod'].iloc[0], row['EndPlaguePeriod']) if pd.notna(row['EndPlaguePeriod']) else None, axis=1)\n</pre> # Create a new column called \"BeginDaysPlague\" plagueSouthScania[\"BeginDaysPlague\"] = plagueSouthScania.apply(     # axis = 1 means apply function to each row     lambda row: begin_days_between(plagueSouthScania[\"BeginPlaguePeriod\"].iloc[0], row[\"BeginPlaguePeriod\"]), axis=1 )  # Create a new column called \"EndDaysPlague\" plagueSouthScania['EndDaysPlague'] = plagueSouthScania.apply(lambda row: end_days_between(     plagueSouthScania['BeginPlaguePeriod'].iloc[0], row['EndPlaguePeriod']) if pd.notna(row['EndPlaguePeriod']) else None, axis=1)  In\u00a0[123]: Copied! <pre># Replace NaN values with a value in some columns (e.g., 0)\nplagueSouthScania['BeginDaysPlague'].fillna(0, inplace=True)\nplagueSouthScania['EndDaysPlague'].fillna(0, inplace=True)\nplagueSouthScania['VictimsNumber'].fillna(0, inplace=True)\n\n# Changing the type of some columns from float to integer for the optimization process\nplagueSouthScania['BeginDaysPlague'] = plagueSouthScania['BeginDaysPlague'].astype(\n    int)\nplagueSouthScania['EndDaysPlague'] = plagueSouthScania['EndDaysPlague'].astype(int)\nplagueSouthScania['VictimsNumber'] = plagueSouthScania['VictimsNumber'].astype(int)\n</pre> # Replace NaN values with a value in some columns (e.g., 0) plagueSouthScania['BeginDaysPlague'].fillna(0, inplace=True) plagueSouthScania['EndDaysPlague'].fillna(0, inplace=True) plagueSouthScania['VictimsNumber'].fillna(0, inplace=True)  # Changing the type of some columns from float to integer for the optimization process plagueSouthScania['BeginDaysPlague'] = plagueSouthScania['BeginDaysPlague'].astype(     int) plagueSouthScania['EndDaysPlague'] = plagueSouthScania['EndDaysPlague'].astype(int) plagueSouthScania['VictimsNumber'] = plagueSouthScania['VictimsNumber'].astype(int)  In\u00a0[124]: Copied! <pre>max_days = plagueSouthScania['EndDaysPlague'].max() \nmax_days\n</pre> max_days = plagueSouthScania['EndDaysPlague'].max()  max_days Out[124]: <pre>1519</pre> <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[125]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].values\n\n    def numPatches(self):\n        return len(self.patchNames())\n\n    def patchPop(self, column_pop: str = 'BEF1699'):\n        return self.gdf[column_pop].values\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].values      def numPatches(self):         return len(self.patchNames())      def patchPop(self, column_pop: str = 'BEF1699'):         return self.gdf[column_pop].values      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()              <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[126]: Copied! <pre>plagueSouthScania = get_centroid(plagueSouthScania)\n</pre> plagueSouthScania = get_centroid(plagueSouthScania)  <p>Generating the differential equations</p> In\u00a0[127]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta']\n    p = parameters['p']\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    mu = parameters['mu']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    beta_matrix =  transmission_matrix_beta(gdf)\n    p_matrix = transmission_matrix_p(gdf)\n    matrix = lambda t : (beta) * beta_matrix + (p)  * p_matrix\n\n    # For including a seasonal transmission rate\n    #seasonal_rate = lambda t : seasonal_transmission_rate(t, bump_center, bump_width, bump_height)\n    #matrix = lambda t : (beta + seasonal_rate(t)) * beta_matrix + (p + seasonal_rate(t))  * p_matrix\n    \n\n    sum_transmission = lambda t : np.sum(matrix(t) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = gamma * (1 - mu) * entry[:, 2]\n    dD = gamma * mu * entry[:, 2]\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n\n    return derivatives\n\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n\n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta']     p = parameters['p']     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     mu = parameters['mu']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      beta_matrix =  transmission_matrix_beta(gdf)     p_matrix = transmission_matrix_p(gdf)     matrix = lambda t : (beta) * beta_matrix + (p)  * p_matrix      # For including a seasonal transmission rate     #seasonal_rate = lambda t : seasonal_transmission_rate(t, bump_center, bump_width, bump_height)     #matrix = lambda t : (beta + seasonal_rate(t)) * beta_matrix + (p + seasonal_rate(t))  * p_matrix           sum_transmission = lambda t : np.sum(matrix(t) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = gamma * (1 - mu) * entry[:, 2]     dD = gamma * mu * entry[:, 2]     derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()      return derivatives   def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]      T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']     solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}  <p>Trying a small dataframe</p> In\u00a0[128]: Copied! <pre># k = 6\n# example = plagueSouthScania.head(k)\n# model_input = Initial_Model(example)\n</pre> # k = 6 # example = plagueSouthScania.head(k) # model_input = Initial_Model(example) In\u00a0[129]: Copied! <pre># Model_test = {'model': SEIRD_model,\n#               'init': {\n#                   'S': model_input.S0,\n#                   'E': model_input.E0,\n#                   'I': model_input.I0,\n#                   'R': model_input.R0,\n#                   'D': model_input.D0,\n#               },  # defining the initial values for the model\n#               'gdf': example,  # defining the graph\n#               'beta': 0.3,\n#               'p': 0.1,\n#               'bump_center': 0.1,\n#               'bump_width': 180.0,\n#               'bump_height': 30.0,\n#               'gamma': 0.06,\n#               'sigma': 0.02,\n#               'mu': 0.2,\n#               'N': model_input.patchPop(),\n#               'n': model_input.n,\n#               'T': model_input.maxDays()}\n\n# model_dict = generate_sol(Model_test)\n</pre> # Model_test = {'model': SEIRD_model, #               'init': { #                   'S': model_input.S0, #                   'E': model_input.E0, #                   'I': model_input.I0, #                   'R': model_input.R0, #                   'D': model_input.D0, #               },  # defining the initial values for the model #               'gdf': example,  # defining the graph #               'beta': 0.3, #               'p': 0.1, #               'bump_center': 0.1, #               'bump_width': 180.0, #               'bump_height': 30.0, #               'gamma': 0.06, #               'sigma': 0.02, #               'mu': 0.2, #               'N': model_input.patchPop(), #               'n': model_input.n, #               'T': model_input.maxDays()}  # model_dict = generate_sol(Model_test) In\u00a0[130]: Copied! <pre># %matplotlib inline\n\n# # Set up the data to fit\n# beginTime = plagueSouthScania['BeginDaysPlague'].values\n# endTime = plagueSouthScania['EndDaysPlague'].values\n# deathData = plagueSouthScania['VictimsNumber'].values\n\n# # Number of patches\n# n = Model_test['n']\n\n# # Set the figsize for each subplot\n# figsize_single_subplot = (8, 2)\n\n# # Calculate the total figure height based on the number of subplots and their height\n# fig_height = figsize_single_subplot[1] * n\n\n# # Create a figure and an array of axes with nrows=n and ncols=1\n# fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n#     figsize_single_subplot[0], fig_height), sharex=False)\n\n# # Plot model solution D for each patch\n# for i in range(n):\n#     if deathData[i] != 0 and endTime[i] != 0:\n#         initial_position = beginTime[i]\n#         final_position = endTime[i]\n#         axes[i].plot(initial_position, 0, 'bo')\n#         axes[i].plot(final_position,\n#                      deathData[i], 'bo')\n#         axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n       \n#     else:\n#         axes[i].plot(model_dict['D'][i],\n#                      color='orange', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n#         axes[i].legend(loc='lower right')\n        \n# # Adjust the layout to avoid overlapping\n# plt.tight_layout()\n# plt.show()\n</pre> # %matplotlib inline  # # Set up the data to fit # beginTime = plagueSouthScania['BeginDaysPlague'].values # endTime = plagueSouthScania['EndDaysPlague'].values # deathData = plagueSouthScania['VictimsNumber'].values  # # Number of patches # n = Model_test['n']  # # Set the figsize for each subplot # figsize_single_subplot = (8, 2)  # # Calculate the total figure height based on the number of subplots and their height # fig_height = figsize_single_subplot[1] * n  # # Create a figure and an array of axes with nrows=n and ncols=1 # fig, axes = plt.subplots(nrows=n, ncols=1, figsize=( #     figsize_single_subplot[0], fig_height), sharex=False)  # # Plot model solution D for each patch # for i in range(n): #     if deathData[i] != 0 and endTime[i] != 0: #         initial_position = beginTime[i] #         final_position = endTime[i] #         axes[i].plot(initial_position, 0, 'bo') #         axes[i].plot(final_position, #                      deathData[i], 'bo') #         axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths')         #     else: #         axes[i].plot(model_dict['D'][i], #                      color='orange', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths') #         axes[i].legend(loc='lower right')          # # Adjust the layout to avoid overlapping # plt.tight_layout() # plt.show() <p>Defining the optimization problem:</p> In\u00a0[131]: Copied! <pre>model_input = Initial_Model(plagueSouthScania)\n</pre> model_input = Initial_Model(plagueSouthScania) In\u00a0[132]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters, beginTime, endTime, deathData):\n    beta, p, bump_center, bump_width, bump_height = parameters\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': plagueSouthScania,\n                  # defining the initial values for the model\n                  'beta': beta,\n                  'p': p,\n                  'bump_center': bump_center,\n                  'bump_width': bump_width,\n                  'bump_height': bump_height,\n                  'gamma': 0.06,\n                  'sigma': 0.02,\n                  'mu': 0.2,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n    model_sol = generate_sol(model_info)\n    totalError = 0\n    n = model_info['n']\n\n    # Calculate the error for each patch\n    errors = np.zeros(n)\n    for i in range(n):\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        if (deathData[i] != 0 and final_position != 0):\n            try:\n                errors[i] = 0.7 * ((model_sol['D'][i][initial_position] - 1.0)**2 + (\n                    model_sol['D'][i][final_position] - deathData[i])**2)\n            except:\n                print(\n                    f\"Error at: n={n}, i={i}, final_position={final_position}, len(model_sol['D'])= {len(model_sol['D'])}, model_sol['D'][i] = {model_sol['D'][i]}, deathData[i] = {deathData[i]}\")\n        else:\n            errors[i] = 0.3 * ((model_sol['D'][i][initial_position] - 1.0)**2)\n\n    # Calculate the total error\n    totalError = np.sum(errors)\n    return totalError\n</pre> # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters, beginTime, endTime, deathData):     beta, p, bump_center, bump_width, bump_height = parameters     model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': plagueSouthScania,                   # defining the initial values for the model                   'beta': beta,                   'p': p,                   'bump_center': bump_center,                   'bump_width': bump_width,                   'bump_height': bump_height,                   'gamma': 0.06,                   'sigma': 0.02,                   'mu': 0.2,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}     model_sol = generate_sol(model_info)     totalError = 0     n = model_info['n']      # Calculate the error for each patch     errors = np.zeros(n)     for i in range(n):         initial_position = beginTime[i]         final_position = endTime[i]         if (deathData[i] != 0 and final_position != 0):             try:                 errors[i] = 0.7 * ((model_sol['D'][i][initial_position] - 1.0)**2 + (                     model_sol['D'][i][final_position] - deathData[i])**2)             except:                 print(                     f\"Error at: n={n}, i={i}, final_position={final_position}, len(model_sol['D'])= {len(model_sol['D'])}, model_sol['D'][i] = {model_sol['D'][i]}, deathData[i] = {deathData[i]}\")         else:             errors[i] = 0.3 * ((model_sol['D'][i][initial_position] - 1.0)**2)      # Calculate the total error     totalError = np.sum(errors)     return totalError  <p>Parameter estimation</p> In\u00a0[133]: Copied! <pre># Set up the data to fit\nbeginTime = plagueSouthScania['BeginDaysPlague'].values\nendTime = plagueSouthScania['EndDaysPlague'].values\ndeathData = plagueSouthScania['VictimsNumber'].values\n\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = 0.3\np_guess = 0.1\nbump_center_guess = 0.1\nbump_width_guess = 180.0\nbump_height_guess = 30.0\n\n\n# Minimize the objective function to obtain estimates for beta and gamma\nresult = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess), args=(beginTime, endTime, deathData),\n                           method='L-BFGS-B'\n                           # ,bounds=[(0, 1), (0, 1), (0, 10), (-2, 2), (-10, 10)]\n                           )\nbeta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x\n\nprint(\"beta = \", beta_estimated)\nprint(\"p = \", p_estimated)\nprint(\"bump_center = \", bump_center_estimated)\nprint(\"bump_width = \", bump_width_estimated)\nprint(\"bump_height = \", bump_height_estimated)\n</pre> # Set up the data to fit beginTime = plagueSouthScania['BeginDaysPlague'].values endTime = plagueSouthScania['EndDaysPlague'].values deathData = plagueSouthScania['VictimsNumber'].values   # Choose initial guesses for the parameters to fit beta_guess = 0.3 p_guess = 0.1 bump_center_guess = 0.1 bump_width_guess = 180.0 bump_height_guess = 30.0   # Minimize the objective function to obtain estimates for beta and gamma result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess), args=(beginTime, endTime, deathData),                            method='L-BFGS-B'                            # ,bounds=[(0, 1), (0, 1), (0, 10), (-2, 2), (-10, 10)]                            ) beta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x  print(\"beta = \", beta_estimated) print(\"p = \", p_estimated) print(\"bump_center = \", bump_center_estimated) print(\"bump_width = \", bump_width_estimated) print(\"bump_height = \", bump_height_estimated) <pre>/usr/local/lib/python3.11/site-packages/scipy/integrate/_odepack_py.py:248: ODEintWarning: Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.\n  warnings.warn(warning_msg, ODEintWarning)\n</pre> <pre>beta =  0.34490334476356177\np =  -11.6081043424794\nbump_center =  0.1\nbump_width =  180.0\nbump_height =  30.0\n</pre> <p>Results from estimations</p> In\u00a0[134]: Copied! <pre># # Set up the data to fit\nbeginTime = plagueSouthScania['BeginDaysPlague'].values\nendTime = plagueSouthScania['EndDaysPlague'].values\ndeathData = plagueSouthScania['VictimsNumber'].values\n\n# # Parameter estimation without considering seasonality in p and beta.\nbeta_estimated =  0.34490334476356177\np_estimated =  -11.6081043424794\nbump_center_estimated =  0.1000005503832051\nbump_width_estimated =  180.00000012694866\nbump_height_estimated =  30.00000035610224\n\n# #result estimation considering seasonality only (beta + seasonality)\n# beta_estimated =  0.3574870151324586\n# p_estimated =  0.07419604016603554\n# bump_center_estimated =  0.10388586284456384\n# bump_width_estimated =  180.00730607111754\n# bump_height_estimated =  30.01454878377533\n\n# #result estimation without considering seasonality\n# beta_estimated =  0.6159244597903136\n# p_estimated =  -0.062116989387247884\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n</pre> # # Set up the data to fit beginTime = plagueSouthScania['BeginDaysPlague'].values endTime = plagueSouthScania['EndDaysPlague'].values deathData = plagueSouthScania['VictimsNumber'].values  # # Parameter estimation without considering seasonality in p and beta. beta_estimated =  0.34490334476356177 p_estimated =  -11.6081043424794 bump_center_estimated =  0.1000005503832051 bump_width_estimated =  180.00000012694866 bump_height_estimated =  30.00000035610224  # #result estimation considering seasonality only (beta + seasonality) # beta_estimated =  0.3574870151324586 # p_estimated =  0.07419604016603554 # bump_center_estimated =  0.10388586284456384 # bump_width_estimated =  180.00730607111754 # bump_height_estimated =  30.01454878377533  # #result estimation without considering seasonality # beta_estimated =  0.6159244597903136 # p_estimated =  -0.062116989387247884 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  <p>Substituting the estimated values into the model and solving it</p> In\u00a0[136]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                    'init': {\n                        'S': model_input.S0,\n                        'E': model_input.E0,\n                        'I': model_input.I0,\n                        'R': model_input.R0,\n                        'D': model_input.D0,\n                    },\n                    'gdf': plagueSouthScania,\n                    # defining the initial values for the model\n                    'beta': beta_estimated,\n                    'p': p_estimated,\n                    'bump_center': bump_center_estimated,\n                    'bump_width': bump_width_estimated,\n                    'bump_height': bump_height_estimated,\n                    'gamma': 0.06,\n                    'sigma': 0.02,\n                    'mu': 0.2,\n                    'N': model_input.patchPop(),\n                    'n': model_input.n,\n                    'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                     'init': {                         'S': model_input.S0,                         'E': model_input.E0,                         'I': model_input.I0,                         'R': model_input.R0,                         'D': model_input.D0,                     },                     'gdf': plagueSouthScania,                     # defining the initial values for the model                     'beta': beta_estimated,                     'p': p_estimated,                     'bump_center': bump_center_estimated,                     'bump_width': bump_width_estimated,                     'bump_height': bump_height_estimated,                     'gamma': 0.06,                     'sigma': 0.02,                     'mu': 0.2,                     'N': model_input.patchPop(),                     'n': model_input.n,                     'T': model_input.maxDays()} model_solution = generate_sol(model_estimation)  <pre>/usr/local/lib/python3.11/site-packages/scipy/integrate/_odepack_py.py:248: ODEintWarning: Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.\n  warnings.warn(warning_msg, ODEintWarning)\n</pre> <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[137]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ntick_positions = plagueSouthScania['BeginDaysPlague'].values\ntick_labels = plagueSouthScania['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Dictionary that reduces the plotting to those plots with data\n#lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]\n\n# Plot model solution D for each patch\nfor i in range(n):\n    if deathData[i] != 0 and endTime[i] != 0:\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        axes[i].plot(initial_position, 0, 'bo')\n        axes[i].plot(final_position,\n                     deathData[i], 'bo')\n        axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc = 'lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n    else:\n        axes[i].plot(model_solution['D'][i],\n                     color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  tick_positions = plagueSouthScania['BeginDaysPlague'].values tick_labels = plagueSouthScania['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Dictionary that reduces the plotting to those plots with data #lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]  # Plot model solution D for each patch for i in range(n):     if deathData[i] != 0 and endTime[i] != 0:         initial_position = beginTime[i]         final_position = endTime[i]         axes[i].plot(initial_position, 0, 'bo')         axes[i].plot(final_position,                      deathData[i], 'bo')         axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc = 'lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)     else:         axes[i].plot(model_solution['D'][i],                      color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[137], line 13\n     10 fig_height = figsize_single_subplot[1] * n\n     12 # Create a figure and an array of axes with nrows=n and ncols=1\n---&gt; 13 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n     14     figsize_single_subplot[0], fig_height), sharex=False)\n     16 tick_positions = plagueSouthScania['BeginDaysPlague'].values\n     17 tick_labels = plagueSouthScania['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/pyplot.py:1502, in subplots(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\n   1358 \"\"\"\n   1359 Create a figure and a set of subplots.\n   1360 \n   (...)\n   1499 \n   1500 \"\"\"\n   1501 fig = figure(**fig_kw)\n-&gt; 1502 axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n   1503                    squeeze=squeeze, subplot_kw=subplot_kw,\n   1504                    gridspec_kw=gridspec_kw, height_ratios=height_ratios,\n   1505                    width_ratios=width_ratios)\n   1506 return fig, axs\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/figure.py:906, in FigureBase.subplots(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\n    903     gridspec_kw['width_ratios'] = width_ratios\n    905 gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)\n--&gt; 906 axs = gs.subplots(sharex=sharex, sharey=sharey, squeeze=squeeze,\n    907                   subplot_kw=subplot_kw)\n    908 return axs\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/gridspec.py:299, in GridSpecBase.subplots(self, sharex, sharey, squeeze, subplot_kw)\n    297         subplot_kw[\"sharex\"] = shared_with[sharex]\n    298         subplot_kw[\"sharey\"] = shared_with[sharey]\n--&gt; 299         axarr[row, col] = figure.add_subplot(\n    300             self[row, col], **subplot_kw)\n    302 # turn off redundant tick labeling\n    303 if sharex in [\"col\", \"all\"]:\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/figure.py:757, in FigureBase.add_subplot(self, *args, **kwargs)\n    754         args = tuple(map(int, str(args[0])))\n    755     projection_class, pkw = self._process_projection_requirements(\n    756         *args, **kwargs)\n--&gt; 757     ax = projection_class(self, *args, **pkw)\n    758     key = (projection_class, pkw)\n    759 return self._add_axes_internal(ax, key)\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/axes/_base.py:683, in _AxesBase.__init__(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\n    680 self.set_axisbelow(mpl.rcParams['axes.axisbelow'])\n    682 self._rasterization_zorder = None\n--&gt; 683 self.clear()\n    685 # funcs used to format x and y - fall back on major formatters\n    686 self.fmt_xdata = None\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/axes/_base.py:1395, in _AxesBase.clear(self)\n   1393     self.cla()\n   1394 else:\n-&gt; 1395     self.__clear()\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/axes/_base.py:1281, in _AxesBase.__clear(self)\n   1279     axis.clear()  # Also resets the scale to linear.\n   1280 for spine in self.spines.values():\n-&gt; 1281     spine.clear()\n   1283 self.ignore_existing_data_limits = True\n   1284 self.callbacks = cbook.CallbackRegistry(\n   1285     signals=[\"xlim_changed\", \"ylim_changed\", \"zlim_changed\"])\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/spines.py:225, in Spine.clear(self)\n    223 self._position = None  # clear position\n    224 if self.axis is not None:\n--&gt; 225     self.axis.clear()\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/axis.py:879, in Axis.clear(self)\n    875 self.labelpad = mpl.rcParams['axes.labelpad']\n    877 self._init()\n--&gt; 879 self._set_scale('linear')\n    881 # Clear the callback registry for this axis, or it may \"leak\"\n    882 self.callbacks = cbook.CallbackRegistry(signals=[\"units\"])\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/axis.py:770, in Axis._set_scale(self, value, **kwargs)\n    768 else:\n    769     self._scale = value\n--&gt; 770 self._scale.set_default_locators_and_formatters(self)\n    772 self.isDefault_majloc = True\n    773 self.isDefault_minloc = True\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/scale.py:105, in LinearScale.set_default_locators_and_formatters(self, axis)\n    103 def set_default_locators_and_formatters(self, axis):\n    104     # docstring inherited\n--&gt; 105     axis.set_major_locator(AutoLocator())\n    106     axis.set_major_formatter(ScalarFormatter())\n    107     axis.set_minor_formatter(NullFormatter())\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/ticker.py:2900, in AutoLocator.__init__(self)\n   2898     nbins = 'auto'\n   2899     steps = [1, 2, 2.5, 5, 10]\n-&gt; 2900 super().__init__(nbins=nbins, steps=steps)\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/ticker.py:2008, in MaxNLocator.__init__(self, nbins, **kwargs)\n   2006 if nbins is not None:\n   2007     kwargs['nbins'] = nbins\n-&gt; 2008 self.set_params(**{**self.default_params, **kwargs})\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/ticker.py:2068, in MaxNLocator.set_params(self, **kwargs)\n   2066     else:\n   2067         self._steps = self._validate_steps(steps)\n-&gt; 2068     self._extended_steps = self._staircase(self._steps)\n   2069 if 'integer' in kwargs:\n   2070     self._integer = kwargs.pop('integer')\n\nFile /usr/local/lib/python3.11/site-packages/matplotlib/ticker.py:2025, in MaxNLocator._staircase(steps)\n   2022         steps = np.concatenate([steps, [10]])\n   2023     return steps\n-&gt; 2025 @staticmethod\n   2026 def _staircase(steps):\n   2027     # Make an extended staircase within which the needed step will be\n   2028     # found.  This is probably much larger than necessary.\n   2029     return np.concatenate([0.1 * steps[:-1], steps, [10 * steps[1]]])\n   2031 def set_params(self, **kwargs):\n\nKeyboardInterrupt: </pre> <p>Plotting the daily deaths by parish</p> In\u00a0[\u00a0]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\n# tick_positions = southeastScania['BeginDaysPlague'].values\n# tick_labels = southeastScania['BeginPlaguePeriod'].apply(\n#     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  # tick_positions = southeastScania['BeginDaysPlague'].values # tick_labels = southeastScania['BeginPlaguePeriod'].apply( #     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()"},{"location":"PlagueProject/FittingSouthScaniaModelObFunct2/","title":"FittingSouthScaniaModelObFunct2","text":"In\u00a0[435]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[436]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[437]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[438]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[439]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[440]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[441]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n\ntype(southScania)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]]  type(southScania) Out[441]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Getting the centroid of each polygon for defining the transmission matrix. First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[442]: Copied! <pre>southScania = get_centroid(southScania)\nsouthScania = southScania.replace(['UNDEFINED', '?'], np.nan)\nsouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    southScania['BeginPlaguePeriod'], format='%b %Y')\nsouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    southScania['EndPlaguePeriod'], format='%b %Y')\n\nlen(southScania)\n</pre> southScania = get_centroid(southScania) southScania = southScania.replace(['UNDEFINED', '?'], np.nan) southScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     southScania['BeginPlaguePeriod'], format='%b %Y') southScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     southScania['EndPlaguePeriod'], format='%b %Y')  len(southScania) Out[442]: <pre>235</pre> <p>Defining a group to work with</p> In\u00a0[443]: Copied! <pre># Filter the data to get only the infected parishes\nsouthScania = southScania[southScania['new_format_BeginPlaguePeriod'].notna()]\n</pre> # Filter the data to get only the infected parishes southScania = southScania[southScania['new_format_BeginPlaguePeriod'].notna()] In\u00a0[446]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n# # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0\n# cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(southScania)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) # # Add a column with the proportion of deaths per parish. If the value is NaN, then the proportion is 0 # cluster1['Proportion'] = cluster1['VictimsNumber']/cluster1['BEF1699']  <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[447]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf, beta_guess:float, mu_guess:float, p_guess:float):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        self.mu = np.full(self.n, mu_guess)\n        self.beta = np.full(self.n, beta_guess)\n        self.p = p_guess\n\n        self.S0 = np.zeros(self.n)       \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name]\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf, beta_guess:float, mu_guess:float, p_guess:float):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         self.mu = np.full(self.n, mu_guess)         self.beta = np.full(self.n, beta_guess)         self.p = p_guess          self.S0 = np.zeros(self.n)                for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name]      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()           <p>Generating the differential equations</p> In\u00a0[448]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[449]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # ensure beta is a numpy array of shape n\n    mu = parameters['mu'] # ensure mu is a numpy array of shape n\n    p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n\n    beta =  beta_matrix(gdf, beta)\n    p_matrix = transmission_matrix_p(gdf, p_coeff)\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + (beta + p_matrix) \n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T'] \n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # ensure beta is a numpy array of shape n     mu = parameters['mu'] # ensure mu is a numpy array of shape n     p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])       beta =  beta_matrix(gdf, beta)     p_matrix = transmission_matrix_p(gdf, p_coeff)      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + (beta + p_matrix)       sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)       dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]      derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']      t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} In\u00a0[450]: Copied! <pre># Selecting specific rows from the dataframe reseting the index\nexample = cluster1\nmodel_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5, p_guess=0.005)\n</pre> # Selecting specific rows from the dataframe reseting the index example = cluster1 model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5, p_guess=0.005) In\u00a0[451]: Copied! <pre>Model_test = {'model': SEIRD_model,\n              'init': {\n                  'S': model_input.S0,\n                  'E': model_input.E0,\n                  'I': model_input.I0,\n                  'R': model_input.R0,\n                  'D': model_input.D0,\n              },  # defining the initial values for the model\n              'gdf': example,  # defining the graph\n              'beta': model_input.beta,\n              'p_coeff': model_input.p,\n              'bump_center': 0.0,\n              'bump_width': 0.0,\n              'bump_height': 0.0,\n              'gamma': 0.4,\n              'sigma': 0.17,\n              'mu': model_input.mu,\n              'N': model_input.patchPop(),\n              'n': model_input.n,\n              'T': model_input.maxDays()}\n\nmodel_dict = generate_sol(Model_test)\n</pre> Model_test = {'model': SEIRD_model,               'init': {                   'S': model_input.S0,                   'E': model_input.E0,                   'I': model_input.I0,                   'R': model_input.R0,                   'D': model_input.D0,               },  # defining the initial values for the model               'gdf': example,  # defining the graph               'beta': model_input.beta,               'p_coeff': model_input.p,               'bump_center': 0.0,               'bump_width': 0.0,               'bump_height': 0.0,               'gamma': 0.4,               'sigma': 0.17,               'mu': model_input.mu,               'N': model_input.patchPop(),               'n': model_input.n,               'T': model_input.maxDays()}  model_dict = generate_sol(Model_test) <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[451], line 22\n      1 Model_test = {'model': SEIRD_model,\n      2               'init': {\n      3                   'S': model_input.S0,\n   (...)\n     19               'n': model_input.n,\n     20               'T': model_input.maxDays()}\n---&gt; 22 model_dict = generate_sol(Model_test)\n\nCell In[449], line 61, in generate_sol(genInput)\n     57 t = np.linspace(0, T, T+1)\n     59 model = genInput['model']\n---&gt; 61 solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n     63 indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n     64 def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/integrate/_odepack_py.py:242, in odeint(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg, tfirst)\n    240 t = copy(t)\n    241 y0 = copy(y0)\n--&gt; 242 output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n    243                          full_output, rtol, atol, tcrit, h0, hmax, hmin,\n    244                          ixpr, mxstep, mxhnil, mxordn, mxords,\n    245                          int(bool(tfirst)))\n    246 if output[-1] &lt; 0:\n    247     warning_msg = _msgs[output[-1]] + \" Run with full_output = 1 to get quantitative information.\"\n\nCell In[449], line 25, in SEIRD_model(y, t, model_parameters)\n     21 entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n     24 beta =  beta_matrix(gdf, beta)\n---&gt; 25 p_matrix = transmission_matrix_p(gdf, p_coeff)\n     27 # For including a seasonal transmission rate\n     28 if SEASONALITY:\n\nFile ~/Desktop/PythonMathematicalModeling/docs/PlagueProject/funct_process_data.py:512, in transmission_matrix_p(gdf, p_coeff, column_centroid, column_pop, column_name)\n    510 centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]\n    511 pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]\n--&gt; 512 pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]\n    513 if name_i != name_j:\n    514     gravitational[i,j] = (pop_i * pop_j) / (centroid_i.distance(centroid_j)**2)\n\nFile /opt/homebrew/lib/python3.11/site-packages/geopandas/geodataframe.py:1475, in GeoDataFrame.__getitem__(self, key)\n   1469 def __getitem__(self, key):\n   1470     \"\"\"\n   1471     If the result is a column containing only 'geometry', return a\n   1472     GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\n   1473     return a GeoDataFrame.\n   1474     \"\"\"\n-&gt; 1475     result = super().__getitem__(key)\n   1476     # Custom logic to avoid waiting for pandas GH51895\n   1477     # result is not geometry dtype for multi-indexes\n   1478     if (\n   1479         pd.api.types.is_scalar(key)\n   1480         and key == \"\"\n   (...)\n   1483         and not is_geometry_type(result)\n   1484     ):\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3752, in DataFrame.__getitem__(self, key)\n   3750 # Do we have a (boolean) 1d indexer?\n   3751 if com.is_bool_indexer(key):\n-&gt; 3752     return self._getitem_bool_array(key)\n   3754 # We are left with two options: a single key, and a collection of keys,\n   3755 # We interpret tuples as collections only for non-MultiIndex\n   3756 is_single_key = isinstance(key, tuple) or not is_list_like(key)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3811, in DataFrame._getitem_bool_array(self, key)\n   3808     return self.copy(deep=None)\n   3810 indexer = key.nonzero()[0]\n-&gt; 3811 return self._take_with_is_copy(indexer, axis=0)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:3948, in NDFrame._take_with_is_copy(self, indices, axis)\n   3940 def _take_with_is_copy(self: NDFrameT, indices, axis: Axis = 0) -&gt; NDFrameT:\n   3941     \"\"\"\n   3942     Internal version of the `take` method that sets the `_is_copy`\n   3943     attribute to keep track of the parent dataframe (using in indexing\n   (...)\n   3946     See the docstring of `take` for full explanation of the parameters.\n   3947     \"\"\"\n-&gt; 3948     result = self._take(indices=indices, axis=axis)\n   3949     # Maybe set copy if we didn't actually change the index.\n   3950     if not result._get_axis(axis).equals(self._get_axis(axis)):\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:3932, in NDFrame._take(self, indices, axis, convert_indices)\n   3924     if (\n   3925         axis == 0\n   3926         and indices.ndim == 1\n   3927         and using_copy_on_write()\n   3928         and is_range_indexer(indices, len(self))\n   3929     ):\n   3930         return self.copy(deep=None)\n-&gt; 3932 new_data = self._mgr.take(\n   3933     indices,\n   3934     axis=self._get_block_manager_axis(axis),\n   3935     verify=True,\n   3936     convert_indices=convert_indices,\n   3937 )\n   3938 return self._constructor(new_data).__finalize__(self, method=\"take\")\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:963, in BaseBlockManager.take(self, indexer, axis, verify, convert_indices)\n    960     indexer = maybe_convert_indices(indexer, n, verify=verify)\n    962 new_labels = self.axes[axis].take(indexer)\n--&gt; 963 return self.reindex_indexer(\n    964     new_axis=new_labels,\n    965     indexer=indexer,\n    966     axis=axis,\n    967     allow_dups=True,\n    968     copy=None,\n    969 )\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:747, in BaseBlockManager.reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\n    740     new_blocks = self._slice_take_blocks_ax0(\n    741         indexer,\n    742         fill_value=fill_value,\n    743         only_slice=only_slice,\n    744         use_na_proxy=use_na_proxy,\n    745     )\n    746 else:\n--&gt; 747     new_blocks = [\n    748         blk.take_nd(\n    749             indexer,\n    750             axis=1,\n    751             fill_value=(\n    752                 fill_value if fill_value is not None else blk.fill_value\n    753             ),\n    754         )\n    755         for blk in self.blocks\n    756     ]\n    758 new_axes = list(self.axes)\n    759 new_axes[axis] = new_axis\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:748, in &lt;listcomp&gt;(.0)\n    740     new_blocks = self._slice_take_blocks_ax0(\n    741         indexer,\n    742         fill_value=fill_value,\n    743         only_slice=only_slice,\n    744         use_na_proxy=use_na_proxy,\n    745     )\n    746 else:\n    747     new_blocks = [\n--&gt; 748         blk.take_nd(\n    749             indexer,\n    750             axis=1,\n    751             fill_value=(\n    752                 fill_value if fill_value is not None else blk.fill_value\n    753             ),\n    754         )\n    755         for blk in self.blocks\n    756     ]\n    758 new_axes = list(self.axes)\n    759 new_axes[axis] = new_axis\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/blocks.py:945, in Block.take_nd(self, indexer, axis, new_mgr_locs, fill_value)\n    942     allow_fill = True\n    944 # Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\n--&gt; 945 new_values = algos.take_nd(\n    946     values, indexer, axis=axis, allow_fill=allow_fill, fill_value=fill_value\n    947 )\n    949 # Called from three places in managers, all of which satisfy\n    950 #  these assertions\n    951 if isinstance(self, ExtensionBlock):\n    952     # NB: in this case, the 'axis' kwarg will be ignored in the\n    953     #  algos.take_nd call above.\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/array_algos/take.py:117, in take_nd(arr, indexer, axis, fill_value, allow_fill)\n    114     return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n    116 arr = np.asarray(arr)\n--&gt; 117 return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/array_algos/take.py:162, in _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n    157     out = np.empty(out_shape, dtype=dtype)\n    159 func = _get_take_nd_function(\n    160     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n    161 )\n--&gt; 162 func(arr, indexer, out, fill_value)\n    164 if flip_order:\n    165     out = out.T\n\nKeyboardInterrupt: </pre> In\u00a0[368]: Copied! <pre>len(model_dict['D'][0])\n</pre> len(model_dict['D'][0]) Out[368]: <pre>304</pre> In\u00a0[370]: Copied! <pre>%matplotlib inline\n\n# Set up the data to fit\nbeginTime = example['BeginDaysPlague'].values\nendTime = example['EndDaysPlague'].values\ndeathData = example['VictimsNumber'].values\n\n# Number of patches\nn = Model_test['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ngato = count_infected_parishes_by_month(example,'JUN 1712',0 )\ntick_positions = gato['DaysFromInitialDate'].values\ntick_labels = gato['date'].apply(\n     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Plot model solution D for each patch\nfor i in range(n):\n    axes[i].plot(model_dict['D'][i],\n                color='orange', label=(model_input.patchNames()[i]))\n    axes[i].axhline(y=1, color='blue', linestyle='--')\n    axes[i].set_ylabel('Cumulative Deaths')\n    axes[i].legend(loc='lower right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n    \n    # if deathData[i] != 0 and endTime[i] != 0:\n    #     initial_position = beginTime[i]\n    #     final_position = endTime[i]\n    #     axes[i].plot(initial_position, 0, 'bo')\n    #     axes[i].plot(final_position,\n    #                  deathData[i], 'bo')\n    #     axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))\n    #     axes[i].set_ylabel('Cumulative Deaths')\n       \n    # else:\n    #     axes[i].plot(model_dict['D'][i],\n    #                  color='orange', label=(model_input.patchNames()[i]))\n    #     axes[i].set_ylabel('Cumulative Deaths')\n    #     axes[i].legend(loc='lower right')\n        \n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Set up the data to fit beginTime = example['BeginDaysPlague'].values endTime = example['EndDaysPlague'].values deathData = example['VictimsNumber'].values  # Number of patches n = Model_test['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  gato = count_infected_parishes_by_month(example,'JUN 1712',0 ) tick_positions = gato['DaysFromInitialDate'].values tick_labels = gato['date'].apply(      lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Plot model solution D for each patch for i in range(n):     axes[i].plot(model_dict['D'][i],                 color='orange', label=(model_input.patchNames()[i]))     axes[i].axhline(y=1, color='blue', linestyle='--')     axes[i].set_ylabel('Cumulative Deaths')     axes[i].legend(loc='lower right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)          # if deathData[i] != 0 and endTime[i] != 0:     #     initial_position = beginTime[i]     #     final_position = endTime[i]     #     axes[i].plot(initial_position, 0, 'bo')     #     axes[i].plot(final_position,     #                  deathData[i], 'bo')     #     axes[i].plot(model_dict['D'][i], color='orange', label=(model_input.patchNames()[i]))     #     axes[i].set_ylabel('Cumulative Deaths')             # else:     #     axes[i].plot(model_dict['D'][i],     #                  color='orange', label=(model_input.patchNames()[i]))     #     axes[i].set_ylabel('Cumulative Deaths')     #     axes[i].legend(loc='lower right')          # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Defining the optimization problem:</p> In\u00a0[\u00a0]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0):\n    parameters = np.array(parameters)\n\n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n].reshape(n,)\n    mu:  np.array = parameters[n:2*n].reshape(n,)\n    p_coeff: float = parameters[2*n]\n    # p_coeff: np.array = parameters[2*n:].reshape(n, n)\n\n    # # Penalize if p_coeff is not symmetric or has non-zero diagonal elements\n    # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):\n    #     return 1e50\n\n    # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)\n    # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p_coeff': p_coeff,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Make a list of the days to iterate over \n    days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values\n    # Initializing the number of infected parishes per month for the model's output\n    model_infected_parishes = np.zeros(len(days))\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(days))\n\n    # Initializing a matrix where the rows represents the number of parishes and the columns the number of days\n    matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n    matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n\n    for k in range(len(grouped_by_parish)):\n        for i, day in enumerate(days):\n            if day &lt; len(model_sol['D'][k]):\n                if model_sol['D'][k][day] &gt;= 1.0 :\n                    matrix_death_parishes_month[k, i] = model_sol['D'][k][day]\n     \n    for i in range(matrix_death_parishes_month.shape[0]):\n        for j in range(matrix_death_parishes_month.shape[1]):\n            if j == 0: # For the first day, there's no previous day to compare\n                matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1\n            else:\n                diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]\n                if diff &gt;= 1.0:\n                    matrix_infected_parishes_month[i,j] = 1\n                else:\n                    matrix_infected_parishes_month[i,j] = 0\n    \n    # Computing the number of infected parishes per month from the matrix\n    for j in range(matrix_infected_parishes_month.shape[1]):\n        # Sum up all the values in the column and store it \n        model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])\n        error[j] = (model_infected_parishes[j] \n                           - cum_infected_parishes_by_month['NumberInfectedParishes'][j])**2\n    \n    max_error = np.max(error)\n    # Computing the error between the model's output and the data\n    total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))\n          \n    return (total_error)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0):     parameters = np.array(parameters)      n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n].reshape(n,)     mu:  np.array = parameters[n:2*n].reshape(n,)     p_coeff: float = parameters[2*n]     # p_coeff: np.array = parameters[2*n:].reshape(n, n)      # # Penalize if p_coeff is not symmetric or has non-zero diagonal elements     # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):     #     return 1e50      # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p_coeff': p_coeff,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Make a list of the days to iterate over      days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values     # Initializing the number of infected parishes per month for the model's output     model_infected_parishes = np.zeros(len(days))     # Initializing the error between the model's output and the data     error = np.zeros(len(days))      # Initializing a matrix where the rows represents the number of parishes and the columns the number of days     matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))     matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))      for k in range(len(grouped_by_parish)):         for i, day in enumerate(days):             if day &lt; len(model_sol['D'][k]):                 if model_sol['D'][k][day] &gt;= 1.0 :                     matrix_death_parishes_month[k, i] = model_sol['D'][k][day]           for i in range(matrix_death_parishes_month.shape[0]):         for j in range(matrix_death_parishes_month.shape[1]):             if j == 0: # For the first day, there's no previous day to compare                 matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1             else:                 diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]                 if diff &gt;= 1.0:                     matrix_infected_parishes_month[i,j] = 1                 else:                     matrix_infected_parishes_month[i,j] = 0          # Computing the number of infected parishes per month from the matrix     for j in range(matrix_infected_parishes_month.shape[1]):         # Sum up all the values in the column and store it          model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])         error[j] = (model_infected_parishes[j]                             - cum_infected_parishes_by_month['NumberInfectedParishes'][j])**2          max_error = np.max(error)     # Computing the error between the model's output and the data     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))                return (total_error) In\u00a0[\u00a0]: Copied! <pre># Define a function to generate random parameters\ndef generate_random_parameters(beta_bounds, mu_bounds, p_bounds):\n    beta = np.random.uniform(low=beta_bounds[0][0], high=beta_bounds[0][1], size=len(beta_guess.flatten()))\n    mu = np.random.uniform(low=mu_bounds[0][0], high=mu_bounds[0][1], size=len(mu_guess.flatten()))\n    p = np.random.uniform(low=p_bounds[0][0], high=p_bounds[0][1], size=1)\n    return np.concatenate((beta, mu, p), axis=None)\n\n# Set up the data to fit\nn = model_input.n\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]*len(beta_guess.flatten())\nmu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0,1)]   # example bounds for p\n\n# Initialize variables to store the best parameters and minimum error\nmin_error = np.inf\nbest_parameters = None\n\n# Initialize a list to store the parameters for each simulation\nparameters_samples = []\n\n# Run Monte Carlo simulation for a specified number of iterations\nnum_iterations = 50\nfor i in range(num_iterations):\n    # Generate random parameters\n    parameters = generate_random_parameters(beta_bounds, mu_bounds, p_bounds)\n    \n    # Calculate the objective function with these parameters\n    error = objectiveFunction(parameters, example, 'ParishName', 0)\n    \n    # If this error is less than the current minimum, update the minimum and best parameters\n    if error &lt; min_error:\n        min_error = error\n        best_parameters = parameters\n    \n    # Store the parameters\n    parameters_samples.append(parameters)  \n\n# Convert the list of parameters to a numpy array\nparameters_samples = np.array(parameters_samples)\n\n# Calculate the mean and standard deviation of the parameters\nmean_parameters = np.mean(parameters_samples, axis=0)\nstd_parameters = np.std(parameters_samples, axis=0)\n\n# Calculate the 2.5th and 97.5th percentiles of the parameters to get their 95% confidence interval\nconf_intervals = np.percentile(parameters_samples, [2.5, 97.5], axis=0)\n\n# Extract estimated parameters\nbeta_estimated = best_parameters[:n].reshape(n,)\nmu_estimated = best_parameters[n:2*n].reshape(n,)\np_estimated = best_parameters[2*n]\n\nprint(\"Minimum error = \", min_error)\nprint(\"Estimated beta = \", beta_estimated)\nprint(\"Estimated mu = \", mu_estimated)\nprint(\"Estimated p = \", p_estimated)\nprint(\"Mean parameters = \", mean_parameters)\nprint(\"Standard deviation of parameters = \", std_parameters)\nprint(\"95% confidence intervals of parameters = \", conf_intervals)\n</pre> # Define a function to generate random parameters def generate_random_parameters(beta_bounds, mu_bounds, p_bounds):     beta = np.random.uniform(low=beta_bounds[0][0], high=beta_bounds[0][1], size=len(beta_guess.flatten()))     mu = np.random.uniform(low=mu_bounds[0][0], high=mu_bounds[0][1], size=len(mu_guess.flatten()))     p = np.random.uniform(low=p_bounds[0][0], high=p_bounds[0][1], size=1)     return np.concatenate((beta, mu, p), axis=None)  # Set up the data to fit n = model_input.n  # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p  # Define the bounds for beta, mu and p beta_bounds = [(0,1)]*len(beta_guess.flatten()) mu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0,1)]   # example bounds for p  # Initialize variables to store the best parameters and minimum error min_error = np.inf best_parameters = None  # Initialize a list to store the parameters for each simulation parameters_samples = []  # Run Monte Carlo simulation for a specified number of iterations num_iterations = 50 for i in range(num_iterations):     # Generate random parameters     parameters = generate_random_parameters(beta_bounds, mu_bounds, p_bounds)          # Calculate the objective function with these parameters     error = objectiveFunction(parameters, example, 'ParishName', 0)          # If this error is less than the current minimum, update the minimum and best parameters     if error &lt; min_error:         min_error = error         best_parameters = parameters          # Store the parameters     parameters_samples.append(parameters)    # Convert the list of parameters to a numpy array parameters_samples = np.array(parameters_samples)  # Calculate the mean and standard deviation of the parameters mean_parameters = np.mean(parameters_samples, axis=0) std_parameters = np.std(parameters_samples, axis=0)  # Calculate the 2.5th and 97.5th percentiles of the parameters to get their 95% confidence interval conf_intervals = np.percentile(parameters_samples, [2.5, 97.5], axis=0)  # Extract estimated parameters beta_estimated = best_parameters[:n].reshape(n,) mu_estimated = best_parameters[n:2*n].reshape(n,) p_estimated = best_parameters[2*n]  print(\"Minimum error = \", min_error) print(\"Estimated beta = \", beta_estimated) print(\"Estimated mu = \", mu_estimated) print(\"Estimated p = \", p_estimated) print(\"Mean parameters = \", mean_parameters) print(\"Standard deviation of parameters = \", std_parameters) print(\"95% confidence intervals of parameters = \", conf_intervals)  <pre>Minimum error =  0.34814814814814815\nEstimated beta =  [0.64987973 0.22903583 0.9820245  0.6754689  0.21312594 0.77667866\n 0.38776625 0.97490697 0.66964508 0.51495067 0.37942417 0.03485997]\nEstimated mu =  [0.06640978 0.52115662 0.71420202 0.26744225 0.36450106 0.62123296\n 0.53275761 0.44479214 0.24188825 0.22472121 0.70050237 0.23030614]\nEstimated p =  0.9768510785555305\nMean parameters =  [0.5094387  0.49642896 0.52201132 0.51193832 0.4914296  0.49272524\n 0.49676018 0.486889   0.46513162 0.54841315 0.48916045 0.50664992\n 0.34180942 0.42284044 0.42712716 0.37808865 0.37611523 0.43874171\n 0.43169394 0.37444783 0.40668436 0.43334175 0.33057175 0.40143622\n 0.51276787]\nStandard deviation of parameters =  [0.30414316 0.27806334 0.28691785 0.30992126 0.2794051  0.2767522\n 0.28345168 0.26270727 0.28265976 0.28099455 0.27347315 0.2434955\n 0.24072617 0.25443417 0.22625941 0.24749245 0.22948697 0.23408049\n 0.23561594 0.19623862 0.22875691 0.21172061 0.22285471 0.21824721\n 0.30016039]\n95% confidence intervals of parameters =  [[0.03410103 0.06277107 0.05308607 0.01978452 0.05303913 0.02563719\n  0.03586343 0.03785868 0.0582274  0.05946476 0.02297158 0.01904743\n  0.01729811 0.04155031 0.05595139 0.03450432 0.00777448 0.07940789\n  0.03453855 0.01837825 0.02751558 0.04612431 0.05003071 0.02873585\n  0.03414461]\n [0.97309849 0.9843308  0.99013366 0.96675548 0.93947323 0.9629817\n  0.96288798 0.94258558 0.94492567 0.95269392 0.95585124 0.86732723\n  0.76273936 0.7959553  0.76609842 0.78912031 0.76830684 0.76524707\n  0.7722505  0.74806983 0.77063877 0.75840733 0.70454595 0.7818536\n  0.9776661 ]]\n</pre> <p>Substituting the estimated values into the model and solving it</p> In\u00a0[\u00a0]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Plot to check how the objective function works</p> In\u00a0[\u00a0]: Copied! <pre>plot_infected_parishes(model_solution, example, 'ParishName', 0)\n</pre> plot_infected_parishes(model_solution, example, 'ParishName', 0) <pre>(array([ 29,  60,  91, 121, 152, 182, 213, 244, 272, 303]),\n array([ 1., 10., 12., 12., 11.,  7.,  0.,  0.,  0.,  0.]))</pre> In\u00a0[\u00a0]: Copied! <pre>############################################################################################################\n</pre> ############################################################################################################ <p>Defining the objective functions</p> In\u00a0[\u00a0]: Copied! <pre># # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n\n# def objectiveFunction_2 (model_sol: dict\n#                          , gdf: gpd.GeoDataFrame = example\n#                          , column_name: str = 'ParishName'\n#                          , n: int = 0\n#                          ):\n#     #Group the dataframe by parish name without repetitions\n#     grouped_by_parish = gdf.groupby(column_name)\n#     # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n#     date = gdf.loc[0, 'BeginPlaguePeriod']\n#     # Getting the number of infected parishes per month from the data\n#     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n#     # Initializing the number of infected parishes per month for the model's output\n#     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n#     # Initializing the error between the model's output and the data\n#     error = np.zeros(len(cum_infected_parishes_by_month))\n    \n#     # Computing the number of infected parishes per month from the model's output\n#     for i in range(len(cum_infected_parishes_by_month)):\n#         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n#         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n        \n#         for k in range(len(grouped_by_parish)):\n#             for day in range(init_days, final_days):\n#                 if model_sol['I'][k][day] &gt; 1:\n#                     model_infected_parishes[i] += 1\n#                     break # Breaks the innermost loop when the condition is met\n#         error[i] = (model_infected_parishes[i] \n#                            - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2\n    \n#     max_error = np.max(error)\n#     # Computing the error between the model's output and the data\n#     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))\n          \n#     return (total_error)\n</pre> # # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data  # def objectiveFunction_2 (model_sol: dict #                          , gdf: gpd.GeoDataFrame = example #                          , column_name: str = 'ParishName' #                          , n: int = 0 #                          ): #     #Group the dataframe by parish name without repetitions #     grouped_by_parish = gdf.groupby(column_name) #     # Defining the initial date of the dataframe to start counting the number of infected parishes per month #     date = gdf.loc[0, 'BeginPlaguePeriod'] #     # Getting the number of infected parishes per month from the data #     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n) #     # Initializing the number of infected parishes per month for the model's output #     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month)) #     # Initializing the error between the model's output and the data #     error = np.zeros(len(cum_infected_parishes_by_month))      #     # Computing the number of infected parishes per month from the model's output #     for i in range(len(cum_infected_parishes_by_month)): #         init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate'] #         final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']          #         for k in range(len(grouped_by_parish)): #             for day in range(init_days, final_days): #                 if model_sol['I'][k][day] &gt; 1: #                     model_infected_parishes[i] += 1 #                     break # Breaks the innermost loop when the condition is met #         error[i] = (model_infected_parishes[i]  #                            - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2      #     max_error = np.max(error) #     # Computing the error between the model's output and the data #     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))            #     return (total_error)  In\u00a0[\u00a0]: Copied! <pre># def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0):\n#     parameters = np.array(parameters)\n\n#     n = model_input.n\n#     # Reshape parameters back to their original shapes\n#     beta: np.array = parameters[:n].reshape(n,)\n#     mu:  np.array = parameters[n:2*n].reshape(n,)\n#     p_coeff: float = parameters[2*n]\n#     # p_coeff: np.array = parameters[2*n:].reshape(n, n)\n\n#     # # Penalize if p_coeff is not symmetric or has non-zero diagonal elements\n#     # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):\n#     #     return 1e50\n\n#     # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)\n#     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T\n\n#     model_info = {'model': SEIRD_model,\n#                   'init': {\n#                       'S': model_input.S0,\n#                       'E': model_input.E0,\n#                       'I': model_input.I0,\n#                       'R': model_input.R0,\n#                       'D': model_input.D0,\n#                   },\n#                   'gdf': gdf,\n#                   'beta': beta,\n#                   'p_coeff': p_coeff,\n#                   'mu': mu,\n#                   'gamma': 0.4,\n#                   'sigma': 0.17,\n#                   'bump_center': 0.0,\n#                   'bump_width': 0.0,\n#                   'bump_height': 0.0,\n#                   'N': model_input.patchPop(),\n#                   'n': model_input.n,\n#                   'T': model_input.maxDays()}\n\n#     model_sol = generate_sol(model_info)\n\n#     # Group the dataframe by parish name without repetitions\n#     grouped_by_parish = gdf.groupby(column_name)\n#     # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n#     date = gdf.loc[0, 'BeginPlaguePeriod']\n#     # Getting the number of infected parishes per month from the data\n#     cum_infected_parishes_by_month = count_infected_parishes_by_month(\n#         gdf, date, n)\n#     # Make a list of the days to iterate over we took initial date because we are using infected humans\n#     days = cum_infected_parishes_by_month['DaysFromInitialDate'].values\n#     # Initializing the number of infected parishes per month for the model's output\n#     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n#     # Initializing the error between the model's output and the data\n#     error = np.zeros(len(cum_infected_parishes_by_month))\n\n#     # Initializing a matrix where the rows represents the number of parishes and the columns the number of days\n#     matrix_infected_parishes = np.zeros((len(grouped_by_parish), len(days)))\n\n#     # Computing the number of infected parishes per month from the model's output\n#     # for i in range(len(cum_infected_parishes_by_month)):\n#     #     init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate']\n#     #     final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']\n\n#     for i, day in enumerate(days):\n#         for k in range(len(grouped_by_parish)):\n#             # for day in range(init_days, min(final_days, len(model_sol['I'][k]))):\n#             if model_sol['I'][k][day] &gt;= 1 :\n#                 model_infected_parishes[i] += 1\n#                 break  # Breaks the innermost loop when the condition is met\n#         error[i] = (model_infected_parishes[i]\n#                     - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2\n\n#     max_error = np.max(error)\n#     # Computing the error between the model's output and the data\n#     total_error = (np.sum(error))/(max_error *\n#                                    len(cum_infected_parishes_by_month))\n\n#     return (total_error)\n</pre> # def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName', n: int = 0): #     parameters = np.array(parameters)  #     n = model_input.n #     # Reshape parameters back to their original shapes #     beta: np.array = parameters[:n].reshape(n,) #     mu:  np.array = parameters[n:2*n].reshape(n,) #     p_coeff: float = parameters[2*n] #     # p_coeff: np.array = parameters[2*n:].reshape(n, n)  #     # # Penalize if p_coeff is not symmetric or has non-zero diagonal elements #     # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0): #     #     return 1e50  #     # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1) #     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T  #     model_info = {'model': SEIRD_model, #                   'init': { #                       'S': model_input.S0, #                       'E': model_input.E0, #                       'I': model_input.I0, #                       'R': model_input.R0, #                       'D': model_input.D0, #                   }, #                   'gdf': gdf, #                   'beta': beta, #                   'p_coeff': p_coeff, #                   'mu': mu, #                   'gamma': 0.4, #                   'sigma': 0.17, #                   'bump_center': 0.0, #                   'bump_width': 0.0, #                   'bump_height': 0.0, #                   'N': model_input.patchPop(), #                   'n': model_input.n, #                   'T': model_input.maxDays()}  #     model_sol = generate_sol(model_info)  #     # Group the dataframe by parish name without repetitions #     grouped_by_parish = gdf.groupby(column_name) #     # Defining the initial date of the dataframe to start counting the number of infected parishes per month #     date = gdf.loc[0, 'BeginPlaguePeriod'] #     # Getting the number of infected parishes per month from the data #     cum_infected_parishes_by_month = count_infected_parishes_by_month( #         gdf, date, n) #     # Make a list of the days to iterate over we took initial date because we are using infected humans #     days = cum_infected_parishes_by_month['DaysFromInitialDate'].values #     # Initializing the number of infected parishes per month for the model's output #     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month)) #     # Initializing the error between the model's output and the data #     error = np.zeros(len(cum_infected_parishes_by_month))  #     # Initializing a matrix where the rows represents the number of parishes and the columns the number of days #     matrix_infected_parishes = np.zeros((len(grouped_by_parish), len(days)))  #     # Computing the number of infected parishes per month from the model's output #     # for i in range(len(cum_infected_parishes_by_month)): #     #     init_days = cum_infected_parishes_by_month.loc[i,'DaysFromInitialDate'] #     #     final_days = cum_infected_parishes_by_month.loc[i,'DaysToEndOfMonth']  #     for i, day in enumerate(days): #         for k in range(len(grouped_by_parish)): #             # for day in range(init_days, min(final_days, len(model_sol['I'][k]))): #             if model_sol['I'][k][day] &gt;= 1 : #                 model_infected_parishes[i] += 1 #                 break  # Breaks the innermost loop when the condition is met #         error[i] = (model_infected_parishes[i] #                     - cum_infected_parishes_by_month['NumberInfectedParishes'][i])**2  #     max_error = np.max(error) #     # Computing the error between the model's output and the data #     total_error = (np.sum(error))/(max_error * #                                    len(cum_infected_parishes_by_month))  #     return (total_error)  In\u00a0[375]: Copied! <pre>### Testing the objective function                         \nobjectiveFunction_2(model_dict, example, 'ParishName', 0)\n</pre> ### Testing the objective function                          objectiveFunction_2(model_dict, example, 'ParishName', 0) In\u00a0[267]: Copied! <pre>def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):\n    results = count_infected_parishes_by_month(df, date, n, column_name, start_date, end_date)\n    plt.plot(results['DaysFromInitialDate'], results['NumberInfectedParishes'],\n              label='Number of infected parishes', color='blue')\n    plt.plot(results['DaysToEndOfMonth'], results['NumberInfectedParishes'],\n              label='Number of infected parishes', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of infected parishes')\n    plt.title('South Scania')\n    plt.show()\n    return results\nplot_parishes_by_month(example, 'JUN 1712', 0)\n</pre> def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):     results = count_infected_parishes_by_month(df, date, n, column_name, start_date, end_date)     plt.plot(results['DaysFromInitialDate'], results['NumberInfectedParishes'],               label='Number of infected parishes', color='blue')     plt.plot(results['DaysToEndOfMonth'], results['NumberInfectedParishes'],               label='Number of infected parishes', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of infected parishes')     plt.title('South Scania')     plt.show()     return results plot_parishes_by_month(example, 'JUN 1712', 0) Out[267]: date DaysFromInitialDate NumberInfectedParishes CumInfectParishes EndOfMonth DaysToEndOfMonth InfectedParishes 0 1712-06-01 0 2 2 1712-06-30 29 {\u00d6JA, YSTAD} 1 1712-07-01 30 6 6 1712-07-31 60 {YSTAD, VALLEBERGA, H\u00d6RUP, BJ\u00c4RESJ\u00d6, STORA K\u00d6P... 2 1712-08-01 61 7 10 1712-08-31 91 {GLEMMINGE, YSTAD, INGELSTORP, BROMMA, STORA K... 3 1712-09-01 92 5 12 1712-09-30 121 {YSTAD, HEDESKOGA, STORA K\u00d6PINGE, \u00d6VRABY, \u00d6JA} 4 1712-10-01 122 4 12 1712-10-31 152 {\u00d6JA, YSTAD, HEDESKOGA, STORA K\u00d6PINGE} 5 1712-11-01 153 3 12 1712-11-30 182 {\u00d6JA, YSTAD, STORA K\u00d6PINGE} 6 1712-12-01 183 3 12 1712-12-31 213 {\u00d6JA, YSTAD, STORA K\u00d6PINGE} 7 1713-01-01 214 2 12 1713-01-31 244 {\u00d6JA, STORA K\u00d6PINGE} 8 1713-02-01 245 1 12 1713-02-28 272 {\u00d6JA} 9 1713-03-01 273 1 12 1713-03-31 303 {\u00d6JA} <p>Plotting the daily deaths by parish</p> In\u00a0[618]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[619]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\n# tick_positions = southeastScania['BeginDaysPlague'].values\n# tick_labels = southeastScania['BeginPlaguePeriod'].apply(\n#     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  # tick_positions = southeastScania['BeginDaysPlague'].values # tick_labels = southeastScania['BeginPlaguePeriod'].apply( #     lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Runnning the classic optimization algorithm\n\n # Set up the data to fit\nn = model_input.n\n\n# # Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess), axis=None)\n\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0, 1)]*len(beta_guess.flatten())\nmu_bounds = [(0, 0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0, 1)]   # example bounds for p\n\n# Concatenate the bounds\nbounds = beta_bounds + mu_bounds + p_bounds\n\n# Minimize the objective function to obtain beta, mu, and p\nresult = optimize.minimize(objectiveFunction, x0=initial_parameters, args=(example, 'ParishName', 0), bounds=bounds\n                           )\n\nbeta_estimated = result.x[:n].reshape(n,)\nmu_estimated = result.x[n:2*n].reshape(n,)\np_estimated = result.x[2*n]\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre>  # Runnning the classic optimization algorithm   # Set up the data to fit n = model_input.n  # # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess), axis=None)   # Define the bounds for beta, mu and p beta_bounds = [(0, 1)]*len(beta_guess.flatten()) mu_bounds = [(0, 0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0, 1)]   # example bounds for p  # Concatenate the bounds bounds = beta_bounds + mu_bounds + p_bounds  # Minimize the objective function to obtain beta, mu, and p result = optimize.minimize(objectiveFunction, x0=initial_parameters, args=(example, 'ParishName', 0), bounds=bounds                            )  beta_estimated = result.x[:n].reshape(n,) mu_estimated = result.x[n:2*n].reshape(n,) p_estimated = result.x[2*n]  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) In\u00a0[\u00a0]: Copied! <pre>group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n                 | (southScania['ParishName'] == '\u00d6JA')\n                 | (southScania['ParishName'] == 'BROMMA')\n                 | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (southScania['ParishName'] == 'VALLEBERGA')\n                 | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (southScania['ParishName'] == 'INGELSTORP')\n                 | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (southScania['ParishName'] == 'HEDESKOGA')\n                 #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> group1 = southScania[(southScania['ParishName'] == 'YSTAD')                  | (southScania['ParishName'] == '\u00d6JA')                  | (southScania['ParishName'] == 'BROMMA')                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (southScania['ParishName'] == 'STORA K\u00d6PINGE')                  | (southScania['ParishName'] == 'VALLEBERGA')                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))                  | (southScania['ParishName'] == 'INGELSTORP')                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))                  | (southScania['ParishName'] == 'HEDESKOGA')                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) ]      group1 = group1.reset_index(drop=True) group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'"},{"location":"PlagueProject/FittingSouthScaniaModelObFunct3/","title":"FittingSouthScaniaModelObFunct3","text":"In\u00a0[156]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[157]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[158]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[159]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[160]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[161]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[162]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n\ntype(southScania)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]]  type(southScania) Out[162]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[163]: Copied! <pre>southScania = get_centroid(southScania)\nsouthScania = southScania.replace(['UNDEFINED', '?'], np.nan)\nsouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    southScania['BeginPlaguePeriod'], format='%b %Y')\nsouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    southScania['EndPlaguePeriod'], format='%b %Y')\n\nlen(southScania)\n</pre> southScania = get_centroid(southScania) southScania = southScania.replace(['UNDEFINED', '?'], np.nan) southScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     southScania['BeginPlaguePeriod'], format='%b %Y') southScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     southScania['EndPlaguePeriod'], format='%b %Y')  len(southScania) Out[163]: <pre>235</pre> <p>Defining a group to work with</p> In\u00a0[164]: Copied! <pre>group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n                 | (southScania['ParishName'] == '\u00d6JA')\n                 | (southScania['ParishName'] == 'BROMMA')\n                 | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (southScania['ParishName'] == 'VALLEBERGA')\n                 | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (southScania['ParishName'] == 'INGELSTORP')\n                 | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (southScania['ParishName'] == 'HEDESKOGA')\n                 #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> group1 = southScania[(southScania['ParishName'] == 'YSTAD')                  | (southScania['ParishName'] == '\u00d6JA')                  | (southScania['ParishName'] == 'BROMMA')                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (southScania['ParishName'] == 'STORA K\u00d6PINGE')                  | (southScania['ParishName'] == 'VALLEBERGA')                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))                  | (southScania['ParishName'] == 'INGELSTORP')                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))                  | (southScania['ParishName'] == 'HEDESKOGA')                  #| ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) ]      group1 = group1.reset_index(drop=True) group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED' <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[165]: Copied! <pre>group = group1\nlen(group)\n</pre> group = group1 len(group) Out[165]: <pre>12</pre> In\u00a0[166]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\ngroup = group.replace(['UNDEFINED', '?'], np.nan)\ngroup['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    group['BeginPlaguePeriod'], format='%b %Y')\ngroup['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    group['EndPlaguePeriod'], format='%b %Y')\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe group = group.replace(['UNDEFINED', '?'], np.nan) group['new_format_BeginPlaguePeriod'] = pd.to_datetime(     group['BeginPlaguePeriod'], format='%b %Y') group['new_format_EndPlaguePeriod'] = pd.to_datetime(     group['EndPlaguePeriod'], format='%b %Y') In\u00a0[167]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n# Fix the tYpe of Victims number to numeric\ncluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber'])\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         ) # Fix the tYpe of Victims number to numeric cluster1['VictimsNumber'] = pd.to_numeric(cluster1['VictimsNumber']) <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[168]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf, beta_guess:float, mu_guess:float, p_guess:float):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        self.mu = np.full(self.n, mu_guess)\n        self.beta = np.full(self.n, beta_guess)\n        self.p = p_guess\n\n        self.S0 = np.zeros(self.n)       \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].unique()\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf, beta_guess:float, mu_guess:float, p_guess:float):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         self.mu = np.full(self.n, mu_guess)         self.beta = np.full(self.n, beta_guess)         self.p = p_guess          self.S0 = np.zeros(self.n)                for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].unique()      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()        <p>Generating the differential equations</p> In\u00a0[169]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[170]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # ensure beta is a numpy array of shape n\n    mu = parameters['mu'] # ensure mu is a numpy array of shape n\n    p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    # Ensure p_coeff is symmetric\n    #p_coeff = np.triu(p_coeff,1) + np.tril(p_coeff, -1)\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n\n    beta =  beta_matrix(gdf, beta)\n    p_matrix = transmission_matrix_p(gdf, p_coeff)\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + (beta + p_matrix) \n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n\n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # ensure beta is a numpy array of shape n     mu = parameters['mu'] # ensure mu is a numpy array of shape n     p_coeff = parameters['p_coeff'] # ensure p is a numpy array of shape (n,n)     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      # Ensure p_coeff is symmetric     #p_coeff = np.triu(p_coeff,1) + np.tril(p_coeff, -1)      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])       beta =  beta_matrix(gdf, beta)     p_matrix = transmission_matrix_p(gdf, p_coeff)      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + (beta + p_matrix)       sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)       dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]      derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Trying a small dataframe</p> In\u00a0[171]: Copied! <pre># Selecting specific rows from the dataframe reseting the index\nexample = cluster1\nmodel_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5, p_guess=0.05)\n</pre> # Selecting specific rows from the dataframe reseting the index example = cluster1 model_input = Initial_Model(example, beta_guess=0.5, mu_guess=0.5, p_guess=0.05) <p>Defining the optimization problem:</p> In\u00a0[172]: Copied! <pre>def objectiveFunction(parameters\n                      , gdf: gpd.GeoDataFrame = example\n                      , column_name: str = 'ParishName'\n                      ):\n    parameters = np.array(parameters)\n\n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n].reshape(n,)\n    mu:  np.array = parameters[n:2*n].reshape(n,)\n    p_coeff: np.array = parameters[2*n:]\n\n    # Penalize if p_coeff is not symmetric or has non-zero diagonal elements\n    # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):\n    #     return 1e50\n\n    # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)\n    # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p_coeff': p_coeff,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    \n    # Getting the number of deaths per month from the data\n    cum_deaths_by_month = count_victims_by_month(gdf)\n\n    # Remove rows where 'EndMonth' is null\n    cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])\n\n    # Initializing the cum. number of deaths per month for the model's output\n    model_deaths_month = np.zeros(len(cum_deaths_by_month))\n    model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))\n\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_deaths_by_month))\n    \n    # Computing the number of cum. deaths per month from the model's output\n    for i in range(len(cum_deaths_by_month)):\n        day = cum_deaths_by_month['CumDays'][i]\n        data = cum_deaths_by_month['CumDeaths'][i]\n            \n        for k in range(len(grouped_by_parish)):\n            model_deaths_month[i] += model_sol['D'][k][day]\n        \n        model_cum_deaths_month[i] = model_deaths_month[i]   \n\n        if i &gt; 0:\n            model_cum_deaths_month[i] += model_cum_deaths_month[i-1] \n        \n        error[i] = (model_deaths_month[i] - data)**2\n        \n    max_error = np.max(error)    \n    # Computing the error between the model's output and the data\n    total_error = (np.sum(error))/(len(error)* max_error)\n          \n    return (total_error)\n</pre> def objectiveFunction(parameters                       , gdf: gpd.GeoDataFrame = example                       , column_name: str = 'ParishName'                       ):     parameters = np.array(parameters)      n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n].reshape(n,)     mu:  np.array = parameters[n:2*n].reshape(n,)     p_coeff: np.array = parameters[2*n:]      # Penalize if p_coeff is not symmetric or has non-zero diagonal elements     # if not (p_coeff.transpose() == p_coeff).all() or np.any(np.diag(p_coeff) != 0):     #     return 1e50      # p_coeff_lower = np.tril(parameters[2*n:].reshape(n, n), -1)     # p_coeff: np.array = p_coeff_lower + p_coeff_lower.T      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p_coeff': p_coeff,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)          # Getting the number of deaths per month from the data     cum_deaths_by_month = count_victims_by_month(gdf)      # Remove rows where 'EndMonth' is null     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])      # Initializing the cum. number of deaths per month for the model's output     model_deaths_month = np.zeros(len(cum_deaths_by_month))     model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))      # Initializing the error between the model's output and the data     error = np.zeros(len(cum_deaths_by_month))          # Computing the number of cum. deaths per month from the model's output     for i in range(len(cum_deaths_by_month)):         day = cum_deaths_by_month['CumDays'][i]         data = cum_deaths_by_month['CumDeaths'][i]                      for k in range(len(grouped_by_parish)):             model_deaths_month[i] += model_sol['D'][k][day]                  model_cum_deaths_month[i] = model_deaths_month[i]             if i &gt; 0:             model_cum_deaths_month[i] += model_cum_deaths_month[i-1]                   error[i] = (model_deaths_month[i] - data)**2              max_error = np.max(error)         # Computing the error between the model's output and the data     total_error = (np.sum(error))/(len(error)* max_error)                return (total_error)         In\u00a0[173]: Copied! <pre># Define a function to generate random parameters\ndef generate_random_parameters(beta_bounds, mu_bounds, p_bounds):\n    beta = np.random.uniform(low=beta_bounds[0][0], high=beta_bounds[0][1], size=len(beta_guess.flatten()))\n    mu = np.random.uniform(low=mu_bounds[0][0], high=mu_bounds[0][1], size=len(mu_guess.flatten()))\n    p = np.random.uniform(low=p_bounds[0][0], high=p_bounds[0][1], size=1)\n    return np.concatenate((beta, mu, p), axis=None)\n\n# Set up the data to fit\nn = model_input.n\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]*len(beta_guess.flatten())\nmu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0,1)]   # example bounds for p\n\n# Initialize variables to store the best parameters and minimum error\nmin_error = np.inf\nbest_parameters = None\n\n# Initialize a list to store the parameters for each simulation\nparameters_samples = []\n\n# Run Monte Carlo simulation for a specified number of iterations\nnum_iterations = 100\nfor i in range(num_iterations):\n    # Generate random parameters\n    parameters = generate_random_parameters(beta_bounds, mu_bounds, p_bounds)\n    \n    # Calculate the objective function with these parameters\n    error = objectiveFunction(parameters, example, 'ParishName')\n    \n    # If this error is less than the current minimum, update the minimum and best parameters\n    if error &lt; min_error:\n        min_error = error\n        best_parameters = parameters\n    \n    # Store the parameters\n    parameters_samples.append(parameters)  \n\n# Convert the list of parameters to a numpy array\nparameters_samples = np.array(parameters_samples)\n\n# Calculate the mean and standard deviation of the parameters\nmean_parameters = np.mean(parameters_samples, axis=0)\nstd_parameters = np.std(parameters_samples, axis=0)\n\n# Calculate the 2.5th and 97.5th percentiles of the parameters to get their 95% confidence interval\nconf_intervals = np.percentile(parameters_samples, [2.5, 97.5], axis=0)\n\n# Extract estimated parameters\nbeta_estimated = best_parameters[:n].reshape(n,)\nmu_estimated = best_parameters[n:2*n].reshape(n,)\np_estimated = best_parameters[2*n]\n\nprint(\"Minimum error = \", min_error)\nprint(\"Estimated beta = \", beta_estimated)\nprint(\"Estimated mu = \", mu_estimated)\nprint(\"Estimated p = \", p_estimated)\nprint(\"Mean parameters = \", mean_parameters)\nprint(\"Standard deviation of parameters = \", std_parameters)\nprint(\"95% confidence intervals of parameters = \", conf_intervals)\n</pre> # Define a function to generate random parameters def generate_random_parameters(beta_bounds, mu_bounds, p_bounds):     beta = np.random.uniform(low=beta_bounds[0][0], high=beta_bounds[0][1], size=len(beta_guess.flatten()))     mu = np.random.uniform(low=mu_bounds[0][0], high=mu_bounds[0][1], size=len(mu_guess.flatten()))     p = np.random.uniform(low=p_bounds[0][0], high=p_bounds[0][1], size=1)     return np.concatenate((beta, mu, p), axis=None)  # Set up the data to fit n = model_input.n  # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p  # Define the bounds for beta, mu and p beta_bounds = [(0,1)]*len(beta_guess.flatten()) mu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0,1)]   # example bounds for p  # Initialize variables to store the best parameters and minimum error min_error = np.inf best_parameters = None  # Initialize a list to store the parameters for each simulation parameters_samples = []  # Run Monte Carlo simulation for a specified number of iterations num_iterations = 100 for i in range(num_iterations):     # Generate random parameters     parameters = generate_random_parameters(beta_bounds, mu_bounds, p_bounds)          # Calculate the objective function with these parameters     error = objectiveFunction(parameters, example, 'ParishName')          # If this error is less than the current minimum, update the minimum and best parameters     if error &lt; min_error:         min_error = error         best_parameters = parameters          # Store the parameters     parameters_samples.append(parameters)    # Convert the list of parameters to a numpy array parameters_samples = np.array(parameters_samples)  # Calculate the mean and standard deviation of the parameters mean_parameters = np.mean(parameters_samples, axis=0) std_parameters = np.std(parameters_samples, axis=0)  # Calculate the 2.5th and 97.5th percentiles of the parameters to get their 95% confidence interval conf_intervals = np.percentile(parameters_samples, [2.5, 97.5], axis=0)  # Extract estimated parameters beta_estimated = best_parameters[:n].reshape(n,) mu_estimated = best_parameters[n:2*n].reshape(n,) p_estimated = best_parameters[2*n]  print(\"Minimum error = \", min_error) print(\"Estimated beta = \", beta_estimated) print(\"Estimated mu = \", mu_estimated) print(\"Estimated p = \", p_estimated) print(\"Mean parameters = \", mean_parameters) print(\"Standard deviation of parameters = \", std_parameters) print(\"95% confidence intervals of parameters = \", conf_intervals)  <pre>Minimum error =  0.23137350349576186\nEstimated beta =  [0.53441291 0.722358   0.12715934 0.64192401 0.28722388 0.0491385\n 0.0644983  0.47391367 0.0789018  0.66191532 0.05822667 0.43662295]\nEstimated mu =  [0.78502917 0.45020428 0.20023834 0.1281377  0.11736029 0.72540797\n 0.30590914 0.26165517 0.50311697 0.16991869 0.71781378 0.20337174]\nEstimated p =  0.37341623714341254\nMean parameters =  [0.5488443  0.53828387 0.51967097 0.48253826 0.51925217 0.52613978\n 0.49063127 0.50861794 0.46950087 0.53833033 0.5035516  0.54167049\n 0.43796173 0.43887893 0.39869425 0.39184858 0.4232503  0.39333341\n 0.39292604 0.41305602 0.42123947 0.34665152 0.4477717  0.36748111\n 0.47754095]\nStandard deviation of parameters =  [0.28265889 0.29055301 0.29940741 0.27615079 0.2776118  0.28248901\n 0.27209923 0.2794085  0.28505325 0.28629689 0.28918863 0.2940708\n 0.23228595 0.21343421 0.21706032 0.21888009 0.24448217 0.25129733\n 0.21451707 0.23361937 0.22102245 0.21670465 0.22999196 0.22273592\n 0.28219689]\n95% confidence intervals of parameters =  [[0.0398765  0.02798684 0.02204221 0.03005552 0.05693398 0.02977254\n  0.04605064 0.03619771 0.01743543 0.03361788 0.01678835 0.04434066\n  0.02835366 0.07021851 0.01190424 0.03836569 0.01415239 0.00711441\n  0.03798839 0.03731668 0.01737885 0.02793462 0.04976984 0.03391423\n  0.01900652]\n [0.98546977 0.97116345 0.96820007 0.92581638 0.9773313  0.97594488\n  0.92796655 0.96963396 0.92891485 0.97777391 0.95233624 0.98676755\n  0.7836854  0.76031836 0.76041675 0.74228179 0.79362909 0.78138652\n  0.77386815 0.78616301 0.79203838 0.7184371  0.77734268 0.78281894\n  0.95064264]]\n</pre> In\u00a0[174]: Copied! <pre># from pyDOE import lhs\n\n# # Define a function to generate parameters using Latin Hypercube Sampling\n# def generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, size):\n#     beta = lhs(n=len(beta_guess.flatten()), samples=size)\n#     beta = beta * (beta_bounds[0][1] - beta_bounds[0][0]) + beta_bounds[0][0]\n    \n#     mu = lhs(n=len(mu_guess.flatten()), samples=size)\n#     mu = mu * (mu_bounds[0][1] - mu_bounds[0][0]) + mu_bounds[0][0]\n    \n#     p = lhs(n=1, samples=size)\n#     p = p * (p_bounds[0][1] - p_bounds[0][0]) + p_bounds[0][0]\n    \n#     return np.concatenate((beta, mu, p), axis=1)\n\n# # Set up the data to fit\n# n = model_input.n\n\n# # Choose initial guesses for the parameters to fit\n# beta_guess = model_input.beta\n# mu_guess = model_input.mu\n# p_guess = model_input.p\n\n# # Define the bounds for beta, mu and p\n# beta_bounds = [(0,1)]*len(beta_guess.flatten())\n# mu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu\n# p_bounds = [(0,1)]   # example bounds for p\n\n# num_iterations = 2\n\n# # Generate parameters using LHS\n# parameters_samples = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, num_iterations)\n\n# # Initialize variables to store the best parameters and minimum error\n# min_error = np.inf\n# best_parameters = None\n\n# # Initialize a list to store the parameters for each simulation\n# parameters_samples = []\n\n# # Run Monte Carlo simulation for a specified number of iterations\n# num_iterations = 2\n# for i in range(num_iterations):\n#     # Generate random parameters\n#     parameters = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds,5)\n    \n#     # Calculate the objective function with these parameters\n#     error = objectiveFunction(parameters, example, 'ParishName')\n    \n#     # If this error is less than the current minimum, update the minimum and best parameters\n#     if error &lt; min_error:\n#         min_error = error\n#         best_parameters = parameters\n    \n#     # Store the parameters\n#     parameters_samples.append(parameters)  \n\n# # Convert the list of parameters to a numpy array\n# parameters_samples = np.array(parameters_samples)\n\n# # Calculate the mean and standard deviation of the parameters\n# mean_parameters = np.mean(parameters_samples, axis=0)\n# std_parameters = np.std(parameters_samples, axis=0)\n\n# # Calculate the 2.5th and 97.5th percentiles of the parameters to get their 95% confidence interval\n# conf_intervals = np.percentile(parameters_samples, [2.5, 97.5], axis=0)\n\n# # Extract estimated parameters\n# beta_estimated = best_parameters[:n].reshape(n,)\n# mu_estimated = best_parameters[n:2*n].reshape(n,)\n# p_estimated = best_parameters[2*n]\n\n# print(\"Minimum error = \", min_error)\n# print(\"Estimated beta = \", beta_estimated)\n# print(\"Estimated mu = \", mu_estimated)\n# print(\"Estimated p = \", p_estimated)\n# print(\"Mean parameters = \", mean_parameters)\n# print(\"Standard deviation of parameters = \", std_parameters)\n# print(\"95% confidence intervals of parameters = \", conf_intervals)\n</pre> # from pyDOE import lhs  # # Define a function to generate parameters using Latin Hypercube Sampling # def generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, size): #     beta = lhs(n=len(beta_guess.flatten()), samples=size) #     beta = beta * (beta_bounds[0][1] - beta_bounds[0][0]) + beta_bounds[0][0]      #     mu = lhs(n=len(mu_guess.flatten()), samples=size) #     mu = mu * (mu_bounds[0][1] - mu_bounds[0][0]) + mu_bounds[0][0]      #     p = lhs(n=1, samples=size) #     p = p * (p_bounds[0][1] - p_bounds[0][0]) + p_bounds[0][0]      #     return np.concatenate((beta, mu, p), axis=1)  # # Set up the data to fit # n = model_input.n  # # Choose initial guesses for the parameters to fit # beta_guess = model_input.beta # mu_guess = model_input.mu # p_guess = model_input.p  # # Define the bounds for beta, mu and p # beta_bounds = [(0,1)]*len(beta_guess.flatten()) # mu_bounds = [(0,0.8)]*len(mu_guess.flatten())  # example bounds for mu # p_bounds = [(0,1)]   # example bounds for p  # num_iterations = 2  # # Generate parameters using LHS # parameters_samples = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds, num_iterations)  # # Initialize variables to store the best parameters and minimum error # min_error = np.inf # best_parameters = None  # # Initialize a list to store the parameters for each simulation # parameters_samples = []  # # Run Monte Carlo simulation for a specified number of iterations # num_iterations = 2 # for i in range(num_iterations): #     # Generate random parameters #     parameters = generate_lhs_parameters(beta_bounds, mu_bounds, p_bounds,5)      #     # Calculate the objective function with these parameters #     error = objectiveFunction(parameters, example, 'ParishName')      #     # If this error is less than the current minimum, update the minimum and best parameters #     if error &lt; min_error: #         min_error = error #         best_parameters = parameters      #     # Store the parameters #     parameters_samples.append(parameters)    # # Convert the list of parameters to a numpy array # parameters_samples = np.array(parameters_samples)  # # Calculate the mean and standard deviation of the parameters # mean_parameters = np.mean(parameters_samples, axis=0) # std_parameters = np.std(parameters_samples, axis=0)  # # Calculate the 2.5th and 97.5th percentiles of the parameters to get their 95% confidence interval # conf_intervals = np.percentile(parameters_samples, [2.5, 97.5], axis=0)  # # Extract estimated parameters # beta_estimated = best_parameters[:n].reshape(n,) # mu_estimated = best_parameters[n:2*n].reshape(n,) # p_estimated = best_parameters[2*n]  # print(\"Minimum error = \", min_error) # print(\"Estimated beta = \", beta_estimated) # print(\"Estimated mu = \", mu_estimated) # print(\"Estimated p = \", p_estimated) # print(\"Mean parameters = \", mean_parameters) # print(\"Standard deviation of parameters = \", std_parameters) # print(\"95% confidence intervals of parameters = \", conf_intervals)  <p>Substituting the estimated values into the model and solving it</p> In\u00a0[175]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p_coeff':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p_coeff':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[176]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[177]: Copied! <pre># Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\n\ndef plot_cum_deaths_model(model_solution\n                          , beta_estimated\n                            , mu_estimated\n                            , p_estimated\n                            , gdf: gpd.GeoDataFrame = example\n                            , column_name: str = 'ParishName'\n                            ):\n      \n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n\n    # Getting the number of deaths per month from the data\n    cum_deaths_by_month = count_victims_by_month(gdf)\n\n    # Remove rows where 'EndMonth' is null\n    cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])\n\n    # Now, you can directly get the 'CumDays' and 'CumDeaths' without looping and checking for nulls\n    days = cum_deaths_by_month['CumDays'].values\n    cum_deaths = cum_deaths_by_month['CumDeaths'].values\n\n    # Initializing the cum. number of deaths per month for the model's output\n    model_deaths_month = np.zeros(len(cum_deaths_by_month))\n    model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))\n\n        \n    # Computing the number of cum. deaths per month from the model's output\n    for i in range(len(cum_deaths_by_month)):\n        day = cum_deaths_by_month['CumDays'][i]\n                   \n        for k in range(len(grouped_by_parish)):\n            model_deaths_month[i] += model_solution['D'][k][day]\n                \n        model_cum_deaths_month[i] = model_deaths_month[i]   \n\n        if i &gt; 0:\n            model_cum_deaths_month[i] += model_cum_deaths_month[i-1] \n       \n           \n    plt.plot(days, model_cum_deaths_month, color='blue') \n    plt.plot(days, cum_deaths, label='Number of infected parishes', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Cumulative Deaths')\n    plt.title('South Scania')\n    plt.show()         \n    return (model_cum_deaths_month, days, cum_deaths)\n</pre> # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data  def plot_cum_deaths_model(model_solution                           , beta_estimated                             , mu_estimated                             , p_estimated                             , gdf: gpd.GeoDataFrame = example                             , column_name: str = 'ParishName'                             ):            #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)      # Getting the number of deaths per month from the data     cum_deaths_by_month = count_victims_by_month(gdf)      # Remove rows where 'EndMonth' is null     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])      # Now, you can directly get the 'CumDays' and 'CumDeaths' without looping and checking for nulls     days = cum_deaths_by_month['CumDays'].values     cum_deaths = cum_deaths_by_month['CumDeaths'].values      # Initializing the cum. number of deaths per month for the model's output     model_deaths_month = np.zeros(len(cum_deaths_by_month))     model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))               # Computing the number of cum. deaths per month from the model's output     for i in range(len(cum_deaths_by_month)):         day = cum_deaths_by_month['CumDays'][i]                             for k in range(len(grouped_by_parish)):             model_deaths_month[i] += model_solution['D'][k][day]                          model_cum_deaths_month[i] = model_deaths_month[i]             if i &gt; 0:             model_cum_deaths_month[i] += model_cum_deaths_month[i-1]                          plt.plot(days, model_cum_deaths_month, color='blue')      plt.plot(days, cum_deaths, label='Number of infected parishes', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Cumulative Deaths')     plt.title('South Scania')     plt.show()              return (model_cum_deaths_month, days, cum_deaths)  In\u00a0[178]: Copied! <pre>plot_cum_deaths_model(model_solution, beta_estimated, mu_estimated, p_estimated, example, 'ParishName')\n</pre> plot_cum_deaths_model(model_solution, beta_estimated, mu_estimated, p_estimated, example, 'ParishName') Out[178]: <pre>(array([ 130.0507343 ,  434.97479935,  981.27260676, 1786.21538646,\n        2625.97192015, 3483.68250267]),\n array([ 91, 121, 152, 213, 244, 303]),\n array([  0,   0,   5, 745, 825, 865]))</pre>"},{"location":"PlagueProject/GenerateAnimations/","title":"GenerateAnimations","text":"In\u00a0[2]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[3]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 In\u00a0[4]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[5]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[6]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[7]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] In\u00a0[8]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) In\u00a0[9]: Copied! <pre># Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n# Create a GeoDataFrame from the DataFrame\nsouthScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry')\n</pre> # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads) # Create a GeoDataFrame from the DataFrame southScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry') <p>Plotting the parishes from South Scania</p> In\u00a0[10]: Copied! <pre># Assigning the coordinate reference system (CRS) to the GeoDataFrame\nsouthScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")\n\n# For checking the CRS\n# southScaniaMap.crs\n</pre> # Assigning the coordinate reference system (CRS) to the GeoDataFrame southScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")  # For checking the CRS # southScaniaMap.crs In\u00a0[11]: Copied! <pre>colorByColumn(southScaniaMap, 'EndPlaguePeriod')\nsouthMap : folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nsouthScaniaMap.explore(\n    m = southMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"South Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    southMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(southMap)  # use folium to add layer control\n\nsouthMap  # show map\n</pre> colorByColumn(southScaniaMap, 'EndPlaguePeriod') southMap : folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  southScaniaMap.explore(     m = southMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"South Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     southMap )  # use folium to add alternative tiles folium.LayerControl().add_to(southMap)  # use folium to add layer control  southMap  # show map Out[11]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook <p>Add color according to the classification suggested by Tommy: (1) Church book exists and has information on plague deaths (2) Church book exists but no information on plague deaths (3) The church book is lacking for the plague years (4) Information on plague in other sources (5) No information about plague in other sources</p> In\u00a0[12]: Copied! <pre>from matplotlib.lines import Line2D\n# 2 and 5 '#fc8d59'\n# 3 and 5 '#d73027'\ndef assign_colors(row):\n    if row['ChurchBook'] == 1 and row['OtherSources'] == 4:\n        return '#56B4E9'\n    if row['ChurchBook'] == 1 and row['OtherSources'] == 5:\n        return '#009E73'\n    if row['ChurchBook'] == 2 and row['OtherSources'] == 4:\n        return '#E69F00'\n    if row['ChurchBook'] == 3 and row['OtherSources'] == 4:\n        return '#F0E442'\n    if row['ChurchBook'] == 2 and row['OtherSources'] == 5:\n        return 'lightgray'\n    elif row['ChurchBook'] == 3 and row['OtherSources'] == 5:\n        return 'lavender'\n    else:\n        return 'white'\n\nsouthScaniaMap['classification'] = southScaniaMap.apply(assign_colors, axis=1)\n# extract the data we're interested in\nclassification = list(southScaniaMap['classification'].values)\n\ncustom_lines = [Line2D([0], [0], color='#56B4E9', lw=3),\n                Line2D([0], [0], color='#009E73', lw=3),\n                Line2D([0], [0], color='#E69F00', lw=3),\n                Line2D([0], [0], color='#F0E442', lw=3),\n                Line2D([0], [0], color='lightgray', lw=3),\n                Line2D([0], [0], color='lavender', lw=3)]\n    \nfig, ax = plt.subplots(figsize=(8,5))\nsouthScaniaMap.plot(ax=ax, color=classification, edgecolor='dimgray', legend=True)\n\n# Additional title at the bottom\nfig.text(0.5, 0.05, 'SouthScania', ha='center', va='center', fontsize=24, fontweight='bold')\n\nplt.axis('off')\n\nax.legend(custom_lines, ['1 and 4', '1 and 5', '2 and 4', '3 and 4', '2 and 5', '3 and 5'], fontsize=9)\nplt.savefig('class_info.png'.format(), dpi=300)\nplt.close()\n</pre> from matplotlib.lines import Line2D # 2 and 5 '#fc8d59' # 3 and 5 '#d73027' def assign_colors(row):     if row['ChurchBook'] == 1 and row['OtherSources'] == 4:         return '#56B4E9'     if row['ChurchBook'] == 1 and row['OtherSources'] == 5:         return '#009E73'     if row['ChurchBook'] == 2 and row['OtherSources'] == 4:         return '#E69F00'     if row['ChurchBook'] == 3 and row['OtherSources'] == 4:         return '#F0E442'     if row['ChurchBook'] == 2 and row['OtherSources'] == 5:         return 'lightgray'     elif row['ChurchBook'] == 3 and row['OtherSources'] == 5:         return 'lavender'     else:         return 'white'  southScaniaMap['classification'] = southScaniaMap.apply(assign_colors, axis=1) # extract the data we're interested in classification = list(southScaniaMap['classification'].values)  custom_lines = [Line2D([0], [0], color='#56B4E9', lw=3),                 Line2D([0], [0], color='#009E73', lw=3),                 Line2D([0], [0], color='#E69F00', lw=3),                 Line2D([0], [0], color='#F0E442', lw=3),                 Line2D([0], [0], color='lightgray', lw=3),                 Line2D([0], [0], color='lavender', lw=3)]      fig, ax = plt.subplots(figsize=(8,5)) southScaniaMap.plot(ax=ax, color=classification, edgecolor='dimgray', legend=True)  # Additional title at the bottom fig.text(0.5, 0.05, 'SouthScania', ha='center', va='center', fontsize=24, fontweight='bold')  plt.axis('off')  ax.legend(custom_lines, ['1 and 4', '1 and 5', '2 and 4', '3 and 4', '2 and 5', '3 and 5'], fontsize=9) plt.savefig('class_info.png'.format(), dpi=300) plt.close() In\u00a0[13]: Copied! <pre>def assign_colors(row):\n    if row['ChurchBook'] == 2 and row['OtherSources'] == 5:\n        return 'lightgray'\n    elif row['ChurchBook'] == 3 and row['OtherSources'] == 5:\n        return 'lavender'\n    else:\n        return 'white'\n\nsouthScaniaMap['classification'] = southScaniaMap.apply(assign_colors, axis=1)\n# extract the data we're interested in\nclassification = list(southScaniaMap['classification'].values)\n</pre> def assign_colors(row):     if row['ChurchBook'] == 2 and row['OtherSources'] == 5:         return 'lightgray'     elif row['ChurchBook'] == 3 and row['OtherSources'] == 5:         return 'lavender'     else:         return 'white'  southScaniaMap['classification'] = southScaniaMap.apply(assign_colors, axis=1) # extract the data we're interested in classification = list(southScaniaMap['classification'].values) <p>We will focus only in the parishes affected by the plague. To do so, first we filter the data frame.</p> In\u00a0[14]: Copied! <pre>plagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red']\nlen(plagueSouthScania)\n</pre> plagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red'] len(plagueSouthScania) Out[14]: <pre>88</pre> <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[15]: Copied! <pre>plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan)\nplagueSouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['BeginPlaguePeriod'], format='%b %Y')\nplagueSouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['EndPlaguePeriod'], format='%b %Y')\n</pre> plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan) plagueSouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['BeginPlaguePeriod'], format='%b %Y') plagueSouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['EndPlaguePeriod'], format='%b %Y') In\u00a0[16]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\nsouthPlague = get_centroid(add_Begin_End_days(sort_by_date(plagueSouthScania), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\n</pre> # Getting the centroid of each polygon for defining the transmission matrix southPlague = get_centroid(add_Begin_End_days(sort_by_date(plagueSouthScania), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) In\u00a0[30]: Copied! <pre>southPlague[(southPlague[\"ParishName\"]=='STORA HAMMAR')]\n</pre> southPlague[(southPlague[\"ParishName\"]=='STORA HAMMAR')] Out[30]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry color classification new_format_BeginPlaguePeriod new_format_EndPlaguePeriod BeginDaysPlague EndDaysPlague centroid 79 SOUTHWEST SKYTTS STORA HAMMAR STORA HAMMARS 165 186 175.5 1 5 JAN 1713 JAN 1713 1 POLYGON ((4180980.622 3171241.055, 4181008.798... red white 1713-01-01 1713-01-01 731 761 POINT (4182485.9874313883 3172727.4783813185) <p>Creating an animation based on the beginning plague period without concatenating the frames:</p> In\u00a0[50]: Copied! <pre>import matplotlib.animation as animation\nimport matplotlib.patches as mpatches\n\n# Group the DataFrame by 'BeginDaysPlague'\ngroups = list(southPlague.groupby('BeginDaysPlague'))\ndates = [groups[i][1]['BeginPlaguePeriod'].iloc[0] for i in range(len(groups))]\n\n# Define a list of colors of the same length as the number of groups\ncolor_list = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n              '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n              '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n              '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5',\n              '#7fc97f']\n\n# Define the figure and axis\nfig, ax = plt.subplots(figsize=(13, 10))\n\ndef animate(i):\n    ax.clear()\n    # Plot the base map\n    SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray')\n    southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')\n\n    legend_elements = []\n\n    # Plot all groups up to the ith group\n    for j in range(i+1):\n        # Get the jth group\n        group_j = southPlague.loc[southPlague['BeginPlaguePeriod'] == dates[j]]\n        \n        # Plot the group\n        group_j.plot(ax=ax, color=color_list[j], edgecolor='black')\n\n        # Add names to each polygon\n        for x, y, label in zip(group_j.geometry.centroid.x, group_j.geometry.centroid.y, group_j['ParishName']):\n            ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n        # Create a Patch for the legend\n        legend_elements.append(mpatches.Patch(facecolor=color_list[j], edgecolor='black', label=dates[j]))\n\n    # Add the legend manually\n    ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n\n    # Set the title\n    ax.set_title('{}'.format(dates[i]), fontdict={'fontsize': '24', 'fontweight' : '10'})\n\n    plt.xlabel('Meters')\n    plt.ylabel('Meters')\n\nani = animation.FuncAnimation(fig, animate, frames=len(groups), repeat=True, interval=100000)\nani.save('plaguebyinitialmonth.gif', writer='imagemagick', fps=1)\nplt.show()\n</pre> import matplotlib.animation as animation import matplotlib.patches as mpatches  # Group the DataFrame by 'BeginDaysPlague' groups = list(southPlague.groupby('BeginDaysPlague')) dates = [groups[i][1]['BeginPlaguePeriod'].iloc[0] for i in range(len(groups))]  # Define a list of colors of the same length as the number of groups color_list = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',               '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',               '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',               '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5',               '#7fc97f']  # Define the figure and axis fig, ax = plt.subplots(figsize=(13, 10))  def animate(i):     ax.clear()     # Plot the base map     SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray')     southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')      legend_elements = []      # Plot all groups up to the ith group     for j in range(i+1):         # Get the jth group         group_j = southPlague.loc[southPlague['BeginPlaguePeriod'] == dates[j]]                  # Plot the group         group_j.plot(ax=ax, color=color_list[j], edgecolor='black')          # Add names to each polygon         for x, y, label in zip(group_j.geometry.centroid.x, group_j.geometry.centroid.y, group_j['ParishName']):             ax.text(x, y, label, fontsize=5, ha='center', va='center')          # Create a Patch for the legend         legend_elements.append(mpatches.Patch(facecolor=color_list[j], edgecolor='black', label=dates[j]))      # Add the legend manually     ax.legend(handles=legend_elements, loc='upper right', fontsize=9)      # Set the title     ax.set_title('{}'.format(dates[i]), fontdict={'fontsize': '24', 'fontweight' : '10'})      plt.xlabel('Meters')     plt.ylabel('Meters')  ani = animation.FuncAnimation(fig, animate, frames=len(groups), repeat=True, interval=100000) ani.save('plaguebyinitialmonth.gif', writer='imagemagick', fps=1) plt.show() <pre>MovieWriter imagemagick unavailable; using Pillow instead.\n</pre> <p>Generating the individual frames of the previous animation:</p> In\u00a0[18]: Copied! <pre>import matplotlib.patches as mpatches\n\n# Group the DataFrame by 'BeginDaysPlague'\ngroups = list(southPlague.groupby('BeginDaysPlague'))\ngroups[1][1]['BeginPlaguePeriod']\ndates = [groups[i][1]['BeginPlaguePeriod'].iloc[0] for i in range(len(groups))]\n\n# Define a list of colors of the same length as the number of groups\ncolor_list = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n              '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n              '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n              '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5',\n              '#7fc97f']\n\n# Create an empty list to hold the legend elements\nlegend_elements = []\n\nfor i in range(len(groups)):\n    fig, ax = plt.subplots(figsize=(13, 10))\n    # Plot the base map\n    SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)\n    southScaniaMap.plot(ax=ax, color='azure', edgecolor='darkgray', legend=False)\n    group_i = southPlague.loc[southPlague['BeginPlaguePeriod'] == dates[i]]\n    group_i.plot(ax=ax, color=color_list[i], \n                 edgecolor='black')\n    \n    # Add names to each polygon\n    for x, y, label in zip(group_i.geometry.centroid.x, group_i.geometry.centroid.y, group_i['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n    # Create a Patch for the legend\n    legend_elements.append(mpatches.Patch(facecolor=color_list[i], edgecolor='black', label=dates[i]))\n\n    plt.xlabel('Meters')\n    plt.ylabel('Meters')\n\n    # Add the legend manually\n    ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n    plt.savefig('plaguebyinitialmonth{}.png'.format(i), dpi=300)\n    plt.close()\n</pre> import matplotlib.patches as mpatches  # Group the DataFrame by 'BeginDaysPlague' groups = list(southPlague.groupby('BeginDaysPlague')) groups[1][1]['BeginPlaguePeriod'] dates = [groups[i][1]['BeginPlaguePeriod'].iloc[0] for i in range(len(groups))]  # Define a list of colors of the same length as the number of groups color_list = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',               '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',               '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',               '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5',               '#7fc97f']  # Create an empty list to hold the legend elements legend_elements = []  for i in range(len(groups)):     fig, ax = plt.subplots(figsize=(13, 10))     # Plot the base map     SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)     southScaniaMap.plot(ax=ax, color='azure', edgecolor='darkgray', legend=False)     group_i = southPlague.loc[southPlague['BeginPlaguePeriod'] == dates[i]]     group_i.plot(ax=ax, color=color_list[i],                   edgecolor='black')          # Add names to each polygon     for x, y, label in zip(group_i.geometry.centroid.x, group_i.geometry.centroid.y, group_i['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')      # Create a Patch for the legend     legend_elements.append(mpatches.Patch(facecolor=color_list[i], edgecolor='black', label=dates[i]))      plt.xlabel('Meters')     plt.ylabel('Meters')      # Add the legend manually     ax.legend(handles=legend_elements, loc='upper right', fontsize=9)     plt.savefig('plaguebyinitialmonth{}.png'.format(i), dpi=300)     plt.close() <p>Creating an animation based on the beginning of the plague period by concatenating the frames:</p> In\u00a0[19]: Copied! <pre>import matplotlib.animation as animation\nimport matplotlib.patches as mpatches\n\n# Group the DataFrame by 'BeginDaysPlague'\ngroups = list(southPlague.groupby('BeginDaysPlague'))\ndates = [groups[i][1]['BeginPlaguePeriod'].iloc[0] for i in range(len(groups))]\n\n#Create a new figure and axes\nfig, ax = plt.subplots(figsize=(13, 10))\n\ndef update(num):\n    ax.clear()\n    # Plot the base map\n    SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)\n    southScaniaMap.plot(ax=ax, color = 'azure', edgecolor='darkgray', legend=False)\n    legend_elements = []\n    \n    # Concatenate all groups up to the current one\n    group = pd.concat([groups[i][1] for i in range(num+1)])\n         \n    # Apply the colormap to the 'color' parameter of the plot function\n    group.plot(ax=ax, column='BeginPlaguePeriod', edgecolor='black', legend=False)\n    \n    # Add names to each polygon\n    for x, y, label in zip(group.geometry.centroid.x, group.geometry.centroid.y, group['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n    \n    ax.set_title('{}'.format(dates[num]), fontdict={'fontsize': '24', 'fontweight' : '10'})\n\n    # Create a Patch for the legend\n    legend_elements.append(mpatches.Patch(facecolor=color_list[num], edgecolor='black', label=dates[num]))\n\n    # Add the legend manually\n    ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n    #ax.set_axis_off()\n    plt.xlabel('Meters')\n    plt.ylabel('Meters')\n\nani = animation.FuncAnimation(fig, update, frames=len(groups), repeat=False, interval=100000)\nani.save('plague1712.gif', writer='imagemagick', fps=1)\nplt.show()\n</pre> import matplotlib.animation as animation import matplotlib.patches as mpatches  # Group the DataFrame by 'BeginDaysPlague' groups = list(southPlague.groupby('BeginDaysPlague')) dates = [groups[i][1]['BeginPlaguePeriod'].iloc[0] for i in range(len(groups))]  #Create a new figure and axes fig, ax = plt.subplots(figsize=(13, 10))  def update(num):     ax.clear()     # Plot the base map     SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)     southScaniaMap.plot(ax=ax, color = 'azure', edgecolor='darkgray', legend=False)     legend_elements = []          # Concatenate all groups up to the current one     group = pd.concat([groups[i][1] for i in range(num+1)])               # Apply the colormap to the 'color' parameter of the plot function     group.plot(ax=ax, column='BeginPlaguePeriod', edgecolor='black', legend=False)          # Add names to each polygon     for x, y, label in zip(group.geometry.centroid.x, group.geometry.centroid.y, group['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')          ax.set_title('{}'.format(dates[num]), fontdict={'fontsize': '24', 'fontweight' : '10'})      # Create a Patch for the legend     legend_elements.append(mpatches.Patch(facecolor=color_list[num], edgecolor='black', label=dates[num]))      # Add the legend manually     ax.legend(handles=legend_elements, loc='upper right', fontsize=9)     #ax.set_axis_off()     plt.xlabel('Meters')     plt.ylabel('Meters')  ani = animation.FuncAnimation(fig, update, frames=len(groups), repeat=False, interval=100000) ani.save('plague1712.gif', writer='imagemagick', fps=1) plt.show() <pre>MovieWriter imagemagick unavailable; using Pillow instead.\n</pre> <p>Generating the individual frames for the animation active plague by month:</p> In\u00a0[20]: Copied! <pre># Define a list of colors of the same length as the number of groups\ncolor_list = ['#E6194B', '#3CB44B', '#FFE119', '#4363D8', '#F58231', \n              '#911EB4', '#42D4F4', '#FABEBE', '#469990', '#E6BEFF', \n              '#9A6324', '#FFFAC8', '#800000', '#AAFFC3', '#808000', \n              '#FFD8B1', '#A9A9A9',  \n              '#008080', '#0000FF', '#00FF00', '#FF00FF',\n              '#800080', '#00FFFF', '#FFA500', '#A52A2A', '#FFFF00'\n            ]\n\n# Create a dictionary that maps each unique date to a color\ndate_to_color = dict(zip(southPlague['BeginPlaguePeriod'].unique(), color_list))\n\n# Create a new column called 'color' that maps the date to the corresponding color\nsouthPlague['color'] = southPlague['BeginPlaguePeriod'].map(date_to_color)\n</pre> # Define a list of colors of the same length as the number of groups color_list = ['#E6194B', '#3CB44B', '#FFE119', '#4363D8', '#F58231',                '#911EB4', '#42D4F4', '#FABEBE', '#469990', '#E6BEFF',                '#9A6324', '#FFFAC8', '#800000', '#AAFFC3', '#808000',                '#FFD8B1', '#A9A9A9',                 '#008080', '#0000FF', '#00FF00', '#FF00FF',               '#800080', '#00FFFF', '#FFA500', '#A52A2A', '#FFFF00'             ]  # Create a dictionary that maps each unique date to a color date_to_color = dict(zip(southPlague['BeginPlaguePeriod'].unique(), color_list))  # Create a new column called 'color' that maps the date to the corresponding color southPlague['color'] = southPlague['BeginPlaguePeriod'].map(date_to_color)   In\u00a0[21]: Copied! <pre>import matplotlib.patches as mpatches\n\ndate_initial = southPlague['new_format_BeginPlaguePeriod'].unique()\ndate_final = southPlague['new_format_EndPlaguePeriod'].unique()\ndates = np.concatenate((date_initial, date_final), axis=None)\ndates = np.sort(np.unique(dates))\n\n# Convert numpy.datetime64 to pandas Timestamp and then format it\nlegends = [pd.Timestamp(date).strftime('%b %Y').upper() if pd.notnull(date) else 'Unknown' for date in dates]\n# Create an empty list to hold the legend elements\nlegend_elements = []\n# Create a new GeoDataFrame where each row corresponds to one month of one polygon\nds = []\n\nfor date in dates:\n    if pd.notna(date):\n        idxs = []\n        labels = []\n        seendates = set()\n        for idx, _ in southPlague.iterrows():\n            ini = southPlague['new_format_BeginPlaguePeriod'].iloc[idx]\n            end = southPlague['new_format_EndPlaguePeriod'].iloc[idx]\n            color = southPlague['color'].iloc[idx]\n            nice_date = southPlague['BeginPlaguePeriod'].iloc[idx]\n            # if each is a valid timestamp\n            if (pd.notna(end) and date &gt;= ini and date &lt;= end) or (pd.isna(end) and date == ini):\n                idxs.append(idx)\n                if nice_date not in seendates:\n                    seendates.add(nice_date)\n                    labels.append(mpatches.Patch(facecolor=color, edgecolor='black', label=nice_date))\n        if len(idxs) &gt; 0:\n            ds.append(southPlague.iloc[idxs])  \n            legend_elements.append(labels)      \n\nfor idx , date, legend, new_gdf in zip(range(len(ds)), dates, legends, ds):\n        fig, ax = plt.subplots(figsize=(10, 7))\n        # Plot the base map\n        #SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)\n        southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')\n        \n        new_gdf.plot(ax=ax, color = new_gdf['color'], edgecolor='black', label=new_gdf['BeginPlaguePeriod'], legend=True)\n\n        # Add names to each polygon\n        for x, y, label in zip(new_gdf.geometry.centroid.x, new_gdf.geometry.centroid.y, new_gdf['ParishName']):\n            ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n        # Set the title\n        ax.set_title('{}'.format(legend), fontdict={'fontsize': '24', 'fontweight' : '10'})\n\n        # Additional title at the bottom\n        fig.text(0.5, 0.05, 'SouthScania', ha='center', va='center', fontsize=24, fontweight='bold')\n\n        plt.axis('off')\n                \n        # Add the legend manually\n        ax.legend(handles= legend_elements[idx], loc='upper right', fontsize=9)\n        plt.savefig('plaguebymonth{}.png'.format(idx), dpi=300)\n        plt.close()\n</pre> import matplotlib.patches as mpatches  date_initial = southPlague['new_format_BeginPlaguePeriod'].unique() date_final = southPlague['new_format_EndPlaguePeriod'].unique() dates = np.concatenate((date_initial, date_final), axis=None) dates = np.sort(np.unique(dates))  # Convert numpy.datetime64 to pandas Timestamp and then format it legends = [pd.Timestamp(date).strftime('%b %Y').upper() if pd.notnull(date) else 'Unknown' for date in dates] # Create an empty list to hold the legend elements legend_elements = [] # Create a new GeoDataFrame where each row corresponds to one month of one polygon ds = []  for date in dates:     if pd.notna(date):         idxs = []         labels = []         seendates = set()         for idx, _ in southPlague.iterrows():             ini = southPlague['new_format_BeginPlaguePeriod'].iloc[idx]             end = southPlague['new_format_EndPlaguePeriod'].iloc[idx]             color = southPlague['color'].iloc[idx]             nice_date = southPlague['BeginPlaguePeriod'].iloc[idx]             # if each is a valid timestamp             if (pd.notna(end) and date &gt;= ini and date &lt;= end) or (pd.isna(end) and date == ini):                 idxs.append(idx)                 if nice_date not in seendates:                     seendates.add(nice_date)                     labels.append(mpatches.Patch(facecolor=color, edgecolor='black', label=nice_date))         if len(idxs) &gt; 0:             ds.append(southPlague.iloc[idxs])               legend_elements.append(labels)        for idx , date, legend, new_gdf in zip(range(len(ds)), dates, legends, ds):         fig, ax = plt.subplots(figsize=(10, 7))         # Plot the base map         #SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)         southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')                  new_gdf.plot(ax=ax, color = new_gdf['color'], edgecolor='black', label=new_gdf['BeginPlaguePeriod'], legend=True)          # Add names to each polygon         for x, y, label in zip(new_gdf.geometry.centroid.x, new_gdf.geometry.centroid.y, new_gdf['ParishName']):             ax.text(x, y, label, fontsize=5, ha='center', va='center')          # Set the title         ax.set_title('{}'.format(legend), fontdict={'fontsize': '24', 'fontweight' : '10'})          # Additional title at the bottom         fig.text(0.5, 0.05, 'SouthScania', ha='center', va='center', fontsize=24, fontweight='bold')          plt.axis('off')                          # Add the legend manually         ax.legend(handles= legend_elements[idx], loc='upper right', fontsize=9)         plt.savefig('plaguebymonth{}.png'.format(idx), dpi=300)         plt.close()         <p>Animation of active plague by month</p> In\u00a0[22]: Copied! <pre>import matplotlib.animation as animation\nimport matplotlib.patches as mpatches\n\ndate_initial = southPlague['new_format_BeginPlaguePeriod'].unique()\ndate_final = southPlague['new_format_EndPlaguePeriod'].unique()\ndates = np.concatenate((date_initial, date_final), axis=None)\ndates = np.sort(np.unique(dates))\n\n# Convert numpy.datetime64 to pandas Timestamp and then format it\nlegends = [pd.Timestamp(date).strftime('%b %Y').upper() if pd.notnull(date) else 'Unknown' for date in dates]\n# Create an empty list to hold the legend elements\nlegend_elements = []\n# Create a new GeoDataFrame where each row corresponds to one month of one polygon\nds = []\n\nfor date in dates:\n    if pd.notna(date):\n        idxs = []\n        labels = []\n        seendates = set()\n        for idx, _ in southPlague.iterrows():\n            ini = southPlague['new_format_BeginPlaguePeriod'].iloc[idx]\n            end = southPlague['new_format_EndPlaguePeriod'].iloc[idx]\n            color = southPlague['color'].iloc[idx]\n            nice_date = southPlague['BeginPlaguePeriod'].iloc[idx]\n            # if each is a valid timestamp\n            if (pd.notna(end) and date &gt;= ini and date &lt;= end) or (pd.isna(end) and date == ini):\n                idxs.append(idx)\n                if nice_date not in seendates:\n                    seendates.add(nice_date)\n                    labels.append(mpatches.Patch(facecolor=color, edgecolor='black', label=nice_date))\n        if len(idxs) &gt; 0:\n            ds.append(southPlague.iloc[idxs])  \n            legend_elements.append(labels)         \n\nfig, ax = plt.subplots(figsize=(13, 10))\n\ndef update(num):\n    ax.clear()\n    date, legend, new_gdf = ds[num], legends[num], ds[num]\n    #SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)\n    southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')\n       \n    new_gdf.plot(ax=ax, color = new_gdf['color'], edgecolor='black', legend=True)\n\n    # Add names to each polygon\n    for x, y, label in zip(new_gdf.geometry.centroid.x, new_gdf.geometry.centroid.y, new_gdf['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n    # Set the title\n    ax.set_title('{}'.format(legend), fontdict={'fontsize': '24', 'fontweight' : '10'})\n    \n    # Additional title at the bottom\n    fig.text(0.5, 0.05, 'SouthScania', ha='center', va='center', fontsize=24, fontweight='bold')\n    \n    ax.legend(handles= legend_elements[num], loc='upper right', fontsize=10)\n    plt.axis('off')\n    #plt.xlabel('Meters')\n    #plt.ylabel('Meters')\n\nani = animation.FuncAnimation(fig, update, frames=len(ds), repeat=True, interval=700000)\nani.save('activeplaguebymonth.gif', writer='imagemagick', fps=1)\n</pre> import matplotlib.animation as animation import matplotlib.patches as mpatches  date_initial = southPlague['new_format_BeginPlaguePeriod'].unique() date_final = southPlague['new_format_EndPlaguePeriod'].unique() dates = np.concatenate((date_initial, date_final), axis=None) dates = np.sort(np.unique(dates))  # Convert numpy.datetime64 to pandas Timestamp and then format it legends = [pd.Timestamp(date).strftime('%b %Y').upper() if pd.notnull(date) else 'Unknown' for date in dates] # Create an empty list to hold the legend elements legend_elements = [] # Create a new GeoDataFrame where each row corresponds to one month of one polygon ds = []  for date in dates:     if pd.notna(date):         idxs = []         labels = []         seendates = set()         for idx, _ in southPlague.iterrows():             ini = southPlague['new_format_BeginPlaguePeriod'].iloc[idx]             end = southPlague['new_format_EndPlaguePeriod'].iloc[idx]             color = southPlague['color'].iloc[idx]             nice_date = southPlague['BeginPlaguePeriod'].iloc[idx]             # if each is a valid timestamp             if (pd.notna(end) and date &gt;= ini and date &lt;= end) or (pd.isna(end) and date == ini):                 idxs.append(idx)                 if nice_date not in seendates:                     seendates.add(nice_date)                     labels.append(mpatches.Patch(facecolor=color, edgecolor='black', label=nice_date))         if len(idxs) &gt; 0:             ds.append(southPlague.iloc[idxs])               legend_elements.append(labels)           fig, ax = plt.subplots(figsize=(13, 10))  def update(num):     ax.clear()     date, legend, new_gdf = ds[num], legends[num], ds[num]     #SkaneMap.plot(ax=ax, color='white', edgecolor='darkgray', legend=False)     southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')             new_gdf.plot(ax=ax, color = new_gdf['color'], edgecolor='black', legend=True)      # Add names to each polygon     for x, y, label in zip(new_gdf.geometry.centroid.x, new_gdf.geometry.centroid.y, new_gdf['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')      # Set the title     ax.set_title('{}'.format(legend), fontdict={'fontsize': '24', 'fontweight' : '10'})          # Additional title at the bottom     fig.text(0.5, 0.05, 'SouthScania', ha='center', va='center', fontsize=24, fontweight='bold')          ax.legend(handles= legend_elements[num], loc='upper right', fontsize=10)     plt.axis('off')     #plt.xlabel('Meters')     #plt.ylabel('Meters')  ani = animation.FuncAnimation(fig, update, frames=len(ds), repeat=True, interval=700000) ani.save('activeplaguebymonth.gif', writer='imagemagick', fps=1) <pre>MovieWriter imagemagick unavailable; using Pillow instead.\n</pre> <p>Doing the clusters under the condition of shared borders</p> In\u00a0[51]: Copied! <pre># Create a graph\nG = nx.Graph()\n\n# Add nodes\nfor index, row in plagueSouthScania.iterrows():\n    G.add_node(index, polygon=row['geometry'])\n\n# Add edges\nfor i, row_i in plagueSouthScania.iterrows():\n    for j, row_j in plagueSouthScania.iterrows():\n        if i != j and row_i['geometry'].touches(row_j['geometry']):\n            G.add_edge(i, j)\n# Find connected components\nconnected_components = list(nx.connected_components(G))\n\n# Create new geodataframes for each subset of connected polygons\ngdfs = [gpd.GeoDataFrame(plagueSouthScania.loc[list(component)], crs=plagueSouthScania.crs) for component in connected_components]\n\nplagueSouthScania = plagueSouthScania.copy()\n\n# Initialize a new column in the original dataframe\nplagueSouthScania['component'] = -1\n\n# Loop over the list of connected components\nfor i, component in enumerate(connected_components):\n    # For each component, set the 'component' value of the corresponding rows to the current component number\n    plagueSouthScania.loc[list(component), 'component'] = i\n</pre> # Create a graph G = nx.Graph()  # Add nodes for index, row in plagueSouthScania.iterrows():     G.add_node(index, polygon=row['geometry'])  # Add edges for i, row_i in plagueSouthScania.iterrows():     for j, row_j in plagueSouthScania.iterrows():         if i != j and row_i['geometry'].touches(row_j['geometry']):             G.add_edge(i, j) # Find connected components connected_components = list(nx.connected_components(G))  # Create new geodataframes for each subset of connected polygons gdfs = [gpd.GeoDataFrame(plagueSouthScania.loc[list(component)], crs=plagueSouthScania.crs) for component in connected_components]  plagueSouthScania = plagueSouthScania.copy()  # Initialize a new column in the original dataframe plagueSouthScania['component'] = -1  # Loop over the list of connected components for i, component in enumerate(connected_components):     # For each component, set the 'component' value of the corresponding rows to the current component number     plagueSouthScania.loc[list(component), 'component'] = i In\u00a0[52]: Copied! <pre>color_list = ['steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen', 'coral'\n                , 'steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen']\nfig, ax = plt.subplots(figsize=(7, 4))\nSkaneMap.plot(ax=ax, color = 'whitesmoke', edgecolor='black',\n              legend=False)\nsouthScaniaMap.plot(ax=ax, color = 'azure',\n                        edgecolor='darkgray', legend=False)\nfor i in range(len(gdfs)):\n    cluster_i = plagueSouthScania[plagueSouthScania['component'] == i]\n    if len(cluster_i) &lt; 3:\n        cluster_i.plot(ax=ax, color = 'peachpuff',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) == 4:\n        cluster_i.plot(ax=ax, color = 'deepskyblue',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) == 9:\n        cluster_i.plot(ax=ax, color = 'salmon',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) &gt; 9:\n        cluster_i.plot(ax=ax, color = 'aquamarine',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> color_list = ['steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen', 'coral'                 , 'steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen'] fig, ax = plt.subplots(figsize=(7, 4)) SkaneMap.plot(ax=ax, color = 'whitesmoke', edgecolor='black',               legend=False) southScaniaMap.plot(ax=ax, color = 'azure',                         edgecolor='darkgray', legend=False) for i in range(len(gdfs)):     cluster_i = plagueSouthScania[plagueSouthScania['component'] == i]     if len(cluster_i) &lt; 3:         cluster_i.plot(ax=ax, color = 'peachpuff',                         edgecolor='black', legend=False)     elif len(cluster_i) == 4:         cluster_i.plot(ax=ax, color = 'deepskyblue',                         edgecolor='black', legend=False)     elif len(cluster_i) == 9:         cluster_i.plot(ax=ax, color = 'salmon',                         edgecolor='black', legend=False)     elif len(cluster_i) &gt; 9:         cluster_i.plot(ax=ax, color = 'aquamarine',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() In\u00a0[53]: Copied! <pre>cluster_0 = plagueSouthScania[plagueSouthScania['component'] == 0]\ncluster_1 = plagueSouthScania[plagueSouthScania['component'] == 1]\ncluster_2 = plagueSouthScania[plagueSouthScania['component'] == 2]\ncluster_3 = plagueSouthScania[plagueSouthScania['component'] == 3]  \ncluster_4 = plagueSouthScania[plagueSouthScania['component'] == 4]\ncluster_5 = plagueSouthScania[plagueSouthScania['component'] == 5]\ncluster_6 = plagueSouthScania[plagueSouthScania['component'] == 6]\ncluster_7 = plagueSouthScania[plagueSouthScania['component'] == 7]\ncluster_8 = plagueSouthScania[plagueSouthScania['component'] == 8]\ncluster_9 = plagueSouthScania[plagueSouthScania['component'] == 9]\ncluster_10 = plagueSouthScania[plagueSouthScania['component'] == 10]\ncluster_11 = plagueSouthScania[plagueSouthScania['component'] == 11]\ncluster_12 = plagueSouthScania[plagueSouthScania['component'] == 12]\ncluster_13 = plagueSouthScania[plagueSouthScania['component'] == 13]\ncluster_14 = plagueSouthScania[plagueSouthScania['component'] == 14]  \nbig_cluster = pd.concat([cluster_6,cluster_2,cluster_13], ignore_index=True) \nsmall_cluster = pd.concat([cluster_11, cluster_12], ignore_index=True)\n</pre> cluster_0 = plagueSouthScania[plagueSouthScania['component'] == 0] cluster_1 = plagueSouthScania[plagueSouthScania['component'] == 1] cluster_2 = plagueSouthScania[plagueSouthScania['component'] == 2] cluster_3 = plagueSouthScania[plagueSouthScania['component'] == 3]   cluster_4 = plagueSouthScania[plagueSouthScania['component'] == 4] cluster_5 = plagueSouthScania[plagueSouthScania['component'] == 5] cluster_6 = plagueSouthScania[plagueSouthScania['component'] == 6] cluster_7 = plagueSouthScania[plagueSouthScania['component'] == 7] cluster_8 = plagueSouthScania[plagueSouthScania['component'] == 8] cluster_9 = plagueSouthScania[plagueSouthScania['component'] == 9] cluster_10 = plagueSouthScania[plagueSouthScania['component'] == 10] cluster_11 = plagueSouthScania[plagueSouthScania['component'] == 11] cluster_12 = plagueSouthScania[plagueSouthScania['component'] == 12] cluster_13 = plagueSouthScania[plagueSouthScania['component'] == 13] cluster_14 = plagueSouthScania[plagueSouthScania['component'] == 14]   big_cluster = pd.concat([cluster_6,cluster_2,cluster_13], ignore_index=True)  small_cluster = pd.concat([cluster_11, cluster_12], ignore_index=True) In\u00a0[54]: Copied! <pre># Define a list of colors of the same length as the number of groups\ncolor_list = ['#E6194B', '#3CB44B', '#FFE119', '#4363D8', '#F58231', \n              '#911EB4', '#42D4F4', '#FABEBE', '#469990', '#E6BEFF', \n              '#9A6324', '#FFFAC8', '#800000', '#AAFFC3', '#808000', \n              '#FFD8B1', '#A9A9A9',  \n              '#008080', '#0000FF', '#00FF00', '#FF00FF',\n              '#800080', '#00FFFF', '#FFA500', '#A52A2A', '#FFFF00'\n            ]\n\n# Create a dictionary that maps each unique date to a color\ndate_to_color = dict(zip(big_cluster['BeginPlaguePeriod'].unique(), color_list))\n\n# Create a new column called 'color' that maps the date to the corresponding color\nbig_cluster['color'] = big_cluster['BeginPlaguePeriod'].map(date_to_color)\n</pre> # Define a list of colors of the same length as the number of groups color_list = ['#E6194B', '#3CB44B', '#FFE119', '#4363D8', '#F58231',                '#911EB4', '#42D4F4', '#FABEBE', '#469990', '#E6BEFF',                '#9A6324', '#FFFAC8', '#800000', '#AAFFC3', '#808000',                '#FFD8B1', '#A9A9A9',                 '#008080', '#0000FF', '#00FF00', '#FF00FF',               '#800080', '#00FFFF', '#FFA500', '#A52A2A', '#FFFF00'             ]  # Create a dictionary that maps each unique date to a color date_to_color = dict(zip(big_cluster['BeginPlaguePeriod'].unique(), color_list))  # Create a new column called 'color' that maps the date to the corresponding color big_cluster['color'] = big_cluster['BeginPlaguePeriod'].map(date_to_color)   <p>The Accumulative</p> In\u00a0[55]: Copied! <pre>cluster = big_cluster\ncluster = cluster.loc[(cluster['BeginPlaguePeriod']!= 'JAN 1711') \n                      &amp; (cluster['BeginPlaguePeriod']!= 'JUN 1711')\n                      &amp; (cluster['BeginPlaguePeriod']!= 'OCT 1711') \n                      &amp; (cluster['BeginPlaguePeriod']!= 'DEC 1711')  \n                      &amp; (cluster['BeginPlaguePeriod']!= 'FEB 1715')]\n</pre> cluster = big_cluster cluster = cluster.loc[(cluster['BeginPlaguePeriod']!= 'JAN 1711')                        &amp; (cluster['BeginPlaguePeriod']!= 'JUN 1711')                       &amp; (cluster['BeginPlaguePeriod']!= 'OCT 1711')                        &amp; (cluster['BeginPlaguePeriod']!= 'DEC 1711')                         &amp; (cluster['BeginPlaguePeriod']!= 'FEB 1715')] In\u00a0[57]: Copied! <pre>group1 = cluster[(cluster['ParishName'] == 'GENARP')\n                 | (cluster['ParishName'] == 'GR\u00d6NBY')\n                 | (cluster['ParishName'] == 'VEBER\u00d6D')\n                 | (cluster['ParishName'] == 'VOMB') \n                 | (cluster['ParishName'] == 'SLIMMINGE')\n                 | (cluster['ParishName'] == 'SKURUP')\n                 | (cluster['ParishName'] == 'B\u00d6RRINGE')\n                 | (cluster['ParishName'] == 'SVEDALA')\n                 | (cluster['ParishName'] == 'SKABERSJ\u00d6')\n                 | (cluster['ParishName'] == 'T\u00d6RRINGE')\n                 | (cluster['ParishName'] == 'S\u00d6DRA \u00c5KARP')\n                 | (cluster['ParishName'] == 'ARRIE')\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1\n</pre> group1 = cluster[(cluster['ParishName'] == 'GENARP')                  | (cluster['ParishName'] == 'GR\u00d6NBY')                  | (cluster['ParishName'] == 'VEBER\u00d6D')                  | (cluster['ParishName'] == 'VOMB')                   | (cluster['ParishName'] == 'SLIMMINGE')                  | (cluster['ParishName'] == 'SKURUP')                  | (cluster['ParishName'] == 'B\u00d6RRINGE')                  | (cluster['ParishName'] == 'SVEDALA')                  | (cluster['ParishName'] == 'SKABERSJ\u00d6')                  | (cluster['ParishName'] == 'T\u00d6RRINGE')                  | (cluster['ParishName'] == 'S\u00d6DRA \u00c5KARP')                  | (cluster['ParishName'] == 'ARRIE') ]      group1 = group1.reset_index(drop=True) group1  Out[57]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry color classification new_format_BeginPlaguePeriod new_format_EndPlaguePeriod component 0 MIDDLE F\u00c4RS VOMB VOMBS 269 303 286.0 1 5 SEP 1712 NOV 1712 9 POLYGON ((4214955.556 3199380.156, 4214967.070... #42D4F4 white 1712-09-01 1712-11-01 2 1 SOUTHWEST BARA GENARP GENARPS 745 841 793.0 1 4 MAY 1712 AUG 1712 70 POLYGON ((4213130.585 3190804.364, 4213215.648... #A9A9A9 white 1712-05-01 1712-08-01 2 2 SOUTHWEST BARA SKABERSJ\u00d6 SKABERSJ\u00d6 485 548 516.5 1 4 AUG 1712 NOV 1712 NaN POLYGON ((4193435.714 3182066.598, 4193378.045... #E6194B white 1712-08-01 1712-11-01 2 3 SOUTHWEST OXIE ARRIE ARRIE 113 127 120.0 2 4 OCT 1712 DEC 1712 15 POLYGON ((4187185.738 3182794.083, 4188296.110... #4363D8 white 1712-10-01 1712-12-01 2 4 SOUTHWEST OXIE SVEDALA SVEDALA 450 508 479.0 1 5 AUG 1712 NOV 1712 15 POLYGON ((4200801.082 3181487.130, 4200913.270... #E6194B white 1712-08-01 1712-11-01 2 5 SOUTHWEST OXIE S\u00d6DRA \u00c5KARP S\u00d6DRA \u00c5KARPS 156 176 166.0 1 5 SEP 1712 DEC 1712 4 POLYGON ((4187385.650 3176017.092, 4187202.472... #42D4F4 white 1712-09-01 1712-12-01 2 6 SOUTHWEST OXIE T\u00d6RRINGE T\u00d6RRINGE 93 105 99.0 1 5 SEP 1712 NOV 1712 NaN POLYGON ((4192313.360 3184522.040, 4192406.262... #42D4F4 white 1712-09-01 1712-11-01 2 7 SOUTHWEST TORNA VEBER\u00d6D VEBER\u00d6DS 519 586 552.5 3 4 JUN 1712 AUG 1712 NaN POLYGON ((4213130.585 3190804.364, 4212952.360... #F58231 white 1712-06-01 1712-08-01 2 8 SOUTHWEST VEMMENH\u00d6G B\u00d6RRINGE B\u00d6RRINGE 359 405 382.0 1 5 AUG 1712 AUG 1712 5 POLYGON ((4204085.740 3178127.746, 4203815.385... #E6194B white 1712-08-01 1712-08-01 2 9 SOUTHWEST VEMMENH\u00d6G GR\u00d6NBY GR\u00d6NBY 185 209 197.0 2 4 MAY 1712 JUN 1712 3 POLYGON ((4205186.159 3176778.384, 4205133.068... #A9A9A9 white 1712-05-01 1712-06-01 2 10 SOUTHWEST VEMMENH\u00d6G SKURUP SKURUPS 547 618 582.5 2 4 JUL 1712 JUL 1712 NaN POLYGON ((4214392.138 3182887.177, 4214576.093... #FFE119 white 1712-07-01 1712-07-01 2 11 SOUTHWEST VEMMENH\u00d6G SLIMMINGE SLIMMINGE 550 621 585.5 3 4 JUN 1712 JUN 1712 NaN POLYGON ((4214392.138 3182887.177, 4214363.468... #F58231 white 1712-06-01 1712-06-01 2 In\u00a0[30]: Copied! <pre>group2 = cluster[(cluster['ParishName'] == 'YSTAD')\n                 | (cluster['ParishName'] == '\u00d6JA')\n                 | (cluster['ParishName'] == 'BROMMA')\n                 | (cluster['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (cluster['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (cluster['ParishName'] == 'VALLEBERGA')\n                 | ((cluster['ParishName'] == 'H\u00d6RUP') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((cluster['ParishName'] == 'GLEMMINGE') &amp; (cluster['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (cluster['ParishName'] == 'INGELSTORP')\n                 | (cluster['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (cluster['ParishName'] == 'HEDESKOGA')\n                 | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup2 = group2.reset_index(drop=True)\n</pre> group2 = cluster[(cluster['ParishName'] == 'YSTAD')                  | (cluster['ParishName'] == '\u00d6JA')                  | (cluster['ParishName'] == 'BROMMA')                  | (cluster['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (cluster['ParishName'] == 'STORA K\u00d6PINGE')                  | (cluster['ParishName'] == 'VALLEBERGA')                  | ((cluster['ParishName'] == 'H\u00d6RUP') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))                  | ((cluster['ParishName'] == 'GLEMMINGE') &amp; (cluster['BeginPlaguePeriod']== 'AUG 1712'))                  | (cluster['ParishName'] == 'INGELSTORP')                  | (cluster['ParishName'] == 'HAMMENH\u00d6G')                  | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))                  | (cluster['ParishName'] == 'HEDESKOGA')                  | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'NOV 1712')) ]      group2 = group2.reset_index(drop=True) In\u00a0[31]: Copied! <pre>group2.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup2.at[1, 'EndPlaguePeriod'] = np.NaN\n</pre> group2.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group2.at[1, 'EndPlaguePeriod'] = np.NaN In\u00a0[32]: Copied! <pre>group3 = cluster[(cluster['ParishName'] == 'S\u00d6DRA \u00c5SUM')\n                 |(cluster['ParishName'] == '\u00d6VED')\n                 | (cluster['ParishName'] == 'S\u00d6VDE')\n                 | (cluster['ParishName'] == 'BRANDSTAD')\n                 | (cluster['ParishName'] == 'BALDRINGE') \n                 | (cluster['ParishName'] == 'RAMS\u00c5SA')\n                 | (cluster['ParishName'] == 'TRYDE') \n                 | (cluster['ParishName'] == 'TRAN\u00c5S')\n                 | (cluster['ParishName'] == 'L\u00d6VESTAD')\n                 | (cluster['ParishName'] == 'VANSTAD')\n                 | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))\n                 | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1713'))\n                 | (cluster['ParishName'] == 'SK\u00c5RBY')\n                 | (cluster['ParishName'] == 'S\u00d6VESTAD')\n                 | (cluster['ParishName'] == 'VILLIE')\n                 | (cluster['ParishName'] == 'BALK\u00c5KRA')\n                 | (cluster['ParishName'] == 'SN\u00c5RESTAD')\n                 | (cluster['ParishName'] == 'V\u00c4STRA N\u00d6BBEL\u00d6V')\n                 | (cluster['ParishName'] == '\u00d6STRA VEMMENH\u00d6G')\n                 | (cluster['ParishName'] == 'SVENSTORP')\n                 | (cluster['ParishName'] == 'LILLA BEDDINGE')\n]     \ngroup3 = group3.reset_index(drop=True)\n</pre> group3 = cluster[(cluster['ParishName'] == 'S\u00d6DRA \u00c5SUM')                  |(cluster['ParishName'] == '\u00d6VED')                  | (cluster['ParishName'] == 'S\u00d6VDE')                  | (cluster['ParishName'] == 'BRANDSTAD')                  | (cluster['ParishName'] == 'BALDRINGE')                   | (cluster['ParishName'] == 'RAMS\u00c5SA')                  | (cluster['ParishName'] == 'TRYDE')                   | (cluster['ParishName'] == 'TRAN\u00c5S')                  | (cluster['ParishName'] == 'L\u00d6VESTAD')                  | (cluster['ParishName'] == 'VANSTAD')                  | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))                  | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1713'))                  | (cluster['ParishName'] == 'SK\u00c5RBY')                  | (cluster['ParishName'] == 'S\u00d6VESTAD')                  | (cluster['ParishName'] == 'VILLIE')                  | (cluster['ParishName'] == 'BALK\u00c5KRA')                  | (cluster['ParishName'] == 'SN\u00c5RESTAD')                  | (cluster['ParishName'] == 'V\u00c4STRA N\u00d6BBEL\u00d6V')                  | (cluster['ParishName'] == '\u00d6STRA VEMMENH\u00d6G')                  | (cluster['ParishName'] == 'SVENSTORP')                  | (cluster['ParishName'] == 'LILLA BEDDINGE') ]      group3 = group3.reset_index(drop=True) In\u00a0[33]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ngroup1 = get_centroid(add_Begin_End_days(sort_by_date(group1), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\ngroup2 = get_centroid(add_Begin_End_days(sort_by_date(group2), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\ngroup3 = get_centroid(add_Begin_End_days(sort_by_date(group3), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\ngroup4 = get_centroid(add_Begin_End_days(sort_by_date(cluster_6), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\n</pre> # Getting the centroid of each polygon for defining the transmission matrix group1 = get_centroid(add_Begin_End_days(sort_by_date(group1), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) group2 = get_centroid(add_Begin_End_days(sort_by_date(group2), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) group3 = get_centroid(add_Begin_End_days(sort_by_date(group3), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) group4 = get_centroid(add_Begin_End_days(sort_by_date(cluster_6), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) In\u00a0[325]: Copied! <pre>fig, ax = plt.subplots(figsize=(13, 10))\n# Plot the base map\nsouthScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')\n\ngroup1.plot(ax=ax, color = color_list[1], edgecolor='black', label=group1['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group1.geometry.centroid.x, group1.geometry.centroid.y, group1['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\ngroup2.plot(ax=ax, color = color_list[2], edgecolor='black', label=group2['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group2.geometry.centroid.x, group2.geometry.centroid.y, group2['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\ngroup3.plot(ax=ax, color = color_list[3], edgecolor='black', label=group3['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group3.geometry.centroid.x, group3.geometry.centroid.y, group3['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\ngroup4.plot(ax=ax, color = color_list[4], edgecolor='black', label=group4['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group4.geometry.centroid.x, group4.geometry.centroid.y, group4['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n# Additional title at the bottom\n#fig.text(0.5, 0.05, 'Cluster 1712 - 1713', ha='center', va='center', fontsize=24, fontweight='bold')\n\nplt.axis('off')\nplt.savefig('cluster1712.pdf'.format(idx), dpi=300)\n</pre> fig, ax = plt.subplots(figsize=(13, 10)) # Plot the base map southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')  group1.plot(ax=ax, color = color_list[1], edgecolor='black', label=group1['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group1.geometry.centroid.x, group1.geometry.centroid.y, group1['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  group2.plot(ax=ax, color = color_list[2], edgecolor='black', label=group2['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group2.geometry.centroid.x, group2.geometry.centroid.y, group2['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  group3.plot(ax=ax, color = color_list[3], edgecolor='black', label=group3['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group3.geometry.centroid.x, group3.geometry.centroid.y, group3['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  group4.plot(ax=ax, color = color_list[4], edgecolor='black', label=group4['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group4.geometry.centroid.x, group4.geometry.centroid.y, group4['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  # Additional title at the bottom #fig.text(0.5, 0.05, 'Cluster 1712 - 1713', ha='center', va='center', fontsize=24, fontweight='bold')  plt.axis('off') plt.savefig('cluster1712.pdf'.format(idx), dpi=300) In\u00a0[329]: Copied! <pre>from matplotlib.lines import Line2D\n\nfig, ax = plt.subplots(figsize=(13, 10))\n# Plot the base map\nsouthScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray')\nplagueSouthScania.plot(ax=ax, color = color_list[15], edgecolor='black', label=plagueSouthScania['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(plagueSouthScania.geometry.centroid.x, plagueSouthScania.geometry.centroid.y, plagueSouthScania['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n\ngroup1.plot(ax=ax, color = color_list[1], edgecolor='black', label=group1['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group1.geometry.centroid.x, group1.geometry.centroid.y, group1['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\ngroup2.plot(ax=ax, color = color_list[2], edgecolor='black', label=group2['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group2.geometry.centroid.x, group2.geometry.centroid.y, group2['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\ngroup3.plot(ax=ax, color = color_list[3], edgecolor='black', label=group3['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group3.geometry.centroid.x, group3.geometry.centroid.y, group3['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\ngroup4.plot(ax=ax, color = color_list[4], edgecolor='black', label=group4['BeginPlaguePeriod'], legend=True)\n # Add names to each polygon\nfor x, y, label in zip(group4.geometry.centroid.x, group4.geometry.centroid.y, group4['ParishName']):\n        ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\n# Additional title at the bottom\n#fig.text(0.5, 0.05, 'Cluster 1712 - 1713', ha='center', va='center', fontsize=24, fontweight='bold')\n\ncustom_lines = [Line2D([0], [0], color=color_list[1], lw=3),\n                Line2D([0], [0], color=color_list[2], lw=3),\n                Line2D([0], [0], color=color_list[4], lw=3),\n                Line2D([0], [0], color=color_list[3], lw=3)\n                ]\nax.legend(custom_lines, ['MAY 1712', 'JUN 1712', 'JUN 1712', 'JUL 1712'], fontsize=9)\nplt.axis('off')\nplt.savefig('southplaguebygroup.png'.format(idx), dpi=300)\n</pre> from matplotlib.lines import Line2D  fig, ax = plt.subplots(figsize=(13, 10)) # Plot the base map southScaniaMap.plot(ax=ax, color=classification, edgecolor='darkgray') plagueSouthScania.plot(ax=ax, color = color_list[15], edgecolor='black', label=plagueSouthScania['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(plagueSouthScania.geometry.centroid.x, plagueSouthScania.geometry.centroid.y, plagueSouthScania['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')   group1.plot(ax=ax, color = color_list[1], edgecolor='black', label=group1['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group1.geometry.centroid.x, group1.geometry.centroid.y, group1['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  group2.plot(ax=ax, color = color_list[2], edgecolor='black', label=group2['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group2.geometry.centroid.x, group2.geometry.centroid.y, group2['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  group3.plot(ax=ax, color = color_list[3], edgecolor='black', label=group3['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group3.geometry.centroid.x, group3.geometry.centroid.y, group3['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  group4.plot(ax=ax, color = color_list[4], edgecolor='black', label=group4['BeginPlaguePeriod'], legend=True)  # Add names to each polygon for x, y, label in zip(group4.geometry.centroid.x, group4.geometry.centroid.y, group4['ParishName']):         ax.text(x, y, label, fontsize=5, ha='center', va='center')  # Additional title at the bottom #fig.text(0.5, 0.05, 'Cluster 1712 - 1713', ha='center', va='center', fontsize=24, fontweight='bold')  custom_lines = [Line2D([0], [0], color=color_list[1], lw=3),                 Line2D([0], [0], color=color_list[2], lw=3),                 Line2D([0], [0], color=color_list[4], lw=3),                 Line2D([0], [0], color=color_list[3], lw=3)                 ] ax.legend(custom_lines, ['MAY 1712', 'JUN 1712', 'JUN 1712', 'JUL 1712'], fontsize=9) plt.axis('off') plt.savefig('southplaguebygroup.png'.format(idx), dpi=300) In\u00a0[44]: Copied! <pre>with plt.xkcd():\n    # Based on \"Stove Ownership\" from XKCD by Randall Munroe\n    # https://xkcd.com/418/\n\n    fig = plt.figure()\n    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n    ax.spines[['top', 'right']].set_visible(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_ylim([-30, 10])\n\n    data = np.ones(100)\n    data[70:] -= np.arange(30)\n\n    ax.annotate(\n        \"THE DAY I REALIZED\\nTHE MODEL DOESN'T\\nWORK ... AT ALL?\",\n        xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))\n    \n    # ax.annotate(\n    #     \"CHECKING THE DATA'S\\n MODEL AGAIN\",\n    #     xy=(99.9, 0.3), arrowprops=dict(arrowstyle='-&gt;'), xytext=(28, -20))\n\n    ax.plot(data)\n\n    ax.set_xlabel('time')\n    ax.set_ylabel('my overall mental health')\n    fig.text(\n        0.5, 0.05,\n        '\"Every day in the modeler\\'s life\"',\n        ha='center')\n</pre> with plt.xkcd():     # Based on \"Stove Ownership\" from XKCD by Randall Munroe     # https://xkcd.com/418/      fig = plt.figure()     ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))     ax.spines[['top', 'right']].set_visible(False)     ax.set_xticks([])     ax.set_yticks([])     ax.set_ylim([-30, 10])      data = np.ones(100)     data[70:] -= np.arange(30)      ax.annotate(         \"THE DAY I REALIZED\\nTHE MODEL DOESN'T\\nWORK ... AT ALL?\",         xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))          # ax.annotate(     #     \"CHECKING THE DATA'S\\n MODEL AGAIN\",     #     xy=(99.9, 0.3), arrowprops=dict(arrowstyle='-&gt;'), xytext=(28, -20))      ax.plot(data)      ax.set_xlabel('time')     ax.set_ylabel('my overall mental health')     fig.text(         0.5, 0.05,         '\"Every day in the modeler\\'s life\"',         ha='center') <pre>findfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\n</pre>"},{"location":"PlagueProject/MetapopModelPlague/","title":"MetapopModelPlague","text":"In\u00a0[1]: Copied! <pre>#Python 3.11.2\n#Import packages\nimport scipy.integrate as scipy\nimport numpy as np\nimport pylab as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport json # for pretty printing\n</pre> #Python 3.11.2 #Import packages import scipy.integrate as scipy import numpy as np import pylab as pl import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from collections import defaultdict import json # for pretty printing <p>Initializing the population size, the initial conditions and the number of patches:</p> In\u00a0[2]: Copied! <pre># Number of patches\nn = 19\n\n # vector of population sizes with lenght n\nN = np.array([204  # Bromma\n              , 217  # Oja\n              , 1895  # S Maria Ystad 1749\n              , 554  # Valleberga\n              , 693  # S Kopinge\n              , 403  # Horups\n              , 582  # Bj\u00e4resj\u00f6 1780\n              , 716  # Villie 1749\n              , 418  # Sn\u00e5restad 1775\n              , 519  # Sk\u00e5rby 1749\n              , 262  # Hammenh\u00f6gs 1749\n              , 560  # Glemminge 1775\n              , 236  # Balk\u00e5kra 1775\n              , 334  # Baldringe 1749\n              , 299  # Ovraby\n              , 761  # S\u00f6vestads 1749\n              , 776  # L\u00f6derups 1749\n              , 951  # Borrby 1775\n              , 358  # Tosterups 1775\n              ]) \n\n# Initial conditions for each patch\n\nI0 = np.zeros(n)  # vector of initial infecteds with lenght n\nI0[0] = 1.0  # the first element of the I0 vector is set to 1\n\nS0 = np.zeros(n)  # vector of initial susceptibles with lenght n\nfor i in range(n):\n    S0[i] = N[i] - I0[i]\n\nR0 = np.zeros(n)  # vector of initial removeds with lenght n\nD0 = np.zeros(n)  # vector of initial deaths with lenght n\n\n#print(S0,I0,R0,D0)\n</pre> # Number of patches n = 19   # vector of population sizes with lenght n N = np.array([204  # Bromma               , 217  # Oja               , 1895  # S Maria Ystad 1749               , 554  # Valleberga               , 693  # S Kopinge               , 403  # Horups               , 582  # Bj\u00e4resj\u00f6 1780               , 716  # Villie 1749               , 418  # Sn\u00e5restad 1775               , 519  # Sk\u00e5rby 1749               , 262  # Hammenh\u00f6gs 1749               , 560  # Glemminge 1775               , 236  # Balk\u00e5kra 1775               , 334  # Baldringe 1749               , 299  # Ovraby               , 761  # S\u00f6vestads 1749               , 776  # L\u00f6derups 1749               , 951  # Borrby 1775               , 358  # Tosterups 1775               ])   # Initial conditions for each patch  I0 = np.zeros(n)  # vector of initial infecteds with lenght n I0[0] = 1.0  # the first element of the I0 vector is set to 1  S0 = np.zeros(n)  # vector of initial susceptibles with lenght n for i in range(n):     S0[i] = N[i] - I0[i]  R0 = np.zeros(n)  # vector of initial removeds with lenght n D0 = np.zeros(n)  # vector of initial deaths with lenght n  #print(S0,I0,R0,D0) In\u00a0[3]: Copied! <pre># Defining the transmission rate matrix as a function of two parameters\ndef TransmissionRateMatrix(beta: float, p: float)-&gt; np.ndarray:\n    return(\n        np.array([\n            [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],\n            [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],\n            [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],\n            [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],\n            [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],\n            [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],\n            [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],\n            [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],\n            [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],\n            [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],\n            [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]\n     ] )\n    )\n    \nprint(TransmissionRateMatrix(2, 1)[0][2])\nsize = TransmissionRateMatrix(2, 1).shape\nprint(size)\n</pre> # Defining the transmission rate matrix as a function of two parameters def TransmissionRateMatrix(beta: float, p: float)-&gt; np.ndarray:     return(         np.array([             [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],             [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],             [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],             [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],             [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],             [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],             [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],             [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],             [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]      ] )     )      print(TransmissionRateMatrix(2, 1)[0][2]) size = TransmissionRateMatrix(2, 1).shape print(size)  <pre>0\n(19, 19)\n</pre> <p>Generating the differential equations</p> In\u00a0[4]: Copied! <pre>def SIRD_model(y, t,\n               model_parameters : tuple[dict]):\n    \n    parameters = model_parameters[0]\n    beta : float = parameters['beta']\n    gamma : float = parameters['gamma']\n    p : float = parameters['p']\n    mu :float = parameters['mu']\n    N = parameters['N']\n    n : int = parameters['n']\n\n    S = defaultdict(float)\n    I = defaultdict(float)\n    R = defaultdict(float)\n    D = defaultdict(float)\n\n    vars = tuple(sum([ [ S[i], I[i], R[i], D[i] ] for i in range(n) ], []))\n    vars = y\n\n   #Choosing the corresponding output for each subpopulation\n    def entryS(i):\n        return vars[4 * i]\n    def entryI(i):\n        return vars[4 * i + 1]\n    def entryR(i):\n        return vars[4 * i + 2]\n    def entryD(i):\n        return vars[4 * i + 3]\n\n    # Initializando the directory for each subpopulation\n    dS = {}\n    dI = {}\n    dR = {}\n    dD = {}\n\n    # Defining the differential equations for each subpopulation\n    for i in range(n):\n        dS[i] = - entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta,p)[i][j] * entryI(j) for j in range(n))\n        dI[i] = entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta,p)[i][j] * entryI(j) for j in range(n)) - gamma * entryI(i)\n        dR[i] = gamma * mu * entryI(i)\n        dD[i] = gamma * (1 - mu) * entryI(i)\n\n    derivates = sum([ [ dS[i], dI[i], dR[i], dD[i] ] for i in range(n) ], [])\n    return derivates   # For odeint\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    model = genInput['model']\n    init = genInput['init']\n    beta = genInput['beta']\n    gamma = genInput['gamma']\n    mu = genInput['mu']\n    n = genInput['n']\n    p = genInput['p']\n    N = genInput['N']\n    T = genInput['T']\n\n    # Initial conditions vector for the metapopulation model. len(initConditions) = 4*n\n    initConditions = tuple(sum([ [ init['S'][i], init['I'][i], init['R'][i], init['D'][i] ] for i in range(n) ], []))\n       \n    # Time vector\n    t = np.linspace(0, T, T+1)\n    \n    # Computing the numerical solution\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {}\n    indexVar['S'] = 0\n    indexVar['I'] = 1\n    indexVar['R'] = 2\n    indexVar['D'] = 3\n\n    def varSol(patch, var):\n        return solution[:, 4*patch + indexVar[var]]\n    \n    return {'S': {patch: varSol(patch, 'S') for patch in range(n)},\n            'I': {patch: varSol(patch, 'I') for patch in range(n)},\n            'R': {patch: varSol(patch, 'R') for patch in range(n)},\n            'D': {patch: varSol(patch, 'D') for patch in range(n)},\n            'N': N,\n            'init': init,\n            'beta': beta,\n            'gamma': gamma,\n            'mu': mu,\n            't': t,\n            'n' : n,\n            'p' : p,\n            'model': model,\n            'raw_solution': solution}\n\n\nModel_test = {'model': SIRD_model,\n            'init': {\n                'S': S0,\n                'I': I0,\n                'R': R0,\n                'D': D0,\n            }, #defining the initial values for the model\n            'beta': 0.484,\n            'gamma': 0.32,\n            'mu': 0.6,\n            'N': N,\n            'n': 19,\n            'p': 0.000000004,\n            'T': 700}\n\nmodel_dict = generate_sol(Model_test)\n</pre> def SIRD_model(y, t,                model_parameters : tuple[dict]):          parameters = model_parameters[0]     beta : float = parameters['beta']     gamma : float = parameters['gamma']     p : float = parameters['p']     mu :float = parameters['mu']     N = parameters['N']     n : int = parameters['n']      S = defaultdict(float)     I = defaultdict(float)     R = defaultdict(float)     D = defaultdict(float)      vars = tuple(sum([ [ S[i], I[i], R[i], D[i] ] for i in range(n) ], []))     vars = y     #Choosing the corresponding output for each subpopulation     def entryS(i):         return vars[4 * i]     def entryI(i):         return vars[4 * i + 1]     def entryR(i):         return vars[4 * i + 2]     def entryD(i):         return vars[4 * i + 3]      # Initializando the directory for each subpopulation     dS = {}     dI = {}     dR = {}     dD = {}      # Defining the differential equations for each subpopulation     for i in range(n):         dS[i] = - entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta,p)[i][j] * entryI(j) for j in range(n))         dI[i] = entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta,p)[i][j] * entryI(j) for j in range(n)) - gamma * entryI(i)         dR[i] = gamma * mu * entryI(i)         dD[i] = gamma * (1 - mu) * entryI(i)      derivates = sum([ [ dS[i], dI[i], dR[i], dD[i] ] for i in range(n) ], [])     return derivates   # For odeint  def generate_sol(genInput: dict) -&gt; dict:     model = genInput['model']     init = genInput['init']     beta = genInput['beta']     gamma = genInput['gamma']     mu = genInput['mu']     n = genInput['n']     p = genInput['p']     N = genInput['N']     T = genInput['T']      # Initial conditions vector for the metapopulation model. len(initConditions) = 4*n     initConditions = tuple(sum([ [ init['S'][i], init['I'][i], init['R'][i], init['D'][i] ] for i in range(n) ], []))             # Time vector     t = np.linspace(0, T, T+1)          # Computing the numerical solution     solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {}     indexVar['S'] = 0     indexVar['I'] = 1     indexVar['R'] = 2     indexVar['D'] = 3      def varSol(patch, var):         return solution[:, 4*patch + indexVar[var]]          return {'S': {patch: varSol(patch, 'S') for patch in range(n)},             'I': {patch: varSol(patch, 'I') for patch in range(n)},             'R': {patch: varSol(patch, 'R') for patch in range(n)},             'D': {patch: varSol(patch, 'D') for patch in range(n)},             'N': N,             'init': init,             'beta': beta,             'gamma': gamma,             'mu': mu,             't': t,             'n' : n,             'p' : p,             'model': model,             'raw_solution': solution}   Model_test = {'model': SIRD_model,             'init': {                 'S': S0,                 'I': I0,                 'R': R0,                 'D': D0,             }, #defining the initial values for the model             'beta': 0.484,             'gamma': 0.32,             'mu': 0.6,             'N': N,             'n': 19,             'p': 0.000000004,             'T': 700}  model_dict = generate_sol(Model_test) <p>Plotting the solutions</p> In\u00a0[5]: Copied! <pre>#Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch] # list of floats         \n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n                for t in range(T_inf, T_sup)]\n\n# Plotting the solution\ndef plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):\n\n    for key in state:\n        if key not in model:\n            raise ValueError(f\"Invalid state: {key}\")\n        for i in range(model['n']):\n            plt.plot(model['t'], model[key][i], label=f'{key} {i}')\n    plt.xlabel('Time')\n    plt.ylabel('Infectious')\n    plt.title('SIRD model')\n    #plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))\n    plt.show()\n\n#Plotting the infected and daily deaths for the first five patches   \nplt.subplot(211)\nfor i in range(5):\n    plt.plot(model_dict['I'][i], label=('patch %s' %(i+1)))\nplt.xlabel('Time')\nplt.ylabel('Infectious')\npl.legend(loc=1)\nplt.subplot(212)\nfor i in range(5):\n    plt.plot(daily_deaths(model_dict, i, 0, 700), label=('patch %s' %(i+1)))\nplt.xlabel('Time')\nplt.ylabel('Daily Deaths')\npl.legend(loc=1)\nplt.show()\n</pre> #Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch] # list of floats              return [cumulative_deaths[t+1] - cumulative_deaths[t]                 for t in range(T_inf, T_sup)]  # Plotting the solution def plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):      for key in state:         if key not in model:             raise ValueError(f\"Invalid state: {key}\")         for i in range(model['n']):             plt.plot(model['t'], model[key][i], label=f'{key} {i}')     plt.xlabel('Time')     plt.ylabel('Infectious')     plt.title('SIRD model')     #plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))     plt.show()  #Plotting the infected and daily deaths for the first five patches    plt.subplot(211) for i in range(5):     plt.plot(model_dict['I'][i], label=('patch %s' %(i+1))) plt.xlabel('Time') plt.ylabel('Infectious') pl.legend(loc=1) plt.subplot(212) for i in range(5):     plt.plot(daily_deaths(model_dict, i, 0, 700), label=('patch %s' %(i+1))) plt.xlabel('Time') plt.ylabel('Daily Deaths') pl.legend(loc=1) plt.show()"},{"location":"PlagueProject/MetapopPlaguelikeKeeling/","title":"MetapopPlaguelikeKeeling","text":"In\u00a0[\u00a0]: Copied! <pre>#Import packages\nimport scipy.integrate as scipy\nimport numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> #Import packages import scipy.integrate as scipy import numpy as np import pylab as pl import pandas as pd import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>n = 2    # Number of patches\nbeta = 0.34    # Infection rate\ngamma = 0.32    # Recovery rate\np = 0.005    # Probability of movement\nmu = 0.6    # Birth rate\n</pre> n = 2    # Number of patches beta = 0.34    # Infection rate gamma = 0.32    # Recovery rate p = 0.005    # Probability of movement mu = 0.6    # Birth rate  In\u00a0[\u00a0]: Copied! <pre># vector of population sizes with lenght n\nN = np.array([204  # Bromma\n              , 217  # Oja\n              , 1895  # S Maria Ystad 1749\n              , 554  # Valleberga\n              , 693  # S Kopinge\n              , 403  # Horups\n              , 582  # Bj\u00e4resj\u00f6 1780\n              , 716  # Villie 1749\n              , 418  # Sn\u00e5restad 1775\n              , 519  # Sk\u00e5rby 1749\n              , 262  # Hammenh\u00f6gs 1749\n              , 560  # Glemminge 1775\n              , 236  # Balk\u00e5kra 1775\n              , 334  # Baldringe 1749\n              , 299  # Ovraby\n              , 761  # S\u00f6vestads 1749\n              , 776  # L\u00f6derups 1749\n              , 951  # Borrby 1775\n              , 358  # Tosterups 1775\n              ])\n\n# Initial conditions for each patch\n\nI0 = np.zeros(n)  # vector of initial infecteds with lenght n\nI0[0] = 1.0  # the first element of the I0 vector is set to 1\n\nS0 = np.zeros(n)  # vector of initial susceptibles with lenght n\nfor i in range(n):\n    S0[i] = N[i] - I0[i]\n\nR0 = np.zeros(n)  # vector of initial removeds with lenght n\nD0 = np.zeros(n)  # vector of initial deaths with lenght n\n\n# Defining the transmission rate matrix as a function of two parameters\n\n\ndef TransmissionRateMatrix(beta: float, p: float):\n    return ([\n        [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],\n        [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],\n        [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],\n        [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],\n        [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],\n        [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],\n        [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],\n        [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],\n        [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],\n        [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],\n        [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],\n        [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],\n        [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]\n    ])\n\n\nprint(TransmissionRateMatrix(0.5, 0.3)[0][18])\n</pre> # vector of population sizes with lenght n N = np.array([204  # Bromma               , 217  # Oja               , 1895  # S Maria Ystad 1749               , 554  # Valleberga               , 693  # S Kopinge               , 403  # Horups               , 582  # Bj\u00e4resj\u00f6 1780               , 716  # Villie 1749               , 418  # Sn\u00e5restad 1775               , 519  # Sk\u00e5rby 1749               , 262  # Hammenh\u00f6gs 1749               , 560  # Glemminge 1775               , 236  # Balk\u00e5kra 1775               , 334  # Baldringe 1749               , 299  # Ovraby               , 761  # S\u00f6vestads 1749               , 776  # L\u00f6derups 1749               , 951  # Borrby 1775               , 358  # Tosterups 1775               ])  # Initial conditions for each patch  I0 = np.zeros(n)  # vector of initial infecteds with lenght n I0[0] = 1.0  # the first element of the I0 vector is set to 1  S0 = np.zeros(n)  # vector of initial susceptibles with lenght n for i in range(n):     S0[i] = N[i] - I0[i]  R0 = np.zeros(n)  # vector of initial removeds with lenght n D0 = np.zeros(n)  # vector of initial deaths with lenght n  # Defining the transmission rate matrix as a function of two parameters   def TransmissionRateMatrix(beta: float, p: float):     return ([         [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],         [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],         [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],         [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],         [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],         [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],         [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],         [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],         [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],         [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],         [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],         [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],         [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],         [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],         [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]     ])   print(TransmissionRateMatrix(0.5, 0.3)[0][18])  <pre>0\n</pre> In\u00a0[\u00a0]: Copied! <pre>INPUT0 = np.hstack((S0, I0, R0, D0))#The numpy.hstack() function takes the arrays S0, I0, R0, D0 and stacks them horizontally to make a single array of (n,4). \nINPUT = np.zeros((4*n))\nfor i in range(n):\n    INPUT[4*i] = INPUT0[i]\n    INPUT[1+4*i] = INPUT0[n+i]\n    INPUT[2+4*i] = INPUT0[2*n+i]\n</pre> INPUT0 = np.hstack((S0, I0, R0, D0))#The numpy.hstack() function takes the arrays S0, I0, R0, D0 and stacks them horizontally to make a single array of (n,4).  INPUT = np.zeros((4*n)) for i in range(n):     INPUT[4*i] = INPUT0[i]     INPUT[1+4*i] = INPUT0[n+i]     INPUT[2+4*i] = INPUT0[2*n+i]"},{"location":"PlagueProject/Model_transmission_matrix_symmetric/","title":"Model transmission matrix symmetric","text":"In\u00a0[208]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[209]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[210]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[211]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)     In\u00a0[212]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[213]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[214]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]] <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[215]: Copied! <pre>southScania = get_centroid(southScania)\n</pre> southScania = get_centroid(southScania) In\u00a0[216]: Copied! <pre>example = southScania[(southScania[\"Region\"]== 'MIDDLE')\n                       &amp; (pd.notna(southScania[\"BeginPlaguePeriod\"]))\n                       ]\nlen(example)\n</pre> example = southScania[(southScania[\"Region\"]== 'MIDDLE')                        &amp; (pd.notna(southScania[\"BeginPlaguePeriod\"]))                        ] len(example) Out[216]: <pre>18</pre> <p>Defining a group to work with</p> In\u00a0[217]: Copied! <pre>group1 = southScania[(southScania['ParishName'] == 'YSTAD')\n                 | (southScania['ParishName'] == '\u00d6JA')\n                 | (southScania['ParishName'] == 'BROMMA')\n                 | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (southScania['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (southScania['ParishName'] == 'VALLEBERGA')\n                 | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))\n                 | (southScania['ParishName'] == 'INGELSTORP')\n                 | (southScania['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (southScania['ParishName'] == 'HEDESKOGA')\n                 | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup1 = group1.reset_index(drop=True)\ngroup1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED'\n</pre> group1 = southScania[(southScania['ParishName'] == 'YSTAD')                  | (southScania['ParishName'] == '\u00d6JA')                  | (southScania['ParishName'] == 'BROMMA')                  | (southScania['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (southScania['ParishName'] == 'STORA K\u00d6PINGE')                  | (southScania['ParishName'] == 'VALLEBERGA')                  | ((southScania['ParishName'] == 'H\u00d6RUP') &amp; (southScania['BeginPlaguePeriod']== 'JUL 1712'))                  | ((southScania['ParishName'] == 'GLEMMINGE') &amp; (southScania['BeginPlaguePeriod']== 'AUG 1712'))                  | (southScania['ParishName'] == 'INGELSTORP')                  | (southScania['ParishName'] == 'HAMMENH\u00d6G')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'SEP 1712'))                  | (southScania['ParishName'] == 'HEDESKOGA')                  | ((southScania['ParishName'] == '\u00d6VRABY') &amp; (southScania['BeginPlaguePeriod']== 'NOV 1712')) ]      group1 = group1.reset_index(drop=True) group1.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group1.at[1, 'EndPlaguePeriod'] = 'UNDEFINED' In\u00a0[218]: Copied! <pre>group = group1[['Region', 'ParishName', 'BEF1699', 'BeginPlaguePeriod'\n               , 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\ngroup\n</pre> group = group1[['Region', 'ParishName', 'BEF1699', 'BeginPlaguePeriod'                , 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] group Out[218]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry 0 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 UNDEFINED ? POLYGON ((4228840.232 3178726.042, 4228969.528... 1 SOUTHEAST BROMMA 154 AUG 1712 UNDEFINED ? POLYGON ((4231996.049 3179728.504, 4232042.002... 2 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... 3 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... 4 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... 5 SOUTHEAST GLEMMINGE 362 AUG 1712 AUG 1712 ? POLYGON ((4246778.053 3180955.654, 4246924.466... 6 SOUTHEAST HAMMENH\u00d6G 169 AUG 1712 UNDEFINED ? POLYGON ((4255090.885 3182137.710, 4254726.251... 7 SOUTHEAST H\u00d6RUP 289 JUL 1712 UNDEFINED 60 POLYGON ((4252154.708 3177178.545, 4251691.166... 8 SOUTHEAST INGELSTORP 358 AUG 1712 UNDEFINED 1 POLYGON ((4246833.197 3177120.892, 4246902.701... 9 SOUTHEAST STORA K\u00d6PINGE 376 JUL 1712 JAN 1713 80 POLYGON ((4240456.475 3179067.689, 4240522.090... 10 SOUTHEAST VALLEBERGA 391 JUL 1712 UNDEFINED ? POLYGON ((4249981.772 3179064.746, 4250047.408... 11 SOUTHEAST \u00d6VRABY 208 SEP 1712 SEP 1712 ? POLYGON ((4242987.654 3183129.802, 4242831.234... 12 SOUTHEAST \u00d6VRABY 208 NOV 1712 NOV 1712 ? POLYGON ((4242987.654 3183129.802, 4242831.234... <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[219]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\ngroup = group.replace(['UNDEFINED', '?'], np.nan)\ngroup['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    group['BeginPlaguePeriod'], format='%b %Y')\ngroup['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    group['EndPlaguePeriod'], format='%b %Y')\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe group = group.replace(['UNDEFINED', '?'], np.nan) group['new_format_BeginPlaguePeriod'] = pd.to_datetime(     group['BeginPlaguePeriod'], format='%b %Y') group['new_format_EndPlaguePeriod'] = pd.to_datetime(     group['EndPlaguePeriod'], format='%b %Y') In\u00a0[220]: Copied! <pre>group['VictimsNumber'] = group.apply(convert_to_int, axis=1)\n#parishes_complete_data =group[(group['EndPlaguePeriod'].notna()) &amp; (group['VictimsNumber'].notna())]\n</pre> group['VictimsNumber'] = group.apply(convert_to_int, axis=1) #parishes_complete_data =group[(group['EndPlaguePeriod'].notna()) &amp; (group['VictimsNumber'].notna())] <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[221]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf, parish_name:str='YSTAD'):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        \n        # Find the index of the parish name\n        parish_index = self.patchNames().tolist().index(parish_name)\n\n        # Set I0 to 1 for the parish index\n        self.I0[parish_index] = 1.0\n               \n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].unique()\n\n    def numPatches(self):\n        return len(self.patchNames())\n    \n    def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n        patchPop = []\n        for name in self.patchNames():\n            unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()\n            if len(unique_pop) &gt; 0:\n                patchPop.append(unique_pop[0])  # append only the first unique population value\n        return np.array(patchPop)\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf, parish_name:str='YSTAD'):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)                  # Find the index of the parish name         parish_index = self.patchNames().tolist().index(parish_name)          # Set I0 to 1 for the parish index         self.I0[parish_index] = 1.0                         for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].unique()      def numPatches(self):         return len(self.patchNames())          def patchPop(self, column_pop: str = 'BEF1699', column_name: str = 'ParishName'):         patchPop = []         for name in self.patchNames():             unique_pop = self.gdf[self.gdf[column_name]== name][column_pop].unique()             if len(unique_pop) &gt; 0:                 patchPop.append(unique_pop[0])  # append only the first unique population value         return np.array(patchPop)      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()            <p>Trying a small dataframe</p> In\u00a0[222]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ncluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                          )                \n                       )\n</pre> # Getting the centroid of each polygon for defining the transmission matrix cluster1 = get_centroid(add_Begin_End_days(sort_by_date(group)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                           )                                        ) In\u00a0[223]: Copied! <pre># cluster1.to_csv('Ystad_cluster.csv')\n# cluster1.to_file('Ystad_cluster.json', driver='GeoJSON')\n</pre> # cluster1.to_csv('Ystad_cluster.csv') # cluster1.to_file('Ystad_cluster.json', driver='GeoJSON')  In\u00a0[235]: Copied! <pre>#example=cluster1.drop([0,1,2,3,5,8,11,20,26,28])\n# example = cluster1\n# model_input = Initial_Model(example, 'YSTAD')\nexample = cluster1[(cluster1['ParishName'] == 'YSTAD')\n                | (cluster1['ParishName'] == '\u00d6JA')\n                | (cluster1['ParishName'] == 'HEDESKOGA')\n                ]\nmodel_input = Initial_Model(example, 'YSTAD')\nexample\n</pre> #example=cluster1.drop([0,1,2,3,5,8,11,20,26,28]) # example = cluster1 # model_input = Initial_Model(example, 'YSTAD') example = cluster1[(cluster1['ParishName'] == 'YSTAD')                 | (cluster1['ParishName'] == '\u00d6JA')                 | (cluster1['ParishName'] == 'HEDESKOGA')                 ] model_input = Initial_Model(example, 'YSTAD') example Out[235]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry new_format_BeginPlaguePeriod new_format_EndPlaguePeriod BeginDaysPlague EndDaysPlague centroid 0 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740.0 POLYGON ((4240506.248 3176029.581, 4238053.574... 1712-06-01 1712-12-01 0 213 POINT (4235880.810923543 3175774.141675906) 1 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40.0 POLYGON ((4236218.454 3180039.080, 4236359.530... 1712-06-01 1713-03-01 0 303 POINT (4236171.52874792 3178038.800015468) 10 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5.0 POLYGON ((4233181.208 3176354.140, 4233118.055... 1712-09-01 1712-10-01 92 152 POINT (4233060.46303023 3177329.2552913846) <p>SCENARIO-1: Considering one beta, one mu, and one p in the model.</p> <p>Generating the differential equations</p> In\u00a0[236]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[238]: Copied! <pre># Transmission matrix defined for SCENARIO 1. \ndef trans_matrix1(gdf: gpd.GeoDataFrame, beta:float, p:float, column_name: str = 'ParishName', column_geometry: str = 'geometry'):\n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n    trans_matrix = np.full((len_unique_names,len_unique_names),p, dtype=float)\n    for i in range(len_unique_names):\n        for j in range(i+1, len_unique_names): \n            polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0]\n            polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0]\n            # If polygons don't touch, set value in trans_matrix to 0\n            if not polygon_i.touches(polygon_j):\n                trans_matrix[i,j] = 0\n                trans_matrix[j,i] = 0\n    np.fill_diagonal(trans_matrix, beta) \n    return trans_matrix\n</pre> # Transmission matrix defined for SCENARIO 1.  def trans_matrix1(gdf: gpd.GeoDataFrame, beta:float, p:float, column_name: str = 'ParishName', column_geometry: str = 'geometry'):     unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)     trans_matrix = np.full((len_unique_names,len_unique_names),p, dtype=float)     for i in range(len_unique_names):         for j in range(i+1, len_unique_names):              polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0]             polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0]             # If polygons don't touch, set value in trans_matrix to 0             if not polygon_i.touches(polygon_j):                 trans_matrix[i,j] = 0                 trans_matrix[j,i] = 0     np.fill_diagonal(trans_matrix, beta)      return trans_matrix In\u00a0[239]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] \n    mu = parameters['mu'] \n    p = parameters['p'] \n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + trans_matrix1(gdf, beta, p)\n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n    \n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta']      mu = parameters['mu']      p = parameters['p']      gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + trans_matrix1(gdf, beta, p)      sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]          derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Defining the optimization problem:</p> In\u00a0[241]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'\n                      , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'\n                        , deathData: str = 'VictimsNumber'\n                    ):\n    \n    parameters = np.array(parameters)\n      \n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[0]\n    mu:  np.array = parameters[1]\n    p: np.array = parameters[2]\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p': p,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Groupby operation\n    grouped_by_parish = gdf.groupby(column_name)\n\n    errors = np.zeros(n)\n    \n    for i, (current_df) in enumerate(grouped_by_parish):\n        len_data_parish = len(current_df)\n        if len_data_parish &lt; 2:         \n            initial_position = current_df[beginTime].values[0]\n            final_position = current_df[endTime].values[0]\n            deaths = current_df[deathData].values[0]\n            errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            if (final_position != 0 and deaths != 0):\n                errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))\n        else:\n            position = current_df[endTime].values\n            monthly_deaths = current_df[deathData].values\n            point_error = abs(model_sol['D'][i][position] - monthly_deaths)\n            errors[i] = np.sum(point_error)\n    \n    totalError = np.sum(errors)\n    return totalError\n\n# Solve the optimization problem and print the result\n\nn = model_input.n\n\nbeta_bounds = [(0.0, 1.0)]\nmu_bounds = [(0.0, 1.0)]\np_bounds = [(0.0, 1.0)]\nbounds = beta_bounds + mu_bounds + p_bounds\n\nresult = gp_minimize(objectiveFunction, bounds, n_calls=40, random_state=0)\n\nparameters_estimated = np.array(result.x)\nbeta_estimated = parameters_estimated[0]\nmu_estimated = parameters_estimated[1]\np_estimated = parameters_estimated[2]\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'                       , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'                         , deathData: str = 'VictimsNumber'                     ):          parameters = np.array(parameters)            n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[0]     mu:  np.array = parameters[1]     p: np.array = parameters[2]      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p': p,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Groupby operation     grouped_by_parish = gdf.groupby(column_name)      errors = np.zeros(n)          for i, (current_df) in enumerate(grouped_by_parish):         len_data_parish = len(current_df)         if len_data_parish &lt; 2:                      initial_position = current_df[beginTime].values[0]             final_position = current_df[endTime].values[0]             deaths = current_df[deathData].values[0]             errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             if (final_position != 0 and deaths != 0):                 errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))         else:             position = current_df[endTime].values             monthly_deaths = current_df[deathData].values             point_error = abs(model_sol['D'][i][position] - monthly_deaths)             errors[i] = np.sum(point_error)          totalError = np.sum(errors)     return totalError  # Solve the optimization problem and print the result  n = model_input.n  beta_bounds = [(0.0, 1.0)] mu_bounds = [(0.0, 1.0)] p_bounds = [(0.0, 1.0)] bounds = beta_bounds + mu_bounds + p_bounds  result = gp_minimize(objectiveFunction, bounds, n_calls=40, random_state=0)  parameters_estimated = np.array(result.x) beta_estimated = parameters_estimated[0] mu_estimated = parameters_estimated[1] p_estimated = parameters_estimated[2]  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n/opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 6\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=65'&gt;66&lt;/a&gt; p_bounds = [(0.0, 1.0)]\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=66'&gt;67&lt;/a&gt; bounds = beta_bounds + mu_bounds + p_bounds\n---&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=68'&gt;69&lt;/a&gt; result = gp_minimize(objectiveFunction, bounds, n_calls=200, random_state=0)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=70'&gt;71&lt;/a&gt; parameters_estimated = np.array(result.x)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=71'&gt;72&lt;/a&gt; beta_estimated = parameters_estimated[0]\n\nFile /opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/gp.py:259, in gp_minimize(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\n    254 if base_estimator is None:\n    255     base_estimator = cook_estimator(\n    256         \"GP\", space=space, random_state=rng.randint(0, np.iinfo(np.int32).max),\n    257         noise=noise)\n--&gt; 259 return base_minimize(\n    260     func, space, base_estimator=base_estimator,\n    261     acq_func=acq_func,\n    262     xi=xi, kappa=kappa, acq_optimizer=acq_optimizer, n_calls=n_calls,\n    263     n_points=n_points, n_random_starts=n_random_starts,\n    264     n_initial_points=n_initial_points,\n    265     initial_point_generator=initial_point_generator,\n    266     n_restarts_optimizer=n_restarts_optimizer,\n    267     x0=x0, y0=y0, random_state=rng, verbose=verbose,\n    268     callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\nFile /opt/homebrew/lib/python3.11/site-packages/skopt/optimizer/base.py:299, in base_minimize(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\n    297 for n in range(n_calls):\n    298     next_x = optimizer.ask()\n--&gt; 299     next_y = func(next_x)\n    300     result = optimizer.tell(next_x, next_y)\n    301     result.specs = specs\n\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 3\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt; p: np.array = parameters[2]\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt; model_info = {'model': SEIRD_model,\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=14'&gt;15&lt;/a&gt;               'init': {\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=15'&gt;16&lt;/a&gt;                   'S': model_input.S0,\n   (...)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=31'&gt;32&lt;/a&gt;               'n': model_input.n,\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=32'&gt;33&lt;/a&gt;               'T': model_input.maxDays()}\n---&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=34'&gt;35&lt;/a&gt; model_sol = generate_sol(model_info)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=36'&gt;37&lt;/a&gt; # Groupby operation\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=37'&gt;38&lt;/a&gt; grouped_by_parish = gdf.groupby(column_name)\n\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 5\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=51'&gt;52&lt;/a&gt; t = np.linspace(0, T, T+1)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=53'&gt;54&lt;/a&gt; model = genInput['model']\n---&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=55'&gt;56&lt;/a&gt; solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=57'&gt;58&lt;/a&gt; indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=58'&gt;59&lt;/a&gt; def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/integrate/_odepack_py.py:242, in odeint(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg, tfirst)\n    240 t = copy(t)\n    241 y0 = copy(y0)\n--&gt; 242 output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n    243                          full_output, rtol, atol, tcrit, h0, hmax, hmin,\n    244                          ixpr, mxstep, mxhnil, mxordn, mxords,\n    245                          int(bool(tfirst)))\n    246 if output[-1] &lt; 0:\n    247     warning_msg = _msgs[output[-1]] + \" Run with full_output = 1 to get quantitative information.\"\n\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 3\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=33'&gt;34&lt;/a&gt; sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=35'&gt;36&lt;/a&gt; dS = -entry[:, 0] / N * sum_transmission(t)\n---&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=36'&gt;37&lt;/a&gt; dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=37'&gt;38&lt;/a&gt; dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=38'&gt;39&lt;/a&gt; dR = (gamma * (1 - mu)) * entry[:, 2]\n\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 3\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=29'&gt;30&lt;/a&gt;     seasonal_rate = lambda w : 0\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=31'&gt;32&lt;/a&gt; matrix = lambda w : seasonal_rate(w) + trans_matrix1(gdf, beta, p)\n---&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=33'&gt;34&lt;/a&gt; sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=35'&gt;36&lt;/a&gt; dS = -entry[:, 0] / N * sum_transmission(t)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=36'&gt;37&lt;/a&gt; dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 3\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=28'&gt;29&lt;/a&gt; else:\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=29'&gt;30&lt;/a&gt;     seasonal_rate = lambda w : 0\n---&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=31'&gt;32&lt;/a&gt; matrix = lambda w : seasonal_rate(w) + trans_matrix1(gdf, beta, p)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=33'&gt;34&lt;/a&gt; sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=35'&gt;36&lt;/a&gt; dS = -entry[:, 0] / N * sum_transmission(t)\n\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb Cell 31 line 9\n      &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; for j in range(i+1, len_unique_names): \n      &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt;     polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0]\n----&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt;     polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0]\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=9'&gt;10&lt;/a&gt;     # If polygons don't touch, set value in trans_matrix to 0\n     &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/Model_transmission_matrix_symmetric.ipynb#X41sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt;     if not polygon_i.touches(polygon_j):\n\nFile /opt/homebrew/lib/python3.11/site-packages/geopandas/geodataframe.py:1475, in GeoDataFrame.__getitem__(self, key)\n   1469 def __getitem__(self, key):\n   1470     \"\"\"\n   1471     If the result is a column containing only 'geometry', return a\n   1472     GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\n   1473     return a GeoDataFrame.\n   1474     \"\"\"\n-&gt; 1475     result = super().__getitem__(key)\n   1476     # Custom logic to avoid waiting for pandas GH51895\n   1477     # result is not geometry dtype for multi-indexes\n   1478     if (\n   1479         pd.api.types.is_scalar(key)\n   1480         and key == \"\"\n   (...)\n   1483         and not is_geometry_type(result)\n   1484     ):\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3752, in DataFrame.__getitem__(self, key)\n   3750 # Do we have a (boolean) 1d indexer?\n   3751 if com.is_bool_indexer(key):\n-&gt; 3752     return self._getitem_bool_array(key)\n   3754 # We are left with two options: a single key, and a collection of keys,\n   3755 # We interpret tuples as collections only for non-MultiIndex\n   3756 is_single_key = isinstance(key, tuple) or not is_list_like(key)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3811, in DataFrame._getitem_bool_array(self, key)\n   3808     return self.copy(deep=None)\n   3810 indexer = key.nonzero()[0]\n-&gt; 3811 return self._take_with_is_copy(indexer, axis=0)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:3948, in NDFrame._take_with_is_copy(self, indices, axis)\n   3940 def _take_with_is_copy(self: NDFrameT, indices, axis: Axis = 0) -&gt; NDFrameT:\n   3941     \"\"\"\n   3942     Internal version of the `take` method that sets the `_is_copy`\n   3943     attribute to keep track of the parent dataframe (using in indexing\n   (...)\n   3946     See the docstring of `take` for full explanation of the parameters.\n   3947     \"\"\"\n-&gt; 3948     result = self._take(indices=indices, axis=axis)\n   3949     # Maybe set copy if we didn't actually change the index.\n   3950     if not result._get_axis(axis).equals(self._get_axis(axis)):\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:3932, in NDFrame._take(self, indices, axis, convert_indices)\n   3924     if (\n   3925         axis == 0\n   3926         and indices.ndim == 1\n   3927         and using_copy_on_write()\n   3928         and is_range_indexer(indices, len(self))\n   3929     ):\n   3930         return self.copy(deep=None)\n-&gt; 3932 new_data = self._mgr.take(\n   3933     indices,\n   3934     axis=self._get_block_manager_axis(axis),\n   3935     verify=True,\n   3936     convert_indices=convert_indices,\n   3937 )\n   3938 return self._constructor(new_data).__finalize__(self, method=\"take\")\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:963, in BaseBlockManager.take(self, indexer, axis, verify, convert_indices)\n    960     indexer = maybe_convert_indices(indexer, n, verify=verify)\n    962 new_labels = self.axes[axis].take(indexer)\n--&gt; 963 return self.reindex_indexer(\n    964     new_axis=new_labels,\n    965     indexer=indexer,\n    966     axis=axis,\n    967     allow_dups=True,\n    968     copy=None,\n    969 )\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:747, in BaseBlockManager.reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\n    740     new_blocks = self._slice_take_blocks_ax0(\n    741         indexer,\n    742         fill_value=fill_value,\n    743         only_slice=only_slice,\n    744         use_na_proxy=use_na_proxy,\n    745     )\n    746 else:\n--&gt; 747     new_blocks = [\n    748         blk.take_nd(\n    749             indexer,\n    750             axis=1,\n    751             fill_value=(\n    752                 fill_value if fill_value is not None else blk.fill_value\n    753             ),\n    754         )\n    755         for blk in self.blocks\n    756     ]\n    758 new_axes = list(self.axes)\n    759 new_axes[axis] = new_axis\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:748, in &lt;listcomp&gt;(.0)\n    740     new_blocks = self._slice_take_blocks_ax0(\n    741         indexer,\n    742         fill_value=fill_value,\n    743         only_slice=only_slice,\n    744         use_na_proxy=use_na_proxy,\n    745     )\n    746 else:\n    747     new_blocks = [\n--&gt; 748         blk.take_nd(\n    749             indexer,\n    750             axis=1,\n    751             fill_value=(\n    752                 fill_value if fill_value is not None else blk.fill_value\n    753             ),\n    754         )\n    755         for blk in self.blocks\n    756     ]\n    758 new_axes = list(self.axes)\n    759 new_axes[axis] = new_axis\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/blocks.py:945, in Block.take_nd(self, indexer, axis, new_mgr_locs, fill_value)\n    942     allow_fill = True\n    944 # Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\n--&gt; 945 new_values = algos.take_nd(\n    946     values, indexer, axis=axis, allow_fill=allow_fill, fill_value=fill_value\n    947 )\n    949 # Called from three places in managers, all of which satisfy\n    950 #  these assertions\n    951 if isinstance(self, ExtensionBlock):\n    952     # NB: in this case, the 'axis' kwarg will be ignored in the\n    953     #  algos.take_nd call above.\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/array_algos/take.py:97, in take_nd(arr, indexer, axis, fill_value, allow_fill)\n     95 if fill_value is lib.no_default:\n     96     fill_value = na_value_for_dtype(arr.dtype, compat=False)\n---&gt; 97 elif isinstance(arr.dtype, np.dtype) and arr.dtype.kind in \"mM\":\n     98     dtype, fill_value = maybe_promote(arr.dtype, fill_value)\n     99     if arr.dtype != dtype:\n    100         # EA.take is strict about returning a new object of the same type\n    101         # so for that case cast upfront\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:533, in DatetimeArray.dtype(self)\n    530     ts = Timestamp._from_value_and_reso(value, reso=self._creso, tz=self.tz)\n    531     return ts\n--&gt; 533 @property\n    534 # error: Return type \"Union[dtype, DatetimeTZDtype]\" of \"dtype\"\n    535 # incompatible with return type \"ExtensionDtype\" in supertype\n    536 # \"ExtensionArray\"\n    537 def dtype(self) -&gt; np.dtype | DatetimeTZDtype:  # type: ignore[override]\n    538     \"\"\"\n    539     The dtype for the DatetimeArray.\n    540 \n   (...)\n    554         is returned.\n    555     \"\"\"\n    556     return self._dtype\n\nKeyboardInterrupt: </pre> <p>Substituting the estimated values into the model and solving it</p> In\u00a0[50]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[51]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>SCENARIO 2: different betas per parish, one mu, and one p.</p> In\u00a0[52]: Copied! <pre># Transmission matrix defined for SCENARIO 2 and SCENARIO 3. \ndef trans_matrix2(gdf: gpd.GeoDataFrame, beta, p:float, column_name: str = 'ParishName', column_geometry: str = 'geometry'):\n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n    trans_matrix = np.full((len_unique_names,len_unique_names),p, dtype=float)\n    for i in range(len_unique_names):\n        for j in range(i, len_unique_names): \n            if i != j:\n                polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0]\n                polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0]\n                # If polygons don't touch, set value in trans_matrix to 0\n                if not polygon_i.touches(polygon_j):\n                    trans_matrix[i,j] = 0\n                    trans_matrix[j,i] = 0\n            else:\n                trans_matrix[i,j] = beta[i]\n    return trans_matrix\n</pre> # Transmission matrix defined for SCENARIO 2 and SCENARIO 3.  def trans_matrix2(gdf: gpd.GeoDataFrame, beta, p:float, column_name: str = 'ParishName', column_geometry: str = 'geometry'):     unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)     trans_matrix = np.full((len_unique_names,len_unique_names),p, dtype=float)     for i in range(len_unique_names):         for j in range(i, len_unique_names):              if i != j:                 polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0]                 polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0]                 # If polygons don't touch, set value in trans_matrix to 0                 if not polygon_i.touches(polygon_j):                     trans_matrix[i,j] = 0                     trans_matrix[j,i] = 0             else:                 trans_matrix[i,j] = beta[i]     return trans_matrix In\u00a0[53]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # beta is a vector of length n\n    mu = parameters['mu'] \n    p = parameters['p'] \n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + trans_matrix2(gdf, beta, p)\n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n    \n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # beta is a vector of length n     mu = parameters['mu']      p = parameters['p']      gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + trans_matrix2(gdf, beta, p)      sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]          derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} In\u00a0[54]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'\n                      , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'\n                        , deathData: str = 'VictimsNumber'\n                    ):\n    \n    parameters = np.array(parameters)  \n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n]\n    mu:  np.array = parameters[n:n+1]\n    p: np.array = parameters[n+1:]\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p': p,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Groupby operation\n    grouped_by_parish = gdf.groupby(column_name)\n\n    errors = np.zeros(n)\n    \n    for i, (name, current_df) in enumerate(grouped_by_parish):\n        len_data_parish = len(current_df)\n        if len_data_parish &lt; 2:         \n            initial_position = current_df[beginTime].values[0]\n            final_position = current_df[endTime].values[0]\n            deaths = current_df[deathData].values[0]\n            errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            if (final_position != 0 and deaths != 0):\n                errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))/len_data_parish\n        else:\n            position = current_df[endTime].values\n            monthly_deaths = current_df[deathData].values\n            point_error = abs(model_sol['D'][i][position] - monthly_deaths)/len_data_parish\n            errors[i] = np.sum(point_error)\n    \n    totalError = np.sum(errors)\n    return totalError\n\n# Solve the optimization problem and print the result\nn = model_input.n\n\nbeta_bounds = [(0.0, 1.0)]*n\nmu_bounds = [(0.0, 1.0)]\np_bounds = [(0.0, 1.0)]\nbounds = beta_bounds + mu_bounds + p_bounds\n\nresult = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)\n\nparameters_estimated = np.array(result.x)\nbeta_estimated = parameters_estimated[:n]\nmu_estimated = parameters_estimated[n:n+1]\np_estimated = parameters_estimated[n+1:]\n\n# Reshaping the parameters back to their original shapes\nbeta_estimated = beta_estimated.reshape(n)\nmu_estimated = mu_estimated.reshape(1)\np_estimated = p_estimated.reshape(1)\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'                       , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'                         , deathData: str = 'VictimsNumber'                     ):          parameters = np.array(parameters)       n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n]     mu:  np.array = parameters[n:n+1]     p: np.array = parameters[n+1:]      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p': p,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Groupby operation     grouped_by_parish = gdf.groupby(column_name)      errors = np.zeros(n)          for i, (name, current_df) in enumerate(grouped_by_parish):         len_data_parish = len(current_df)         if len_data_parish &lt; 2:                      initial_position = current_df[beginTime].values[0]             final_position = current_df[endTime].values[0]             deaths = current_df[deathData].values[0]             errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             if (final_position != 0 and deaths != 0):                 errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))/len_data_parish         else:             position = current_df[endTime].values             monthly_deaths = current_df[deathData].values             point_error = abs(model_sol['D'][i][position] - monthly_deaths)/len_data_parish             errors[i] = np.sum(point_error)          totalError = np.sum(errors)     return totalError  # Solve the optimization problem and print the result n = model_input.n  beta_bounds = [(0.0, 1.0)]*n mu_bounds = [(0.0, 1.0)] p_bounds = [(0.0, 1.0)] bounds = beta_bounds + mu_bounds + p_bounds  result = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)  parameters_estimated = np.array(result.x) beta_estimated = parameters_estimated[:n] mu_estimated = parameters_estimated[n:n+1] p_estimated = parameters_estimated[n+1:]  # Reshaping the parameters back to their original shapes beta_estimated = beta_estimated.reshape(n) mu_estimated = mu_estimated.reshape(1) p_estimated = p_estimated.reshape(1)  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  510.21911271127897\nbeta =  [0.23424816 0.         1.         1.        ]\nmu =  [0.04700026]\np =  [0.65775792]\n</pre> In\u00a0[55]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) In\u00a0[56]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['EndDaysPlague'].values\ntick_labels = example['EndPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['EndDaysPlague'].values tick_labels = example['EndPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>SCENARIO 3: different beta's and mu's per parish, and one p.</p> In\u00a0[57]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # beta is a vector of length n\n    mu = parameters['mu'] # mu is a vector of length n\n    p = parameters['p'] \n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + trans_matrix2(gdf, beta, p)\n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n    \n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # beta is a vector of length n     mu = parameters['mu'] # mu is a vector of length n     p = parameters['p']      gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + trans_matrix2(gdf, beta, p)      sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]          derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} In\u00a0[58]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'\n                      , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'\n                        , deathData: str = 'VictimsNumber'\n                    ):\n    \n    parameters = np.array(parameters)  \n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n]\n    mu:  np.array = parameters[n:2*n]\n    p: np.array = parameters[2*n:]\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p': p,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Groupby operation\n    grouped_by_parish = gdf.groupby(column_name)\n\n    errors = np.zeros(n)\n    \n    for i, (name, current_df) in enumerate(grouped_by_parish):\n        len_data_parish = len(current_df)\n        if len_data_parish &lt; 2:         \n            initial_position = current_df[beginTime].values[0]\n            final_position = current_df[endTime].values[0]\n            deaths = current_df[deathData].values[0]\n            errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            if (final_position != 0 and deaths != 0):\n                errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))/len_data_parish\n        else:\n            position = current_df[endTime].values\n            monthly_deaths = current_df[deathData].values\n            point_error = abs(model_sol['D'][i][position] - monthly_deaths)/len_data_parish\n            errors[i] = np.sum(point_error)\n    \n    totalError = np.sum(errors)\n    return totalError\n\n# Solve the optimization problem and print the result\nn = model_input.n\n\nbeta_bounds = [(0.0, 1.0)]*n\nmu_bounds = [(0.0, 1.0)]*n\np_bounds = [(0.0, 1.0)]\nbounds = beta_bounds + mu_bounds + p_bounds\n\nresult = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)\n\nparameters_estimated = np.array(result.x)\nbeta_estimated = parameters_estimated[:n]\nmu_estimated = parameters_estimated[n:2*n]\np_estimated = parameters_estimated[2*n:]\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'                       , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'                         , deathData: str = 'VictimsNumber'                     ):          parameters = np.array(parameters)       n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n]     mu:  np.array = parameters[n:2*n]     p: np.array = parameters[2*n:]      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p': p,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Groupby operation     grouped_by_parish = gdf.groupby(column_name)      errors = np.zeros(n)          for i, (name, current_df) in enumerate(grouped_by_parish):         len_data_parish = len(current_df)         if len_data_parish &lt; 2:                      initial_position = current_df[beginTime].values[0]             final_position = current_df[endTime].values[0]             deaths = current_df[deathData].values[0]             errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             if (final_position != 0 and deaths != 0):                 errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))/len_data_parish         else:             position = current_df[endTime].values             monthly_deaths = current_df[deathData].values             point_error = abs(model_sol['D'][i][position] - monthly_deaths)/len_data_parish             errors[i] = np.sum(point_error)          totalError = np.sum(errors)     return totalError  # Solve the optimization problem and print the result n = model_input.n  beta_bounds = [(0.0, 1.0)]*n mu_bounds = [(0.0, 1.0)]*n p_bounds = [(0.0, 1.0)] bounds = beta_bounds + mu_bounds + p_bounds  result = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)  parameters_estimated = np.array(result.x) beta_estimated = parameters_estimated[:n] mu_estimated = parameters_estimated[n:2*n] p_estimated = parameters_estimated[2*n:]  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  314.3428871148728\nbeta =  [0.06379724 0.         0.53494085 0.08384181]\nmu =  [0.         0.07167827 1.         0.16690667]\np =  [0.41008519]\n</pre> In\u00a0[59]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) In\u00a0[60]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>SCENARIO 4: different beta's, mu's, and p's.</p> In\u00a0[61]: Copied! <pre>def trans_matrix4(gdf: gpd.GeoDataFrame, beta:np.array, p_coeff:np.array, column_name: str = 'ParishName', column_geometry: str = 'geometry'):\n    # Get unique parish names \n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n\n    # Initialize the beta matrix\n    beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)\n    np.fill_diagonal(beta_matrix, beta)\n\n    # Initialize the p_coeff matrix\n    p_coeff = np.full((len_unique_names, len_unique_names), p_coeff, dtype=float)\n\n    # Initialize the p_matrix\n    p_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)\n\n    # Initialize the transmission matrix between patches\n    trans_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)\n\n    for i in range(len_unique_names):\n        for j in range(i+1,len_unique_names):\n            name_i = unique_names[i]\n            name_j = unique_names[j]\n            polygon_i = gdf[gdf[column_name] == name_i][column_geometry].values[0]\n            polygon_j = gdf[gdf[column_name] == name_j][column_geometry].values[0]\n            # pVal = getValueAt(p, n, i, j)\n\n            if polygon_i.touches(polygon_j) and name_i != name_j:\n                p_matrix[i,j] = p_coeff[i,j]\n                p_matrix[j,i] = p_matrix[i,j]\n            else:\n                p_matrix[i,j] = 0\n                p_matrix[j,i] = 0\n    trans_matrix = p_matrix + beta_matrix\n\n    return trans_matrix\n</pre> def trans_matrix4(gdf: gpd.GeoDataFrame, beta:np.array, p_coeff:np.array, column_name: str = 'ParishName', column_geometry: str = 'geometry'):     # Get unique parish names      unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)      # Initialize the beta matrix     beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)     np.fill_diagonal(beta_matrix, beta)      # Initialize the p_coeff matrix     p_coeff = np.full((len_unique_names, len_unique_names), p_coeff, dtype=float)      # Initialize the p_matrix     p_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)      # Initialize the transmission matrix between patches     trans_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)      for i in range(len_unique_names):         for j in range(i+1,len_unique_names):             name_i = unique_names[i]             name_j = unique_names[j]             polygon_i = gdf[gdf[column_name] == name_i][column_geometry].values[0]             polygon_j = gdf[gdf[column_name] == name_j][column_geometry].values[0]             # pVal = getValueAt(p, n, i, j)              if polygon_i.touches(polygon_j) and name_i != name_j:                 p_matrix[i,j] = p_coeff[i,j]                 p_matrix[j,i] = p_matrix[i,j]             else:                 p_matrix[i,j] = 0                 p_matrix[j,i] = 0     trans_matrix = p_matrix + beta_matrix      return trans_matrix   In\u00a0[62]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # beta is a vector of length n\n    mu = parameters['mu'] # mu is a vector of length n\n    p = parameters['p'] # p is a matrix of coefficients of size n x n\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + trans_matrix4(gdf, beta, p)\n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n    \n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # beta is a vector of length n     mu = parameters['mu'] # mu is a vector of length n     p = parameters['p'] # p is a matrix of coefficients of size n x n     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + trans_matrix4(gdf, beta, p)      sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]          derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} In\u00a0[63]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'\n                      , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'\n                        , deathData: str = 'VictimsNumber'\n                    ):\n    \n    parameters = np.array(parameters)  \n    \n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n].reshape(n,)\n    mu:  np.array = parameters[n:2*n].reshape(n,)\n    p: np.array = parameters[2*n:].reshape(n,n)\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p': p,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Groupby operation\n    grouped_by_parish = gdf.groupby(column_name)\n\n    errors = np.zeros(n)\n    \n    for i, (name, current_df) in enumerate(grouped_by_parish):\n        len_data_parish = len(current_df)\n        if len_data_parish &lt; 2:         \n            initial_position = current_df[beginTime].values[0]\n            final_position = current_df[endTime].values[0]\n            deaths = current_df[deathData].values[0]\n            errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            if (final_position != 0 and deaths != 0):\n                errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))/len_data_parish\n        else:\n            position = current_df[endTime].values\n            monthly_deaths = current_df[deathData].values\n            point_error = abs(model_sol['D'][i][position] - monthly_deaths)/len_data_parish\n            errors[i] = np.sum(point_error)\n    \n    totalError = np.sum(errors)\n    return totalError\n\n# Solve the optimization problem and print the result\nn = model_input.n\n\nbeta_bounds = [(0.0, 1.0)]*n\nmu_bounds = [(0.0, 1.0)]*n\np_bounds = [(0.0, 1.0)]*(n**2)\nbounds = beta_bounds + mu_bounds + p_bounds\n\nresult = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)\n\nparameters_estimated = np.array(result.x)\nbeta_estimated = parameters_estimated[:n].reshape(n,)\nmu_estimated = parameters_estimated[n:2*n].reshape(n,)\np_estimated = parameters_estimated[2*n:].reshape(n,n)\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'                       , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'                         , deathData: str = 'VictimsNumber'                     ):          parameters = np.array(parameters)            n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n].reshape(n,)     mu:  np.array = parameters[n:2*n].reshape(n,)     p: np.array = parameters[2*n:].reshape(n,n)      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p': p,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Groupby operation     grouped_by_parish = gdf.groupby(column_name)      errors = np.zeros(n)          for i, (name, current_df) in enumerate(grouped_by_parish):         len_data_parish = len(current_df)         if len_data_parish &lt; 2:                      initial_position = current_df[beginTime].values[0]             final_position = current_df[endTime].values[0]             deaths = current_df[deathData].values[0]             errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             if (final_position != 0 and deaths != 0):                 errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))/len_data_parish         else:             position = current_df[endTime].values             monthly_deaths = current_df[deathData].values             point_error = abs(model_sol['D'][i][position] - monthly_deaths)/len_data_parish             errors[i] = np.sum(point_error)          totalError = np.sum(errors)     return totalError  # Solve the optimization problem and print the result n = model_input.n  beta_bounds = [(0.0, 1.0)]*n mu_bounds = [(0.0, 1.0)]*n p_bounds = [(0.0, 1.0)]*(n**2) bounds = beta_bounds + mu_bounds + p_bounds  result = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)  parameters_estimated = np.array(result.x) beta_estimated = parameters_estimated[:n].reshape(n,) mu_estimated = parameters_estimated[n:2*n].reshape(n,) p_estimated = parameters_estimated[2*n:].reshape(n,n)  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  322.80761898020626\nbeta =  [0.         0.37494025 0.49396017 0.01273258]\nmu =  [0.         0.         1.         0.27842865]\np =  [[0.01698767 0.34962728 0.43714813 0.87640502]\n [0.72182987 0.27401243 0.47604301 0.00542061]\n [0.40975478 0.08984837 0.57868247 0.07960025]\n [0.8951978  0.24805556 0.57034719 0.53677993]]\n</pre> In\u00a0[64]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) In\u00a0[65]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>SCENARIO 5: different beta's, mu's, one p considering the gravitational term.</p> In\u00a0[81]: Copied! <pre>def trans_matrix5(gdf: gpd.GeoDataFrame, beta:np.array, p:float\n                  , column_name: str = 'ParishName'\n                  , column_geometry: str = 'geometry'\n                  , column_centroid: str = 'centroid'\n                  , column_pop: str = 'BEF1699'):\n    # Get unique parish names \n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n\n    # Initialize the beta matrix\n    beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)\n    np.fill_diagonal(beta_matrix, beta)\n\n    # Initialize the p_coeff matrix\n    gravitational = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)\n\n    # Initialize the transmission matrix between patches\n    trans_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)\n\n    for i in range(len_unique_names):\n        for j in range(i+1,len_unique_names):\n            name_i = unique_names[i]\n            name_j = unique_names[j]\n            centroid_i = gdf[gdf[column_name] == name_i][column_centroid].values[0]\n            centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]\n            pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]\n            pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]\n            polygon_i = gdf[gdf[column_name] == name_i][column_geometry].values[0]\n            polygon_j = gdf[gdf[column_name] == name_j][column_geometry].values[0]\n            # pVal = getValueAt(p, n, i, j)\n\n            if polygon_i.touches(polygon_j) and name_i != name_j:\n                gravitational[i,j] = p*((pop_i * pop_j) / (centroid_i.distance(centroid_j)**2))\n                gravitational[j,i] = gravitational[i,j]\n            else:\n                gravitational[i,j] = 0\n                gravitational[j,i] = 0\n    trans_matrix = gravitational + beta_matrix\n    return trans_matrix\n</pre> def trans_matrix5(gdf: gpd.GeoDataFrame, beta:np.array, p:float                   , column_name: str = 'ParishName'                   , column_geometry: str = 'geometry'                   , column_centroid: str = 'centroid'                   , column_pop: str = 'BEF1699'):     # Get unique parish names      unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)      # Initialize the beta matrix     beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)     np.fill_diagonal(beta_matrix, beta)      # Initialize the p_coeff matrix     gravitational = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)      # Initialize the transmission matrix between patches     trans_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)      for i in range(len_unique_names):         for j in range(i+1,len_unique_names):             name_i = unique_names[i]             name_j = unique_names[j]             centroid_i = gdf[gdf[column_name] == name_i][column_centroid].values[0]             centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]             pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]             pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]             polygon_i = gdf[gdf[column_name] == name_i][column_geometry].values[0]             polygon_j = gdf[gdf[column_name] == name_j][column_geometry].values[0]             # pVal = getValueAt(p, n, i, j)              if polygon_i.touches(polygon_j) and name_i != name_j:                 gravitational[i,j] = p*((pop_i * pop_j) / (centroid_i.distance(centroid_j)**2))                 gravitational[j,i] = gravitational[i,j]             else:                 gravitational[i,j] = 0                 gravitational[j,i] = 0     trans_matrix = gravitational + beta_matrix     return trans_matrix   In\u00a0[82]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta'] # beta is a vector of length n\n    mu = parameters['mu'] # mu is a vector of length n\n    p = parameters['p'] # p is a float\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    bump_center = parameters['bump_center']\n    bump_width = parameters['bump_width']\n    bump_height = parameters['bump_height']\n    N = parameters['N']\n    n = parameters['n']\n\n    vars = y\n    \n    def entryfun(i, offset): return vars[5 * i + offset]\n\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n\n    matrix = lambda w : seasonal_rate(w) + trans_matrix5(gdf, beta, p)\n\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]\n    dI = sigma * entry[:, 1] - gamma * entry[:, 2]\n    dR = (gamma * (1 - mu)) * entry[:, 2]\n    dD = (gamma * mu) * entry[:, 2]\n    \n    derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}\n    def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta'] # beta is a vector of length n     mu = parameters['mu'] # mu is a vector of length n     p = parameters['p'] # p is a float     gamma = parameters['gamma']     sigma = parameters['sigma']     bump_center = parameters['bump_center']     bump_width = parameters['bump_width']     bump_height = parameters['bump_height']     N = parameters['N']     n = parameters['n']      vars = y          def entryfun(i, offset): return vars[5 * i + offset]      # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(5)] for i in range(len(vars) // 5)])      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0      matrix = lambda w : seasonal_rate(w) + trans_matrix5(gdf, beta, p)      sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 2], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dE = entry[:, 0] / N * sum_transmission(t) - sigma * entry[:, 1]     dI = sigma * entry[:, 1] - gamma * entry[:, 2]     dR = (gamma * (1 - mu)) * entry[:, 2]     dD = (gamma * mu) * entry[:, 2]          derivatives = np.stack((dS, dE, dI, dR, dD), axis=1).flatten()     return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i])]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']      solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {'S': 0, 'E': 1, 'I': 2, 'R': 3, 'D': 4}     def varSol(patch, var): return solution[:, 5*patch + indexVar[var]]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} In\u00a0[83]: Copied! <pre>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'\n                      , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'\n                        , deathData: str = 'VictimsNumber'\n                    ):\n    \n    parameters = np.array(parameters)  \n    \n    n = model_input.n\n    # Reshape parameters back to their original shapes\n    beta: np.array = parameters[:n]\n    mu:  np.array = parameters[n:2*n]\n    p: np.array = parameters[2*n:]\n\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': gdf,\n                  'beta': beta,\n                  'p': p,\n                  'mu': mu,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n\n    model_sol = generate_sol(model_info)\n\n    # Groupby operation\n    grouped_by_parish = gdf.groupby(column_name)\n\n    errors = np.zeros(n)\n    \n    for i, (name, current_df) in enumerate(grouped_by_parish):\n        len_data_parish = len(current_df)\n        if len_data_parish &lt; 2:         \n            initial_position = current_df[beginTime].values[0]\n            final_position = current_df[endTime].values[0]\n            deaths = current_df[deathData].values[0]\n            errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)\n            if (final_position != 0 and deaths != 0):\n                errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))\n        else:\n            position = current_df[endTime].values\n            monthly_deaths = current_df[deathData].values\n            point_error = abs(model_sol['D'][i][position] - monthly_deaths)\n            errors[i] = np.sum(point_error)\n    \n    totalError = np.sum(errors)\n    return totalError\n\n# Solve the optimization problem and print the result\nn = model_input.n\n\nbeta_bounds = [(0.0, 1.0)]*n\nmu_bounds = [(0.0, 1.0)]*n\np_bounds = [(0.0, 1.0)]\nbounds = beta_bounds + mu_bounds + p_bounds\n\nresult = gp_minimize(objectiveFunction, bounds, n_calls=200, random_state=0)\n\nparameters_estimated = np.array(result.x)\nbeta_estimated = parameters_estimated[:n]\nmu_estimated = parameters_estimated[n:2*n]\np_estimated = parameters_estimated[2*n:]\n\nprint(\"error = \", result.fun)\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> def objectiveFunction(parameters, gdf: gpd.GeoDataFrame = example, column_name: str = 'ParishName'                       , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague'                         , deathData: str = 'VictimsNumber'                     ):          parameters = np.array(parameters)            n = model_input.n     # Reshape parameters back to their original shapes     beta: np.array = parameters[:n]     mu:  np.array = parameters[n:2*n]     p: np.array = parameters[2*n:]      model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': gdf,                   'beta': beta,                   'p': p,                   'mu': mu,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}      model_sol = generate_sol(model_info)      # Groupby operation     grouped_by_parish = gdf.groupby(column_name)      errors = np.zeros(n)          for i, (name, current_df) in enumerate(grouped_by_parish):         len_data_parish = len(current_df)         if len_data_parish &lt; 2:                      initial_position = current_df[beginTime].values[0]             final_position = current_df[endTime].values[0]             deaths = current_df[deathData].values[0]             errors[i] = abs(model_sol['D'][i][initial_position] - 1.0)             if (final_position != 0 and deaths != 0):                 errors[i] = (errors[i]+ abs(model_sol['D'][i][final_position] - deaths))         else:             position = current_df[endTime].values             monthly_deaths = current_df[deathData].values             point_error = abs(model_sol['D'][i][position] - monthly_deaths)             errors[i] = np.sum(point_error)          totalError = np.sum(errors)     return totalError  # Solve the optimization problem and print the result n = model_input.n  beta_bounds = [(0.0, 1.0)]*n mu_bounds = [(0.0, 1.0)]*n p_bounds = [(0.0, 1.0)] bounds = beta_bounds + mu_bounds + p_bounds  result = gp_minimize(objectiveFunction, bounds, n_calls=200, random_state=0)  parameters_estimated = np.array(result.x) beta_estimated = parameters_estimated[:n] mu_estimated = parameters_estimated[n:2*n] p_estimated = parameters_estimated[2*n:]  print(\"error = \", result.fun) print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) <pre>error =  1789.4916348774443\nbeta =  [0.69286045 0.46025617 0.84927706 0.28078925]\nmu =  [0.         0.15295981 1.         0.74921919]\np =  [0.77408326]\n</pre> In\u00a0[79]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': example,\n                  # defining the initial values for the model\n                  'beta': beta_estimated,\n                  'p':p_estimated,\n                  'mu': mu_estimated,\n                  'gamma': 0.4,\n                  'sigma': 0.17,\n                  'bump_center': 0.0,\n                  'bump_width': 0.0,\n                  'bump_height': 0.0,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': example,                   # defining the initial values for the model                   'beta': beta_estimated,                   'p':p_estimated,                   'mu': mu_estimated,                   'gamma': 0.4,                   'sigma': 0.17,                   'bump_center': 0.0,                   'bump_width': 0.0,                   'bump_height': 0.0,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()} model_solution = generate_sol(model_estimation) In\u00a0[80]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * 1.5*n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n# Wrap the axes array into a numpy array to make it easier to work with\naxes = np.array(axes).reshape(n)\n\n# Plot model solution D for each patch\ntick_positions = example['BeginDaysPlague'].values\ntick_labels = example['BeginPlaguePeriod'].values\n# Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = example.groupby('ParishName')\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data\n\n# Plot model solution D and the data for each patch    \nfor i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df['BeginDaysPlague'].values[0]\n        final_position = current_df['EndDaysPlague'].values[0]\n        deaths = current_df['VictimsNumber'].values[0]\n        if final_position != 0 and deaths != 0:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(final_position, deaths, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n        else:\n            axes[i].plot(initial_position, 0, 'bo')\n            axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)\n            axes[i].set_ylabel('Cumulative Deaths')\n            axes[i].legend(loc='lower right')\n            axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n      \n    else:\n        time = np.zeros(len_data_parish)\n        cumdeathData = np.zeros(len_data_parish)\n        for j in range(len_data_parish):\n            time[j] = current_df['EndDaysPlague'].values[j]\n            cumdeathData[j] = current_df['VictimsNumber'].values[j]         \n        axes[i].plot(time, cumdeathData, 'bo', label='Observed data')\n        axes[i].plot(model_solution[\"D\"][i], \n                        color='orange', label=current_parish)\n        axes[i].set_ylabel('Cummulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * 1.5*n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  # Wrap the axes array into a numpy array to make it easier to work with axes = np.array(axes).reshape(n)  # Plot model solution D for each patch tick_positions = example['BeginDaysPlague'].values tick_labels = example['BeginPlaguePeriod'].values # Create a dictionary where the key is the parish name and the value is the dataframe grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish:     group_dict[name] = data  # Plot model solution D and the data for each patch     for i in range(n):     current_parish = model_input.patchNames()[i]     current_df = group_dict[current_parish]     len_data_parish = len(current_df)     if len_data_parish &lt; 2:                  initial_position = current_df['BeginDaysPlague'].values[0]         final_position = current_df['EndDaysPlague'].values[0]         deaths = current_df['VictimsNumber'].values[0]         if final_position != 0 and deaths != 0:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(final_position, deaths, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)         else:             axes[i].plot(initial_position, 0, 'bo')             axes[i].plot(model_solution['D'][i], color='orange', label=current_parish)             axes[i].set_ylabel('Cumulative Deaths')             axes[i].legend(loc='lower right')             axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9)            else:         time = np.zeros(len_data_parish)         cumdeathData = np.zeros(len_data_parish)         for j in range(len_data_parish):             time[j] = current_df['EndDaysPlague'].values[j]             cumdeathData[j] = current_df['VictimsNumber'].values[j]                  axes[i].plot(time, cumdeathData, 'bo', label='Observed data')         axes[i].plot(model_solution[\"D\"][i],                          color='orange', label=current_parish)         axes[i].set_ylabel('Cummulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels, rotation=70, fontsize=9) # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Set up the data to fit\nn = model_input.n\n\n# # Choose initial guesses for the parameters to fit\nbeta_guess = model_input.beta\nmu_guess = model_input.mu\np_guess = model_input.p_coeff(p_guess=0.3)\ninitial_parameters = np.concatenate(\n    (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)\n\n# Define the bounds for beta, mu and p\nbeta_bounds = [(0,1)]*len(beta_guess.flatten())\nmu_bounds = [(0.1,0.8)]*len(mu_guess.flatten())  # example bounds for mu\np_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p\n\n# Concatenate the bounds\nbounds = beta_bounds + mu_bounds + p_bounds\n# def set_constraint(x):\n#     p = x[2*n:].reshape(n, n)\n#     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n#     #[p[i,j] - p[j,i] == 0 for i in range(n) for j in range(n)] \n#     for i in range(n):\n#         for j in range(n):\n#             if p[i,j] - p[j,i] != 0:\n#                return 1\n#     return 0\n\n# def set_constraint(x):\n#     p = x[2*n:].reshape(n, n)\n#     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n#     diff_matrix = np.abs(p - p.T)\n#     if np.any(diff_matrix != 0):\n#         return 1\n#     return 0\n\n# Minimize the objective function to obtain beta, mu, and p\nresult = optimize.minimize(objectiveFunction, x0=initial_parameters\n                           , args=(example, 'ParishName', 'BeginDaysPlague', 'EndDaysPlague', 'VictimsNumber')\n                           , bounds=bounds\n                           # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other\n                           #, constraints = ({'type': 'eq', 'fun': set_constraint})\n\n\n                           # Constraint for checking that the vector p after a transformation gives a symmetric matrix \n                           #, constraints = ({'type': 'eq', 'fun': lambda p: np.triu(p,1) + np.tril(p, -1)})\n                           )\n\nbeta_estimated = result.x[:n].reshape(n,)\nmu_estimated = result.x[n:2*n].reshape(n,)\np_estimated = result.x[2*n:].reshape(n, n)\n\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"p = \", p_estimated)\n</pre> # Set up the data to fit n = model_input.n  # # Choose initial guesses for the parameters to fit beta_guess = model_input.beta mu_guess = model_input.mu p_guess = model_input.p_coeff(p_guess=0.3) initial_parameters = np.concatenate(     (beta_guess.flatten(), mu_guess.flatten(), p_guess.flatten()), axis=None)  # Define the bounds for beta, mu and p beta_bounds = [(0,1)]*len(beta_guess.flatten()) mu_bounds = [(0.1,0.8)]*len(mu_guess.flatten())  # example bounds for mu p_bounds = [(0,1)]*len(p_guess.flatten())    # example bounds for p  # Concatenate the bounds bounds = beta_bounds + mu_bounds + p_bounds # def set_constraint(x): #     p = x[2*n:].reshape(n, n) #     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other #     #[p[i,j] - p[j,i] == 0 for i in range(n) for j in range(n)]  #     for i in range(n): #         for j in range(n): #             if p[i,j] - p[j,i] != 0: #                return 1 #     return 0  # def set_constraint(x): #     p = x[2*n:].reshape(n, n) #     # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other #     diff_matrix = np.abs(p - p.T) #     if np.any(diff_matrix != 0): #         return 1 #     return 0  # Minimize the objective function to obtain beta, mu, and p result = optimize.minimize(objectiveFunction, x0=initial_parameters                            , args=(example, 'ParishName', 'BeginDaysPlague', 'EndDaysPlague', 'VictimsNumber')                            , bounds=bounds                            # Constraint to check that positions (i,j) and (j,i) of the matrix p are equal to each other                            #, constraints = ({'type': 'eq', 'fun': set_constraint})                              # Constraint for checking that the vector p after a transformation gives a symmetric matrix                             #, constraints = ({'type': 'eq', 'fun': lambda p: np.triu(p,1) + np.tril(p, -1)})                            )  beta_estimated = result.x[:n].reshape(n,) mu_estimated = result.x[n:2*n].reshape(n,) p_estimated = result.x[2*n:].reshape(n, n)  print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"p = \", p_estimated) In\u00a0[\u00a0]: Copied! <pre># Selecting two parishes to test the model\nexample1 = parishes_complete_data[parishes_complete_data['ParishName']=='HEDESKOGA']\n#example = cluster1[cluster1['ParishName'].isin(['YSTAD','\u00d6JA'])]\n#example = cluster1\nexample1\n</pre> # Selecting two parishes to test the model example1 = parishes_complete_data[parishes_complete_data['ParishName']=='HEDESKOGA'] #example = cluster1[cluster1['ParishName'].isin(['YSTAD','\u00d6JA'])] #example = cluster1 example1 In\u00a0[\u00a0]: Copied! <pre>cluster=example1.drop([2,3,4,5,10,11,12,13,14,15])\n#cluster = example1  \nexample = get_centroid(add_Begin_End_days(sort_by_date(cluster)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n                        )\n\nmodel_input = Initial_Model(example, 'HEDESKOGA')\nexample\n</pre> cluster=example1.drop([2,3,4,5,10,11,12,13,14,15]) #cluster = example1   example = get_centroid(add_Begin_End_days(sort_by_date(cluster)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )                         )  model_input = Initial_Model(example, 'HEDESKOGA') example"},{"location":"PlagueProject/Number_infected_parishes/","title":"Number infected parishes","text":"In\u00a0[91]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[92]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[93]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[94]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)     In\u00a0[95]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Geographical data</p> In\u00a0[96]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] <p>Population data, number of deaths, and duration (information based on Bodil's appendix and Lennart's data)</p> In\u00a0[97]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsouthScania = gpd.GeoDataFrame(southScania, geometry='geometry')\nsouthScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'\n                           , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'\n                           ]]\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southScania = gpd.GeoDataFrame(southScania, geometry='geometry') southScania = southScania[['Region','ParishName', 'BEF1699', 'BeginPlaguePeriod'                            , 'EndPlaguePeriod', 'VictimsNumber', 'geometry'                            ]] In\u00a0[98]: Copied! <pre># Filtering the data to get only the infected parishes\n# Checking that the beginplague period is not NaN\nsoutheastScania = southScania[southScania[\"BeginPlaguePeriod\"].notna()]\n# Sorting the values by the beginplague period\n#southeastScania = southeastScania.sort_values(by=['BeginPlaguePeriod'])\nsoutheastScania.loc[0:20]\n</pre> # Filtering the data to get only the infected parishes # Checking that the beginplague period is not NaN southeastScania = southScania[southScania[\"BeginPlaguePeriod\"].notna()] # Sorting the values by the beginplague period #southeastScania = southeastScania.sort_values(by=['BeginPlaguePeriod']) southeastScania.loc[0:20]  Out[98]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry 2 SOUTHEAST ELJAR\u00d6D 320 JAN 1713 UNDEFINED 3 POLYGON ((4244692.078 3203779.021, 4244526.296... 7 SOUTHEAST S\u00d6DRA MELLBY 628 OCT 1711 NOV 1711 3 POLYGON ((4257537.206 3204129.443, 4257600.960... 9 SOUTHEAST BALDRINGE 235 AUG 1712 UNDEFINED ? POLYGON ((4233715.236 3189888.314, 4233917.612... 10 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 UNDEFINED ? POLYGON ((4228840.232 3178726.042, 4228969.528... 12 SOUTHEAST BROMMA 154 APR 1712 MAY 1712 ? POLYGON ((4231996.049 3179728.504, 4232042.002... 13 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... 16 SOUTHEAST S\u00d6VESTAD 559 OCT 1712 DEC 1712 ? POLYGON ((4228741.788 3181517.301, 4228687.928... 17 SOUTHEAST TRAN\u00c5S 339 DEC 1712 SEP 1713 127 POLYGON ((4245163.417 3193919.664, 4244977.993... 18 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... 19 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[99]: Copied! <pre>southScania = get_centroid(southScania)\ninfectedParishes = southScania[pd.notna(southScania[\"BeginPlaguePeriod\"])\n                       ]\nlen(infectedParishes)\ninfectedParishes\n</pre> southScania = get_centroid(southScania) infectedParishes = southScania[pd.notna(southScania[\"BeginPlaguePeriod\"])                        ] len(infectedParishes) infectedParishes Out[99]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry centroid 2 SOUTHEAST ELJAR\u00d6D 320 JAN 1713 UNDEFINED 3 POLYGON ((4244692.078 3203779.021, 4244526.296... POINT (4246105.744419839 3204718.059444113) 7 SOUTHEAST S\u00d6DRA MELLBY 628 OCT 1711 NOV 1711 3 POLYGON ((4257537.206 3204129.443, 4257600.960... POINT (4257271.750092611 3201592.5498128883) 9 SOUTHEAST BALDRINGE 235 AUG 1712 UNDEFINED ? POLYGON ((4233715.236 3189888.314, 4233917.612... POINT (4234524.376012389 3187670.6848119795) 10 SOUTHEAST BJ\u00c4RESJ\u00d6 376 JUL 1712 UNDEFINED ? POLYGON ((4228840.232 3178726.042, 4228969.528... POINT (4230600.699862956 3177291.2599278623) 12 SOUTHEAST BROMMA 154 APR 1712 MAY 1712 ? POLYGON ((4231996.049 3179728.504, 4232042.002... POINT (4234027.027249504 3179705.7627846766) ... ... ... ... ... ... ... ... ... 218 SOUTHWEST LILLA BEDDINGE 231 AUG 1712 SEP 1712 6 POLYGON ((4212038.606 3168945.381, 4212268.157... POINT (4210585.4892176185 3167891.3264763635) 222 SOUTHWEST SKURUP 547 JUL 1712 JUL 1712 ? POLYGON ((4214392.138 3182887.177, 4214576.093... POINT (4214330.468931566 3180670.917955833) 223 SOUTHWEST SLIMMINGE 550 JUN 1712 JUN 1712 ? POLYGON ((4214392.138 3182887.177, 4214363.468... POINT (4216637.909781777 3185001.1490382226) 225 SOUTHWEST SVENSTORP 126 AUG 1712 MAR 1713 24 POLYGON ((4215749.098 3175950.389, 4215849.551... POINT (4214416.796297111 3175926.1005712086) 234 SOUTHWEST \u00d6STRA VEMMENH\u00d6G 531 AUG 1712 AUG 1712 66 POLYGON ((4216860.481 3175184.843, 4216980.245... POINT (4216244.96882687 3171789.6451484654) <p>88 rows \u00d7 8 columns</p> In\u00a0[100]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe\ninfectedParishes = infectedParishes.replace(['UNDEFINED', '?'], np.nan)\ninfectedParishes['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    infectedParishes['BeginPlaguePeriod'], format='%b %Y')\ninfectedParishes['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    infectedParishes['EndPlaguePeriod'], format='%b %Y') + pd.offsets.MonthEnd(1)\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our Geodataframe infectedParishes = infectedParishes.replace(['UNDEFINED', '?'], np.nan) infectedParishes['new_format_BeginPlaguePeriod'] = pd.to_datetime(     infectedParishes['BeginPlaguePeriod'], format='%b %Y') infectedParishes['new_format_EndPlaguePeriod'] = pd.to_datetime(     infectedParishes['EndPlaguePeriod'], format='%b %Y') + pd.offsets.MonthEnd(1) In\u00a0[101]: Copied! <pre>#infectedParishes.to_csv('infectedSouthParishes.csv', index=False)\n</pre> #infectedParishes.to_csv('infectedSouthParishes.csv', index=False) In\u00a0[102]: Copied! <pre>from pandas.tseries.offsets import DateOffset, MonthEnd\n\ndef count_infected_by_month(df\n                            , date\n                            , n\n                            , column_name: str = 'ParishName'\n                            , start_date: str = 'BeginPlaguePeriod'\n                            , end_date: str = 'EndPlaguePeriod'):\n    # Create a copy of the dataframe\n    df_copy = df.copy()\n\n    # Convert your date columns to datetime format\n    df_copy[start_date] = pd.to_datetime(df_copy[start_date], format='%b %Y')\n    df_copy[end_date] = pd.to_datetime(df_copy[end_date], format='%b %Y', errors='coerce')\n\n    # Replace NaT with corresponding date in start_date column plus n months\n    df_copy[end_date] = df_copy[end_date].fillna(df_copy[start_date] + DateOffset(months=n))\n\n    # Convert your date to datetime format\n    date = pd.to_datetime(date, format='%b %Y')\n\n    # Add the converted date to a new column in df\n    df_copy['ConvertedDate'] = date\n\n    # Define the range of dates\n    dates = pd.date_range(start=date, end=df_copy[end_date].max(), freq='MS')\n\n    # Create a unique identifier combining Parish and date ranges\n    df_copy['UniqueID'] = df_copy[column_name].astype(str) + '_' + df_copy[start_date].astype(str) + '_' + df_copy[end_date].astype(str)\n\n    # Create a dataframe to store the results\n    results = pd.DataFrame({'date': dates\n                            , 'DaysFromInitialDate': (dates - df_copy[start_date].min()).days\n                            , 'NumberInfectedParishes': 0\n                            , 'CumInfectParishes': 0\n                            , 'EndOfMonth': (dates + MonthEnd(1))\n                            })\n\n    # Initialize an empty list to store the sets of infected parishes\n    infected_parishes = []\n\n    # Iterate over the dates\n    for date in dates:\n        # Count nodes where infection start date is before or on the given date \n        # and either there is no end date or the end date is after the given date\n        infected_nodes = df_copy[(df_copy[start_date] &lt;= date) &amp; (df_copy[end_date] &gt;= date)]\n        \n        # Store the results\n        results.loc[results['date'] == date, 'NumberInfectedParishes'] = infected_nodes['UniqueID'].nunique()  # Count only unique instances\n\n        # Add the set of infected parishes to the list\n        infected_parishes.append(set(infected_nodes[column_name]))\n\n    # Add a new column to count the days from the initial date to the end of the month\n    results['DaysToEndOfMonth'] = (results['EndOfMonth'] - df_copy[start_date].min()).dt.days\n\n    # Add a new column with the sets of infected parishes\n    results['InfectedParishes'] = infected_parishes  \n\n    # Calculate the cumulative number of infected parishes by month using the sets\n    CumInfectParishes = np.zeros(len(dates), dtype=int)\n    \n    if len(infected_parishes[0]) &gt; 0:\n        CumInfectParishes[0] = len(infected_parishes[0])\n        # Defining a variable to store the union of the infected parishes\n        union_infected_parishes = set(infected_parishes[0])  \n    else:\n        union_infected_parishes = set()\n\n    for i in range(1, len(infected_parishes)): \n        if len(infected_parishes[i]) &gt; 0: \n            new_infections = infected_parishes[i].difference(union_infected_parishes)\n            CumInfectParishes[i] = CumInfectParishes[i-1] + len(new_infections)\n            # Update the union of infected parishes\n            union_infected_parishes.update(new_infections)\n        else:\n            CumInfectParishes[i] = CumInfectParishes[i-1]\n\n    # Add a new column with the cumulative number of infected parishes\n    results['CumInfectParishes'] = CumInfectParishes    \n\n    # Counting the number of victims per month\n   \n    return results\n</pre> from pandas.tseries.offsets import DateOffset, MonthEnd  def count_infected_by_month(df                             , date                             , n                             , column_name: str = 'ParishName'                             , start_date: str = 'BeginPlaguePeriod'                             , end_date: str = 'EndPlaguePeriod'):     # Create a copy of the dataframe     df_copy = df.copy()      # Convert your date columns to datetime format     df_copy[start_date] = pd.to_datetime(df_copy[start_date], format='%b %Y')     df_copy[end_date] = pd.to_datetime(df_copy[end_date], format='%b %Y', errors='coerce')      # Replace NaT with corresponding date in start_date column plus n months     df_copy[end_date] = df_copy[end_date].fillna(df_copy[start_date] + DateOffset(months=n))      # Convert your date to datetime format     date = pd.to_datetime(date, format='%b %Y')      # Add the converted date to a new column in df     df_copy['ConvertedDate'] = date      # Define the range of dates     dates = pd.date_range(start=date, end=df_copy[end_date].max(), freq='MS')      # Create a unique identifier combining Parish and date ranges     df_copy['UniqueID'] = df_copy[column_name].astype(str) + '_' + df_copy[start_date].astype(str) + '_' + df_copy[end_date].astype(str)      # Create a dataframe to store the results     results = pd.DataFrame({'date': dates                             , 'DaysFromInitialDate': (dates - df_copy[start_date].min()).days                             , 'NumberInfectedParishes': 0                             , 'CumInfectParishes': 0                             , 'EndOfMonth': (dates + MonthEnd(1))                             })      # Initialize an empty list to store the sets of infected parishes     infected_parishes = []      # Iterate over the dates     for date in dates:         # Count nodes where infection start date is before or on the given date          # and either there is no end date or the end date is after the given date         infected_nodes = df_copy[(df_copy[start_date] &lt;= date) &amp; (df_copy[end_date] &gt;= date)]                  # Store the results         results.loc[results['date'] == date, 'NumberInfectedParishes'] = infected_nodes['UniqueID'].nunique()  # Count only unique instances          # Add the set of infected parishes to the list         infected_parishes.append(set(infected_nodes[column_name]))      # Add a new column to count the days from the initial date to the end of the month     results['DaysToEndOfMonth'] = (results['EndOfMonth'] - df_copy[start_date].min()).dt.days      # Add a new column with the sets of infected parishes     results['InfectedParishes'] = infected_parishes        # Calculate the cumulative number of infected parishes by month using the sets     CumInfectParishes = np.zeros(len(dates), dtype=int)          if len(infected_parishes[0]) &gt; 0:         CumInfectParishes[0] = len(infected_parishes[0])         # Defining a variable to store the union of the infected parishes         union_infected_parishes = set(infected_parishes[0])       else:         union_infected_parishes = set()      for i in range(1, len(infected_parishes)):          if len(infected_parishes[i]) &gt; 0:              new_infections = infected_parishes[i].difference(union_infected_parishes)             CumInfectParishes[i] = CumInfectParishes[i-1] + len(new_infections)             # Update the union of infected parishes             union_infected_parishes.update(new_infections)         else:             CumInfectParishes[i] = CumInfectParishes[i-1]      # Add a new column with the cumulative number of infected parishes     results['CumInfectParishes'] = CumInfectParishes          # Counting the number of victims per month         return results In\u00a0[103]: Copied! <pre># Defining a function to count the number of victims per month\ndef count_victims_by_month(gdf: gpd.GeoDataFrame \n                           , column_name: str = 'ParishName'\n                           , begin_date: str = 'BeginPlaguePeriod'\n                           , victims_column: str = 'VictimsNumber'\n                           , end_date: str = 'EndPlaguePeriod'\n                           , pop_size: str = 'BEF1699'):\n    # Create a copy of the dataframe\n    gdf_copy = gdf.copy()\n\n    # Filter the dataframe to get only the infected parishes\n    gdf_copy = gdf_copy[gdf_copy[begin_date].notnull()]\n\n    # Add a new column with the converted date to iterate over\n    gdf_copy['new_format_BeginPlaguePeriod'] = pd.to_datetime(gdf_copy[begin_date], format='%b %Y', errors='coerce')\n    gdf_copy['new_format_EndPlaguePeriod'] = pd.to_datetime(gdf_copy[end_date], format='%b %Y', errors='coerce') + pd.offsets.MonthEnd(1)\n\n    # sort df by date and add a column with the number of days corresponding to the beginning and end of the month\n    gdf_copy = add_Begin_End_days(sort_by_date(gdf_copy)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n   \n    # Fix the type of the victims number column to integer \n    gdf_copy[victims_column] = pd.to_numeric(gdf_copy[victims_column], errors='coerce')\n    # Now replace np.nan with a default value (like 0) if you want\n    gdf_copy[victims_column].fillna(0, inplace=True)\n    # Finally, convert the column to integer\n    gdf_copy[victims_column] = gdf_copy[victims_column].astype(int)\n\n    # Get the gdf sorted by the end of the plague period\n    gdf_copy = gdf_copy.sort_values('new_format_EndPlaguePeriod')\n\n    # Get the unique dates \n    months = gdf_copy['new_format_EndPlaguePeriod'].unique()\n        \n    # Create a dataframe to store the results\n    results = pd.DataFrame({ 'EndMonth': months\n                            , 'CumDays' : gdf_copy['EndDaysPlague'].unique()\n                            , 'NumberDeaths': 0\n                            , 'CumDeaths': 0\n                            , 'CumPop': 0\n                            , 'Parishes': \"\"\n                            })\n    # Iterate over the dates\n    total_deaths = 0\n        \n    for i, date in enumerate(months):\n        if pd.notna(date):\n            # fill the dataframe \"results\" so in the correspondant row the\n            # number of deaths is added to the column number deaths\n            numberOfDeaths = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] == date, victims_column].sum()\n            parishes = ','.join(gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] &lt;= date, column_name])\n            results.loc[results['EndMonth'] == date, 'Parishes'] = parishes\n            results.loc[results['EndMonth'] == date, 'NumberDeaths'] = numberOfDeaths\n            total_deaths += numberOfDeaths\n            results.loc[results['EndMonth'] == date, 'CumDeaths'] = total_deaths\n\n            results.loc[results['EndMonth'] == date, 'CumPop'] = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod']\n                                                                               &lt;= date, pop_size].sum()\n            \n    return results\n</pre> # Defining a function to count the number of victims per month def count_victims_by_month(gdf: gpd.GeoDataFrame                             , column_name: str = 'ParishName'                            , begin_date: str = 'BeginPlaguePeriod'                            , victims_column: str = 'VictimsNumber'                            , end_date: str = 'EndPlaguePeriod'                            , pop_size: str = 'BEF1699'):     # Create a copy of the dataframe     gdf_copy = gdf.copy()      # Filter the dataframe to get only the infected parishes     gdf_copy = gdf_copy[gdf_copy[begin_date].notnull()]      # Add a new column with the converted date to iterate over     gdf_copy['new_format_BeginPlaguePeriod'] = pd.to_datetime(gdf_copy[begin_date], format='%b %Y', errors='coerce')     gdf_copy['new_format_EndPlaguePeriod'] = pd.to_datetime(gdf_copy[end_date], format='%b %Y', errors='coerce') + pd.offsets.MonthEnd(1)      # sort df by date and add a column with the number of days corresponding to the beginning and end of the month     gdf_copy = add_Begin_End_days(sort_by_date(gdf_copy)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )         # Fix the type of the victims number column to integer      gdf_copy[victims_column] = pd.to_numeric(gdf_copy[victims_column], errors='coerce')     # Now replace np.nan with a default value (like 0) if you want     gdf_copy[victims_column].fillna(0, inplace=True)     # Finally, convert the column to integer     gdf_copy[victims_column] = gdf_copy[victims_column].astype(int)      # Get the gdf sorted by the end of the plague period     gdf_copy = gdf_copy.sort_values('new_format_EndPlaguePeriod')      # Get the unique dates      months = gdf_copy['new_format_EndPlaguePeriod'].unique()              # Create a dataframe to store the results     results = pd.DataFrame({ 'EndMonth': months                             , 'CumDays' : gdf_copy['EndDaysPlague'].unique()                             , 'NumberDeaths': 0                             , 'CumDeaths': 0                             , 'CumPop': 0                             , 'Parishes': \"\"                             })     # Iterate over the dates     total_deaths = 0              for i, date in enumerate(months):         if pd.notna(date):             # fill the dataframe \"results\" so in the correspondant row the             # number of deaths is added to the column number deaths             numberOfDeaths = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] == date, victims_column].sum()             parishes = ','.join(gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] &lt;= date, column_name])             results.loc[results['EndMonth'] == date, 'Parishes'] = parishes             results.loc[results['EndMonth'] == date, 'NumberDeaths'] = numberOfDeaths             total_deaths += numberOfDeaths             results.loc[results['EndMonth'] == date, 'CumDeaths'] = total_deaths              results.loc[results['EndMonth'] == date, 'CumPop'] = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod']                                                                                &lt;= date, pop_size].sum()                  return results  In\u00a0[104]: Copied! <pre># Plot the number of infected parishes per month \ndef plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):\n    results = count_infected_by_month(df, date, n, column_name, start_date, end_date)\n    plt.plot(results['date'], results['NumberInfectedParishes'],\n              label='Number of infected parishes', color='blue')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of infected parishes')\n    plt.title('South Scania')\n    plt.show()\n</pre> # Plot the number of infected parishes per month  def plot_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):     results = count_infected_by_month(df, date, n, column_name, start_date, end_date)     plt.plot(results['date'], results['NumberInfectedParishes'],               label='Number of infected parishes', color='blue')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of infected parishes')     plt.title('South Scania')     plt.show() In\u00a0[105]: Copied! <pre># Plot the cumulative number of parishes per month \ndef plot_cum_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):\n    results = count_infected_by_month(df, date, n, column_name, start_date, end_date)\n    plt.plot(results['date'], results['CumInfectParishes'], \n             label='Cumulative number of infected parishes', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Cumulative number of infected parishes')\n    plt.title('South Scania')\n    plt.show()\n</pre> # Plot the cumulative number of parishes per month  def plot_cum_parishes_by_month(df, date, n, column_name: str = 'ParishName', start_date: str = 'BeginPlaguePeriod', end_date: str = 'EndPlaguePeriod'):     results = count_infected_by_month(df, date, n, column_name, start_date, end_date)     plt.plot(results['date'], results['CumInfectParishes'],               label='Cumulative number of infected parishes', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Cumulative number of infected parishes')     plt.title('South Scania')     plt.show() In\u00a0[106]: Copied! <pre># Plot the cumulative number of deaths per month \ndef plot_cum_deaths_by_month(df):\n    results = count_victims_by_month(df)\n    plt.plot(results['EndMonth'], results['CumDeaths'],\n              label='Cumulative number of deaths', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Cum. number of deaths')\n    plt.title('South Scania')\n    plt.show()\n\n# Plot the number of deaths per month\ndef plot_deaths_by_month(df):\n    results = count_victims_by_month(df)\n    plt.plot(results['EndMonth'], results['NumberDeaths'],\n              label='Number of deaths', color='blue')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of deaths')\n    plt.title('South Scania')\n    plt.show()\n</pre> # Plot the cumulative number of deaths per month  def plot_cum_deaths_by_month(df):     results = count_victims_by_month(df)     plt.plot(results['EndMonth'], results['CumDeaths'],               label='Cumulative number of deaths', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Cum. number of deaths')     plt.title('South Scania')     plt.show()  # Plot the number of deaths per month def plot_deaths_by_month(df):     results = count_victims_by_month(df)     plt.plot(results['EndMonth'], results['NumberDeaths'],               label='Number of deaths', color='blue')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of deaths')     plt.title('South Scania')     plt.show()    <p>Example 1 considering only three parishes (Ystad, \u00d6ja, and Hedeskoga) with complete information</p> In\u00a0[107]: Copied! <pre>example1=infectedParishes[(infectedParishes['ParishName']=='YSTAD')\n                          |(infectedParishes['ParishName']=='\u00d6JA')\n                            |(infectedParishes['ParishName']=='HEDESKOGA')\n                            ]\n</pre> example1=infectedParishes[(infectedParishes['ParishName']=='YSTAD')                           |(infectedParishes['ParishName']=='\u00d6JA')                             |(infectedParishes['ParishName']=='HEDESKOGA')                             ]  In\u00a0[108]: Copied! <pre>add_Begin_End_days(sort_by_date(example1)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n</pre> add_Begin_End_days(sort_by_date(example1)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          ) Out[108]: Region ParishName BEF1699 BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry centroid new_format_BeginPlaguePeriod new_format_EndPlaguePeriod BeginDaysPlague EndDaysPlague 0 SOUTHEAST YSTAD 1782 JUN 1712 DEC 1712 740 POLYGON ((4240506.248 3176029.581, 4238053.574... POINT (4235880.810923543 3175774.141675906) 1712-06-01 1712-12-31 0 213 1 SOUTHEAST \u00d6JA 156 JUN 1712 MAR 1713 40 POLYGON ((4236218.454 3180039.080, 4236359.530... POINT (4236171.52874792 3178038.800015468) 1712-06-01 1713-03-31 0 303 2 SOUTHEAST HEDESKOGA 148 SEP 1712 OCT 1712 5 POLYGON ((4233181.208 3176354.140, 4233118.055... POINT (4233060.46303023 3177329.2552913846) 1712-09-01 1712-10-31 92 152 In\u00a0[109]: Copied! <pre>count_infected_by_month(example1, 'JUN 1712',0)\n</pre> count_infected_by_month(example1, 'JUN 1712',0) Out[109]: date DaysFromInitialDate NumberInfectedParishes CumInfectParishes EndOfMonth DaysToEndOfMonth InfectedParishes 0 1712-06-01 0 2 2 1712-06-30 29 {\u00d6JA, YSTAD} 1 1712-07-01 30 2 2 1712-07-31 60 {\u00d6JA, YSTAD} 2 1712-08-01 61 2 2 1712-08-31 91 {\u00d6JA, YSTAD} 3 1712-09-01 92 3 3 1712-09-30 121 {\u00d6JA, HEDESKOGA, YSTAD} 4 1712-10-01 122 3 3 1712-10-31 152 {\u00d6JA, HEDESKOGA, YSTAD} 5 1712-11-01 153 2 3 1712-11-30 182 {\u00d6JA, YSTAD} 6 1712-12-01 183 2 3 1712-12-31 213 {\u00d6JA, YSTAD} 7 1713-01-01 214 1 3 1713-01-31 244 {\u00d6JA} 8 1713-02-01 245 1 3 1713-02-28 272 {\u00d6JA} 9 1713-03-01 273 1 3 1713-03-31 303 {\u00d6JA} In\u00a0[110]: Copied! <pre>plot_parishes_by_month(example1, 'JUN 1712', 0)\n</pre> plot_parishes_by_month(example1, 'JUN 1712', 0) In\u00a0[111]: Copied! <pre>plot_cum_parishes_by_month(example1, 'JUN 1712', 0)\n</pre> plot_cum_parishes_by_month(example1, 'JUN 1712', 0) In\u00a0[112]: Copied! <pre>count_victims_by_month(example1)\n</pre> count_victims_by_month(example1) Out[112]: EndMonth CumDays NumberDeaths CumDeaths CumPop Parishes 0 1712-10-31 152 5 5 148 HEDESKOGA 1 1712-12-31 213 740 745 1930 HEDESKOGA,YSTAD 2 1713-03-31 303 40 785 2086 HEDESKOGA,YSTAD,\u00d6JA In\u00a0[113]: Copied! <pre>plot_cum_deaths_by_month(example1)\n</pre> plot_cum_deaths_by_month(example1) In\u00a0[114]: Copied! <pre>plot_deaths_by_month(example1)\n</pre> plot_deaths_by_month(example1) In\u00a0[115]: Copied! <pre>count_infected_by_month(infectedParishes, 'JAN 1711', 0)\n</pre> count_infected_by_month(infectedParishes, 'JAN 1711', 0) Out[115]: date DaysFromInitialDate NumberInfectedParishes CumInfectParishes EndOfMonth DaysToEndOfMonth InfectedParishes 0 1711-01-01 0 1 1 1711-01-31 30 {NORRA R\u00d6RUM} 1 1711-02-01 31 1 1 1711-02-28 58 {NORRA R\u00d6RUM} 2 1711-03-01 59 1 1 1711-03-31 89 {NORRA R\u00d6RUM} 3 1711-04-01 90 1 1 1711-04-30 119 {NORRA R\u00d6RUM} 4 1711-05-01 120 1 1 1711-05-31 150 {NORRA R\u00d6RUM} 5 1711-06-01 151 2 2 1711-06-30 180 {NORRA R\u00d6RUM, \u00d6VRABY} 6 1711-07-01 181 2 2 1711-07-31 211 {NORRA R\u00d6RUM, \u00d6VRABY} 7 1711-08-01 212 1 2 1711-08-31 242 {\u00d6VRABY} 8 1711-09-01 243 1 2 1711-09-30 272 {\u00d6VRABY} 9 1711-10-01 273 2 3 1711-10-31 303 {\u00d6VRABY, S\u00d6DRA MELLBY} 10 1711-11-01 304 1 3 1711-11-30 333 {S\u00d6DRA MELLBY} 11 1711-12-01 334 1 4 1711-12-31 364 {H\u00d6RBY} 12 1712-01-01 365 1 4 1712-01-31 395 {H\u00d6RBY} 13 1712-02-01 396 1 4 1712-02-29 424 {H\u00d6RBY} 14 1712-03-01 425 3 6 1712-03-31 455 {H\u00d6RBY, HOFTERUP, HURVA} 15 1712-04-01 456 3 8 1712-04-30 485 {BROMMA, HOFTERUP, BARSEB\u00c4CK} 16 1712-05-01 486 5 10 1712-05-31 516 {HOFTERUP, GR\u00d6NBY, GENARP, BROMMA, BARSEB\u00c4CK} 17 1712-06-01 517 11 17 1712-06-30 546 {\u00d6JA, HOFTERUP, GR\u00d6NBY, GENARP, MALM\u00d6 SANKT PE... 18 1712-07-01 547 20 28 1712-07-31 577 {S\u00d6DRA \u00c5SUM, H\u00d6RUP, \u00d6JA, GENARP, VALLEBERGA, L... 19 1712-08-01 578 40 52 1712-08-31 608 {\u00d6STRA VEMMENH\u00d6G, S\u00d6VDE, HAMMENH\u00d6G, S\u00d6DRA \u00c5SUM... 20 1712-09-01 609 37 64 1712-09-30 638 {SANKT PETERS KLOSTER, R\u00c4NG, S\u00d6DRA \u00c5SUM, SN\u00c5RE... 21 1712-10-01 639 39 72 1712-10-31 669 {SANKT PETERS KLOSTER, R\u00c4NG, ARRIE, BOSJ\u00d6KLOST... 22 1712-11-01 670 33 74 1712-11-30 699 {SANKT PETERS KLOSTER, R\u00c4NG, ARRIE, S\u00d6VESTAD, ... 23 1712-12-01 700 17 75 1712-12-31 730 {TRAN\u00c5S, \u00d6JA, TRYDE, ARRIE, MALM\u00d6 SANKT PETRI,... 24 1713-01-01 731 10 78 1713-01-31 761 {TRAN\u00c5S, \u00d6JA, ELJAR\u00d6D, MALM\u00d6 SANKT PETRI, SVEN... 25 1713-02-01 762 6 78 1713-02-28 789 {TRAN\u00c5S, \u00d6JA, SVENSTORP, SN\u00c5RESTAD, L\u00d6DERUP, S... 26 1713-03-01 790 6 79 1713-03-31 820 {TRAN\u00c5S, \u00d6JA, SVENSTORP, SN\u00c5RESTAD, L\u00d6DERUP, B... 27 1713-04-01 821 4 79 1713-04-30 850 {TRAN\u00c5S, BORRBY, SN\u00c5RESTAD, L\u00d6DERUP} 28 1713-05-01 851 5 80 1713-05-31 881 {TRAN\u00c5S, SN\u00c5RESTAD, L\u00d6DERUP, L\u00d6VESTAD, BORRBY} 29 1713-06-01 882 5 80 1713-06-30 911 {TRAN\u00c5S, SN\u00c5RESTAD, L\u00d6DERUP, L\u00d6VESTAD, BORRBY} 30 1713-07-01 912 7 81 1713-07-31 942 {TRAN\u00c5S, VANSTAD, SN\u00c5RESTAD, L\u00d6DERUP, R\u00d6DDINGE... 31 1713-08-01 943 7 81 1713-08-31 973 {TRAN\u00c5S, VANSTAD, SN\u00c5RESTAD, L\u00d6DERUP, R\u00d6DDINGE... 32 1713-09-01 974 8 82 1713-09-30 1003 {TRAN\u00c5S, VANSTAD, L\u00d6DERUP, R\u00d6DDINGE, L\u00d6VESTAD,... 33 1713-10-01 1004 8 83 1713-10-31 1034 {VANSTAD, L\u00d6DERUP, R\u00d6DDINGE, L\u00d6VESTAD, BORRBY,... 34 1713-11-01 1035 1 83 1713-11-30 1064 {L\u00d6VESTAD} 35 1713-12-01 1065 1 83 1713-12-31 1095 {L\u00d6VESTAD} 36 1714-01-01 1096 0 83 1714-01-31 1126 {} 37 1714-02-01 1127 0 83 1714-02-28 1154 {} 38 1714-03-01 1155 0 83 1714-03-31 1185 {} 39 1714-04-01 1186 0 83 1714-04-30 1215 {} 40 1714-05-01 1216 0 83 1714-05-31 1246 {} 41 1714-06-01 1247 0 83 1714-06-30 1276 {} 42 1714-07-01 1277 0 83 1714-07-31 1307 {} 43 1714-08-01 1308 0 83 1714-08-31 1338 {} 44 1714-09-01 1339 0 83 1714-09-30 1368 {} 45 1714-10-01 1369 0 83 1714-10-31 1399 {} 46 1714-11-01 1400 0 83 1714-11-30 1429 {} 47 1714-12-01 1430 0 83 1714-12-31 1460 {} 48 1715-01-01 1461 0 83 1715-01-31 1491 {} 49 1715-02-01 1492 1 83 1715-02-28 1519 {R\u00d6DDINGE} In\u00a0[116]: Copied! <pre>plot_parishes_by_month(infectedParishes, 'JAN 1711', 0)\n</pre> plot_parishes_by_month(infectedParishes, 'JAN 1711', 0) In\u00a0[117]: Copied! <pre>plot_cum_parishes_by_month(infectedParishes, 'JAN 1711', 0)\n</pre> plot_cum_parishes_by_month(infectedParishes, 'JAN 1711', 0) In\u00a0[118]: Copied! <pre>count_victims_by_month(infectedParishes)\n</pre> count_victims_by_month(infectedParishes) Out[118]: EndMonth CumDays NumberDeaths CumDeaths CumPop Parishes 0 1711-07-31 211 45 45 278 NORRA R\u00d6RUM 1 1711-10-31 303 60 105 486 NORRA R\u00d6RUM,\u00d6VRABY 2 1711-11-30 333 3 108 1114 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY 3 1712-03-31 455 76 184 1870 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY 4 1712-05-31 516 0 184 2024 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA 5 1712-06-30 546 3 187 2759 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 6 1712-07-31 577 0 187 3306 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 7 1712-08-31 608 314 501 7670 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 8 1712-09-30 638 173 674 9230 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 9 1712-10-31 669 92 766 11949 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 10 1712-11-30 699 247 1013 16121 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 11 1712-12-31 730 912 1925 21887 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 12 1713-01-31 761 416 2341 28168 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 13 1713-02-28 789 0 2341 28399 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 14 1713-03-31 820 64 2405 28681 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 15 1713-08-31 973 38 2443 28914 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 16 1713-09-30 1003 127 2570 29253 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 17 1713-10-31 1034 328 2898 32234 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 18 1713-12-31 1095 95 2993 32867 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 19 1715-02-28 1519 5 2998 33187 NORRA R\u00d6RUM,\u00d6VRABY,S\u00d6DRA MELLBY,H\u00d6RBY,BROMMA,G... 20 NaT 0 0 0 0 In\u00a0[119]: Copied! <pre>plot_cum_deaths_by_month(infectedParishes)\n</pre> plot_cum_deaths_by_month(infectedParishes) In\u00a0[120]: Copied! <pre>plot_deaths_by_month(infectedParishes)\n</pre> plot_deaths_by_month(infectedParishes) In\u00a0[121]: Copied! <pre># Read a csv file \nystad_group = pd.read_csv(os.path.join(data_private_folder, 'yellow_group.csv'), sep=',')\n</pre> # Read a csv file  ystad_group = pd.read_csv(os.path.join(data_private_folder, 'yellow_group.csv'), sep=',') In\u00a0[122]: Copied! <pre>count_infected_by_month(ystad_group, 'JUN 1712', 0)\n</pre> count_infected_by_month(ystad_group, 'JUN 1712', 0) Out[122]: date DaysFromInitialDate NumberInfectedParishes CumInfectParishes EndOfMonth DaysToEndOfMonth InfectedParishes 0 1712-06-01 0 2 2 1712-06-30 29 {\u00d6JA, YSTAD} 1 1712-07-01 30 6 6 1712-07-31 60 {\u00d6JA, VALLEBERGA, STORA K\u00d6PINGE, BJ\u00c4RESJ\u00d6, H\u00d6R... 2 1712-08-01 61 7 10 1712-08-31 91 {\u00d6JA, BROMMA, HAMMENH\u00d6G, STORA K\u00d6PINGE, INGELS... 3 1712-09-01 92 5 12 1712-09-30 121 {\u00d6JA, \u00d6VRABY, STORA K\u00d6PINGE, HEDESKOGA, YSTAD} 4 1712-10-01 122 4 12 1712-10-31 152 {\u00d6JA, STORA K\u00d6PINGE, HEDESKOGA, YSTAD} 5 1712-11-01 153 4 12 1712-11-30 182 {\u00d6JA, STORA K\u00d6PINGE, \u00d6VRABY, YSTAD} 6 1712-12-01 183 3 12 1712-12-31 213 {\u00d6JA, STORA K\u00d6PINGE, YSTAD} 7 1713-01-01 214 2 12 1713-01-31 244 {\u00d6JA, STORA K\u00d6PINGE} 8 1713-02-01 245 1 12 1713-02-28 272 {\u00d6JA} 9 1713-03-01 273 1 12 1713-03-31 303 {\u00d6JA} In\u00a0[123]: Copied! <pre>plot_cum_parishes_by_month(ystad_group, 'JUN 1712', 0)\n</pre> plot_cum_parishes_by_month(ystad_group, 'JUN 1712', 0) In\u00a0[124]: Copied! <pre>count_victims_by_month(ystad_group)\n</pre> count_victims_by_month(ystad_group) Out[124]: EndMonth CumDays NumberDeaths CumDeaths CumPop Parishes 0 1712-08-31 91 1 1 720 GLEMMINGE,INGELSTORP 1 1712-09-30 121 0 1 928 GLEMMINGE,INGELSTORP,\u00d6VRABY 2 1712-10-31 152 5 6 1076 GLEMMINGE,INGELSTORP,\u00d6VRABY,HEDESKOGA 3 1712-11-30 182 0 6 1284 GLEMMINGE,INGELSTORP,\u00d6VRABY,HEDESKOGA,\u00d6VRABY 4 1712-12-31 213 740 746 3066 GLEMMINGE,INGELSTORP,\u00d6VRABY,HEDESKOGA,\u00d6VRABY,Y... 5 1713-01-31 244 80 826 3442 GLEMMINGE,INGELSTORP,\u00d6VRABY,HEDESKOGA,\u00d6VRABY,Y... 6 1713-03-31 303 40 866 3598 GLEMMINGE,INGELSTORP,\u00d6VRABY,HEDESKOGA,\u00d6VRABY,Y... 7 NaT 0 0 0 0 In\u00a0[125]: Copied! <pre>plot_cum_deaths_by_month(ystad_group)\n</pre> plot_cum_deaths_by_month(ystad_group)"},{"location":"PlagueProject/ParameterEstimation/","title":"ParameterEstimation","text":"In\u00a0[69]: Copied! <pre>#Python 3.11.2\n#Import packages\nimport scipy.integrate as scipy\nimport scipy.optimize as optimize\nimport scipy.stats as stats\nimport pandas as pd\nimport numpy as np\nimport pylab as pl\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport json # for pretty printing\n\nimport shutil\nimport sys\nimport os.path\n</pre> #Python 3.11.2 #Import packages import scipy.integrate as scipy import scipy.optimize as optimize import scipy.stats as stats import pandas as pd import numpy as np import pylab as pl import random import matplotlib.pyplot as plt from collections import defaultdict import json # for pretty printing  import shutil import sys import os.path In\u00a0[70]: Copied! <pre>climate_data = np.array([[120., 4.835], [210., 15.405], [270., 8.315]\n                         , [375., -0.73], [480.,  5.08], [570., 14.945]\n                         , [630., 8.58], [735., 1.38], [840., 6.37]\n                         , [930., 16.015], [990., 8.065], [1095., 0.29]\n                         , [1200.,4.435], [1290., 15.885], [1350., 8.26]\n                         , [1455., 0.69], [1560., 2.43], [1650., 14.77]\n                         , [1710., 7.795], [1815., 0.56], [1920., 4.17]\n                         , [2010., 14.325], [2070., 8.11], [2175., 0.135]\n                         , [2280., 6.195], [2370., 14.495], [2430., 8.46]\n                         , [2535., -4.24], [2640., 4.335], [2730., 15.35], [2790., 7.1]\n                         ])\n</pre> climate_data = np.array([[120., 4.835], [210., 15.405], [270., 8.315]                          , [375., -0.73], [480.,  5.08], [570., 14.945]                          , [630., 8.58], [735., 1.38], [840., 6.37]                          , [930., 16.015], [990., 8.065], [1095., 0.29]                          , [1200.,4.435], [1290., 15.885], [1350., 8.26]                          , [1455., 0.69], [1560., 2.43], [1650., 14.77]                          , [1710., 7.795], [1815., 0.56], [1920., 4.17]                          , [2010., 14.325], [2070., 8.11], [2175., 0.135]                          , [2280., 6.195], [2370., 14.495], [2430., 8.46]                          , [2535., -4.24], [2640., 4.335], [2730., 15.35], [2790., 7.1]                          ]) In\u00a0[71]: Copied! <pre>plt.plot(climate_data[:,0], climate_data[:,1])\n</pre> plt.plot(climate_data[:,0], climate_data[:,1]) Out[71]: <pre>[&lt;matplotlib.lines.Line2D at 0x17d847790&gt;]</pre> <p>Select the model</p> In\u00a0[72]: Copied! <pre>def residuals(parameters, climate_data):\n    beta0, beta1, a = parameters\n    x = climate_data[:,0] # time\n    y = climate_data[:,1] # temperature\n    return y - (beta0 + beta1 * np.sin(2 * np.pi * x / 365.25 - a))\n</pre> def residuals(parameters, climate_data):     beta0, beta1, a = parameters     x = climate_data[:,0] # time     y = climate_data[:,1] # temperature     return y - (beta0 + beta1 * np.sin(2 * np.pi * x / 365.25 - a)) <p>Find a best fit</p> In\u00a0[73]: Copied! <pre>def sos(parameters, climate_data):\n    return np.sum(residuals(parameters, climate_data)**2)\n\ndef best_fit(fcn, climate_data, disp=1):\n    parameter_guess = np.array([6.91, -7.93, -8.07])\n    return optimize.fmin(fcn, parameter_guess, args=(climate_data,), disp = disp)\n\nparameter_fit = best_fit(sos, climate_data)\n\nfor name,value in zip(['beta0', 'beta1', 'a'], parameter_fit):\n    print(name, \" = \",  round(value,2))\n</pre> def sos(parameters, climate_data):     return np.sum(residuals(parameters, climate_data)**2)  def best_fit(fcn, climate_data, disp=1):     parameter_guess = np.array([6.91, -7.93, -8.07])     return optimize.fmin(fcn, parameter_guess, args=(climate_data,), disp = disp)  parameter_fit = best_fit(sos, climate_data)  for name,value in zip(['beta0', 'beta1', 'a'], parameter_fit):     print(name, \" = \",  round(value,2)) <pre>Optimization terminated successfully.\n         Current function value: 145.553338\n         Iterations: 62\n         Function evaluations: 117\nbeta0  =  5.89\nbeta1  =  -7.31\na  =  -7.68\n</pre> In\u00a0[74]: Copied! <pre>beta0  =  6.91\nbeta1  =  -7.93\na  =  -8.07\nt = np.linspace(0, 3000, 1000)\nplt.plot(t,(beta0 + beta1 * np.sin(2 * np.pi * t / 365.25 - a)))\nplt.scatter(climate_data[:,0], climate_data[:,1])\npl.show()\n</pre> beta0  =  6.91 beta1  =  -7.93 a  =  -8.07 t = np.linspace(0, 3000, 1000) plt.plot(t,(beta0 + beta1 * np.sin(2 * np.pi * t / 365.25 - a))) plt.scatter(climate_data[:,0], climate_data[:,1]) pl.show() <p>Testing the fitting process with a SIR model and synthetic data generated by Mathematica.</p> In\u00a0[83]: Copied! <pre># Define or import the data\ndata = np.array([1.19646, 1.43109, 1.71112, 2.04506, 2.44293, 2.91644, 3.47919, \n                 4.14694, 4.93776, 5.87222, 6.9734, 8.26688, 9.78045, 11.5436, \n                 13.5864, 15.9384, 18.6265, 21.672, 25.0875, 28.8727, 33.0102, \n                 37.4611, 42.1613, 47.0193, 51.917, 56.7128, 61.2487, 65.3608, \n                 68.8917, 71.7039, 73.6916, 74.7898, 74.9782, 74.2812, 72.7626, \n                 70.5173, 67.6618, 64.3231, 60.6301, 56.7058, 52.6616, 48.5941, \n                 44.5836, 40.6936, 36.9722, 33.4535, 30.1593, 27.1016, 24.284, 21.7042]\n                )\n# Define the model \ndef SIR_model(y, t, beta, gamma):\n    S, I, R = y\n    dS_dt = - beta * S * I / (S + I + R)\n    dI_dt = beta * S * I / (S + I + R) - gamma * I\n    dR_dt = gamma * I\n    return ([dS_dt, dI_dt, dR_dt])\n\n# Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters,  *args):\n    S0, I0, beta, gamma = parameters\n    y_obseved, time = args\n    solution = scipy.odeint(SIR_model, [S0, I0, 0], time, args =(beta,gamma))\n    y_predicted = solution[:,1] #only need the infected\n    error = np.sum((y_predicted - y_obseved)**2)\n    return error\n\n# Set up the data and initial conditions\nt = np.linspace(0, len(data)) # assume one time step per day\ny_obseved = data # the data we are trying to fit\n\n# Choose initial guesses for the parameters to fit\nS0_guess = 999\nI0_guess = 1\nbeta_guess = 0.05\ngamma_guess = 0.7\n\n# Minimize the objective function to obtain estimates for S0, I0, beta and gamma\nresult = optimize.minimize(objectiveFunction, [S0_guess, I0_guess, beta_guess, gamma_guess], args=(y_obseved,t), method='L-BFGS-B', bounds=[(0, 1000), (0, 1000), (0, 2), (0, 2)])\nS0_estimated, I0_estimated, beta_estimated, gamma_estimated = result.x\n\nprint(\"S0 = \", S0_estimated)\nprint(\"I0 = \", I0_estimated)\nprint(\"beta = \", beta_estimated)\nprint(\"gamma = \", gamma_estimated)\n\n\n#Plot the obseved data\nplt.plot(t, y_obseved, 'bo', label='Observed data')\n\n#Plot the predicted values from the SIR model using the estimated parameters\ny_predicted = scipy.odeint(SIR_model, [S0_estimated, I0_estimated,0], t, args =(beta_estimated,gamma_estimated))\nplt.plot(t, y_predicted[:,1], 'r-', label='Predicted data')\n\n#Add labels and legend\nplt.xlabel('Time (days)')\nplt.ylabel('Infected population')\nplt.legend()\n\n#Show the plot\nplt.show()\n</pre> # Define or import the data data = np.array([1.19646, 1.43109, 1.71112, 2.04506, 2.44293, 2.91644, 3.47919,                   4.14694, 4.93776, 5.87222, 6.9734, 8.26688, 9.78045, 11.5436,                   13.5864, 15.9384, 18.6265, 21.672, 25.0875, 28.8727, 33.0102,                   37.4611, 42.1613, 47.0193, 51.917, 56.7128, 61.2487, 65.3608,                   68.8917, 71.7039, 73.6916, 74.7898, 74.9782, 74.2812, 72.7626,                   70.5173, 67.6618, 64.3231, 60.6301, 56.7058, 52.6616, 48.5941,                   44.5836, 40.6936, 36.9722, 33.4535, 30.1593, 27.1016, 24.284, 21.7042]                 ) # Define the model  def SIR_model(y, t, beta, gamma):     S, I, R = y     dS_dt = - beta * S * I / (S + I + R)     dI_dt = beta * S * I / (S + I + R) - gamma * I     dR_dt = gamma * I     return ([dS_dt, dI_dt, dR_dt])  # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters,  *args):     S0, I0, beta, gamma = parameters     y_obseved, time = args     solution = scipy.odeint(SIR_model, [S0, I0, 0], time, args =(beta,gamma))     y_predicted = solution[:,1] #only need the infected     error = np.sum((y_predicted - y_obseved)**2)     return error  # Set up the data and initial conditions t = np.linspace(0, len(data)) # assume one time step per day y_obseved = data # the data we are trying to fit  # Choose initial guesses for the parameters to fit S0_guess = 999 I0_guess = 1 beta_guess = 0.05 gamma_guess = 0.7  # Minimize the objective function to obtain estimates for S0, I0, beta and gamma result = optimize.minimize(objectiveFunction, [S0_guess, I0_guess, beta_guess, gamma_guess], args=(y_obseved,t), method='L-BFGS-B', bounds=[(0, 1000), (0, 1000), (0, 2), (0, 2)]) S0_estimated, I0_estimated, beta_estimated, gamma_estimated = result.x  print(\"S0 = \", S0_estimated) print(\"I0 = \", I0_estimated) print(\"beta = \", beta_estimated) print(\"gamma = \", gamma_estimated)   #Plot the obseved data plt.plot(t, y_obseved, 'bo', label='Observed data')  #Plot the predicted values from the SIR model using the estimated parameters y_predicted = scipy.odeint(SIR_model, [S0_estimated, I0_estimated,0], t, args =(beta_estimated,gamma_estimated)) plt.plot(t, y_predicted[:,1], 'r-', label='Predicted data')  #Add labels and legend plt.xlabel('Time (days)') plt.ylabel('Infected population') plt.legend()  #Show the plot plt.show() <pre>S0 =  999.0\nI0 =  2.080568554492916\nbeta =  0.6518262889244687\ngamma =  0.4839597935855828\n</pre>"},{"location":"PlagueProject/ParameterEstimationParish/","title":"ParameterEstimationParish","text":"In\u00a0[175]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[176]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[177]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[178]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[179]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[180]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] In\u00a0[181]: Copied! <pre># Set the working directory for private files\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n</pre> # Set the working directory for private files # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) <p>Set the working directory for private files with monthly data for some parishes</p> In\u00a0[182]: Copied! <pre># Southeast Scania\nsoutheast_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southeast')\n# Middle Scania\nmiddle_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Middle')\n# Southwest Scania\nsouthwest_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southwest')\n</pre> # Southeast Scania southeast_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southeast') # Middle Scania middle_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Middle') # Southwest Scania southwest_parishes_folder = os.path.join(data_private_folder, 'Deaths_parish', 'Plague', 'Southwest') <p>Function to call the data by parish and transform the date to an appropiate format</p> In\u00a0[183]: Copied! <pre>def get_parish_data(parish_name, parish_folder):\n    parish_path = os.path.join(parish_folder, parish_name + '.xlsx')\n    parish = pd.read_excel(parish_path, sheet_name='Plague')\n\n    # Convert 'EndDate' to datetime with appropriate format\n    parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%b %Y')\n    parish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')\n    parish['first_day'] = parish['NewEndDate'].dt.to_timestamp()\n    parish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')\n\n    # Add a column with the days since the first date and then cumsum\n    parish['Days'] = parish['last_day'].dt.daysinmonth\n    parish['Days'] = parish['Days'].cumsum()\n    return parish\n</pre> def get_parish_data(parish_name, parish_folder):     parish_path = os.path.join(parish_folder, parish_name + '.xlsx')     parish = pd.read_excel(parish_path, sheet_name='Plague')      # Convert 'EndDate' to datetime with appropriate format     parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%b %Y')     parish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')     parish['first_day'] = parish['NewEndDate'].dt.to_timestamp()     parish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')      # Add a column with the days since the first date and then cumsum     parish['Days'] = parish['last_day'].dt.daysinmonth     parish['Days'] = parish['Days'].cumsum()     return parish In\u00a0[184]: Copied! <pre># parish_path = os.path.join(southeast_parishes_folder, 'Ystad' + '.xlsx')\n# parish = pd.read_excel(parish_path, sheet_name='Plague')\n\n# # Fix the date format for Ystad (special case for the type of information)\n# parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%d/%m/%Y')\n# # Add a column with the days since the first date\n# parish['Days'] = (parish['NewEndDate'] - parish['NewEndDate'].min()).dt.days\n\n# # # Fix the date format\n# parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%d/%m/%Y')\n# # #Add a column with the days since the first date\n# parish['Days'] = (parish['NewEndDate'] - parish['NewEndDate'].min()).dt.days\n# parish\n</pre> # parish_path = os.path.join(southeast_parishes_folder, 'Ystad' + '.xlsx') # parish = pd.read_excel(parish_path, sheet_name='Plague')  # # Fix the date format for Ystad (special case for the type of information) # parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%d/%m/%Y') # # Add a column with the days since the first date # parish['Days'] = (parish['NewEndDate'] - parish['NewEndDate'].min()).dt.days  # # # Fix the date format # parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%d/%m/%Y') # # #Add a column with the days since the first date # parish['Days'] = (parish['NewEndDate'] - parish['NewEndDate'].min()).dt.days # parish <p>Initializing the population size (pop_parish) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[185]: Copied! <pre>def get_parish_info(parish_name, df: pd.DataFrame, column_name='ParishName', column_pop='BEF1699'):\n    pop_df = df[(df[column_name] == parish_name)][column_pop]\n    name_df = df[(df[column_name] == parish_name)][column_name]\n    \n    if not pop_df.empty and not name_df.empty:\n        pop_parish = pop_df.values[0]\n        name_parish = name_df.values[0]\n    else:\n        pop_parish = None\n        name_parish = None\n\n    return pop_parish, name_parish\n</pre> def get_parish_info(parish_name, df: pd.DataFrame, column_name='ParishName', column_pop='BEF1699'):     pop_df = df[(df[column_name] == parish_name)][column_pop]     name_df = df[(df[column_name] == parish_name)][column_name]          if not pop_df.empty and not name_df.empty:         pop_parish = pop_df.values[0]         name_parish = name_df.values[0]     else:         pop_parish = None         name_parish = None      return pop_parish, name_parish In\u00a0[186]: Copied! <pre>get_parish_info('YSTAD', southScania)[0]\n</pre> get_parish_info('YSTAD', southScania)[0] Out[186]: <pre>1782</pre> In\u00a0[187]: Copied! <pre>class Initial_Model:\n    def __init__(self, name_parish, df1, df2:pd.DataFrame = southScania):\n        self.df = df1\n        self.parish_name = name_parish\n        self.pop_parish = get_parish_info(name_parish, df2)[0]\n        self.E0 = 0.0\n        self.I0 = 1.0\n        self.R0 = 0.0\n        self.D0 = 0.0\n        self.S0 = self.pop_parish - self.E0 - self.I0 - self.R0 - self.D0\n        \n           \n    def maxDays(self, column_EndDays: str = 'Days'):\n        return self.df[column_EndDays].max()+1\n</pre> class Initial_Model:     def __init__(self, name_parish, df1, df2:pd.DataFrame = southScania):         self.df = df1         self.parish_name = name_parish         self.pop_parish = get_parish_info(name_parish, df2)[0]         self.E0 = 0.0         self.I0 = 1.0         self.R0 = 0.0         self.D0 = 0.0         self.S0 = self.pop_parish - self.E0 - self.I0 - self.R0 - self.D0                          def maxDays(self, column_EndDays: str = 'Days'):         return self.df[column_EndDays].max()+1             In\u00a0[188]: Copied! <pre>Initial_Model('YSTAD', get_parish_data('YSTAD', southeast_parishes_folder)).S0\n</pre> Initial_Model('YSTAD', get_parish_data('YSTAD', southeast_parishes_folder)).S0 Out[188]: <pre>1781.0</pre> <p>Generating the differential equations</p> In\u00a0[189]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[190]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    df = parameters['df']\n    beta = parameters['beta']\n    gamma = parameters['gamma']\n    sigma = parameters['sigma']\n    mu = parameters['mu']\n    N = parameters['N']\n               \n    # Create a vector of variables\n    S, E, I, R, D = y\n   \n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n        \n   \n    dS = - (beta + seasonal_rate(t)) * S * (I / N)\n    dE = (beta + seasonal_rate(t))  * S * (I / N)  - sigma * E\n    dI = sigma * E - gamma * I\n    dR = gamma * (1 - mu) * I\n    dD = (gamma * mu) * I\n\n    derivatives = [dS, dE, dI, dR, dD]\n\n    return derivatives\n \n   \ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    initConditions = [init['S'], init['E'], init['I'], init['R'], init['D']]\n    \n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n    output = scipy.odeint(func=model, y0=initConditions, t = t, args=((genInput,),), full_output=1)\n    # output is a tuple with two elements, the first element is the solution\n    # array and the second element is an object with additional information\n    solution = output[0]  \n    \n    # Get the solution for each variable\n    S = solution[:, 0]\n    E = solution[:, 1]\n    I = solution[:, 2]\n    R = solution[:, 3]\n    D = solution[:, 4]\n        \n    return {'S': S, 'E': E, 'I': I, 'R': R, 'D': D}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     df = parameters['df']     beta = parameters['beta']     gamma = parameters['gamma']     sigma = parameters['sigma']     mu = parameters['mu']     N = parameters['N']                     # Create a vector of variables     S, E, I, R, D = y         # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0                  dS = - (beta + seasonal_rate(t)) * S * (I / N)     dE = (beta + seasonal_rate(t))  * S * (I / N)  - sigma * E     dI = sigma * E - gamma * I     dR = gamma * (1 - mu) * I     dD = (gamma * mu) * I      derivatives = [dS, dE, dI, dR, dD]      return derivatives       def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     initConditions = [init['S'], init['E'], init['I'], init['R'], init['D']]          T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']     output = scipy.odeint(func=model, y0=initConditions, t = t, args=((genInput,),), full_output=1)     # output is a tuple with two elements, the first element is the solution     # array and the second element is an object with additional information     solution = output[0]            # Get the solution for each variable     S = solution[:, 0]     E = solution[:, 1]     I = solution[:, 2]     R = solution[:, 3]     D = solution[:, 4]              return {'S': S, 'E': E, 'I': I, 'R': R, 'D': D} In\u00a0[191]: Copied! <pre># Computing the daily deaths from the model output at the observed times\ndef daily_deaths(solution_model: dict, observedTIme: list, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = [solution_model['D'][t] for t in observedTIme]  # list of floats\n    return ([cumulative_deaths[0]]+[cumulative_deaths[i] - cumulative_deaths[i-1]\n            for i in range(1, len(cumulative_deaths))])\n</pre>  # Computing the daily deaths from the model output at the observed times def daily_deaths(solution_model: dict, observedTIme: list, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = [solution_model['D'][t] for t in observedTIme]  # list of floats     return ([cumulative_deaths[0]]+[cumulative_deaths[i] - cumulative_deaths[i-1]             for i in range(1, len(cumulative_deaths))]) In\u00a0[192]: Copied! <pre>parish_file = get_parish_data('Ystad', southeast_parishes_folder)\nparish_file\n</pre> parish_file = get_parish_data('Ystad', southeast_parishes_folder) parish_file Out[192]: ParishName EndDate Deaths CumDeaths BEF1699 NewEndDate first_day last_day Days 0 YSTAD Jun 1712 26 26 1782 1712-06 1712-06-01 1712-06-30 23:59:59.999999999 30 1 YSTAD Jul 1712 80 106 1782 1712-07 1712-07-01 1712-07-31 23:59:59.999999999 61 2 YSTAD Aug 1712 303 409 1782 1712-08 1712-08-01 1712-08-31 23:59:59.999999999 92 3 YSTAD Sep 1712 202 611 1782 1712-09 1712-09-01 1712-09-30 23:59:59.999999999 122 4 YSTAD Oct 1712 84 695 1782 1712-10 1712-10-01 1712-10-31 23:59:59.999999999 153 5 YSTAD Nov 1712 35 730 1782 1712-11 1712-11-01 1712-11-30 23:59:59.999999999 183 6 YSTAD Dec 1712 5 735 1782 1712-12 1712-12-01 1712-12-31 23:59:59.999999999 214 In\u00a0[193]: Copied! <pre>model_input = Initial_Model('YSTAD', get_parish_data('YSTAD', southeast_parishes_folder))\n</pre> model_input = Initial_Model('YSTAD', get_parish_data('YSTAD', southeast_parishes_folder)) In\u00a0[194]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters):\n    beta, mu = parameters    \n    # Define the model parameters\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'E': model_input.E0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'df': parish_file,  # defining the dataframe to work with\n                  # defining the initial values for the model\n                  'beta': beta,\n                  'gamma': 0.4, # 1/5, 5 days to recover\n                  'sigma': 0.17, # 1/3, 3 days to show symptoms\n                  'mu': mu,\n                  #'bump_center': 0.0,\n                #   'bump_width': 0.0,\n                #   'bump_height': 0.0,\n                  'N': model_input.pop_parish,\n                  'T': model_input.maxDays(),\n                  }\n    model_sol = generate_sol(model_info)\n    #daily_deaths_model = daily_deaths(model_sol, observedTime, 0, model_input.maxDays())\n    observedTime = parish_file['Days'].values\n    cumDeathData = parish_file['CumDeaths'].values\n    totalError = 0\n    n = len(observedTime)\n    \n    for i in range(n):\n        position = observedTime[i]\n        error_i = (model_sol['D'][position] - cumDeathData[i])**2\n        totalError += error_i\n    return totalError\n</pre> # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters):     beta, mu = parameters         # Define the model parameters     model_info = {'model': SEIRD_model,                   'init': {                       'S': model_input.S0,                       'E': model_input.E0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'df': parish_file,  # defining the dataframe to work with                   # defining the initial values for the model                   'beta': beta,                   'gamma': 0.4, # 1/5, 5 days to recover                   'sigma': 0.17, # 1/3, 3 days to show symptoms                   'mu': mu,                   #'bump_center': 0.0,                 #   'bump_width': 0.0,                 #   'bump_height': 0.0,                   'N': model_input.pop_parish,                   'T': model_input.maxDays(),                   }     model_sol = generate_sol(model_info)     #daily_deaths_model = daily_deaths(model_sol, observedTime, 0, model_input.maxDays())     observedTime = parish_file['Days'].values     cumDeathData = parish_file['CumDeaths'].values     totalError = 0     n = len(observedTime)          for i in range(n):         position = observedTime[i]         error_i = (model_sol['D'][position] - cumDeathData[i])**2         totalError += error_i     return totalError <p>Defining the Bayesian optimization problem:</p> In\u00a0[195]: Copied! <pre>bounds=[(0.0, 1.0) # beta\n        , (0.2, 0.8) # mu\n        ]\n# Set up the data to fit\nobservedTime = parish_file['Days'].values\ndeathData = parish_file['Deaths'].values\ncumDeathData = parish_file['CumDeaths'].values\n\nr = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)\n\n\nbeta_estimated, mu_estimated  = r.x\nobj_value = r.fun\n\nprint(\"beta = \", beta_estimated)\nprint(\"mu = \", mu_estimated)\nprint(\"Objective value = \", obj_value)\n</pre> bounds=[(0.0, 1.0) # beta         , (0.2, 0.8) # mu         ] # Set up the data to fit observedTime = parish_file['Days'].values deathData = parish_file['Deaths'].values cumDeathData = parish_file['CumDeaths'].values  r = gp_minimize(objectiveFunction, bounds, n_calls=100, random_state=0)   beta_estimated, mu_estimated  = r.x obj_value = r.fun  print(\"beta = \", beta_estimated) print(\"mu = \", mu_estimated) print(\"Objective value = \", obj_value)  <pre>beta =  0.6823338902831212\nmu =  0.5798895496334534\nObjective value =  2765.561956818971\n</pre> <p>Substituting the estimated values into the model and solving it</p> In\u00a0[196]: Copied! <pre>model_estimation = {'model': SEIRD_model,\n                    'init': {\n                        'S': model_input.S0,\n                        'E': model_input.E0,\n                        'I': model_input.I0,\n                        'R': model_input.R0,\n                        'D': model_input.D0,\n                    },\n                    'df': parish_file,  # defining the dataframe to work with\n                    # defining the initial values for the model\n                    'beta': beta_estimated,\n                    'gamma': 0.4, #  1.7 days to recover\n                    'sigma': 0.17, # 1/3, 3 days to show symptoms\n                    'mu': mu_estimated ,\n                    'N': model_input.pop_parish,\n                    'T': model_input.maxDays()+20}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SEIRD_model,                     'init': {                         'S': model_input.S0,                         'E': model_input.E0,                         'I': model_input.I0,                         'R': model_input.R0,                         'D': model_input.D0,                     },                     'df': parish_file,  # defining the dataframe to work with                     # defining the initial values for the model                     'beta': beta_estimated,                     'gamma': 0.4, #  1.7 days to recover                     'sigma': 0.17, # 1/3, 3 days to show symptoms                     'mu': mu_estimated ,                     'N': model_input.pop_parish,                     'T': model_input.maxDays()+20} model_solution = generate_sol(model_estimation) <p>Plotting the cummulative number of deaths</p> In\u00a0[197]: Copied! <pre>%matplotlib inline\n\n# Set up the data to fit\ntime = parish_file['Days'].values \ncumdeathData = parish_file['CumDeaths'].values\n\n# Set the figsize for each subplot\nplt.figure(figsize=(10, 5))\n\ntick_positions = parish_file['Days'].values\ntick_labels = parish_file['EndDate'].values\n\n#Plot the obseved data\nplt.plot(time, cumdeathData, 'bo', label='Observed data')\n\n# Plot daily deaths\nplt.plot(model_solution[\"D\"], \n                 color='orange', label=(model_input.parish_name))\nplt.ylabel('Cummulative Deaths')\nplt.legend(loc='upper left')\nplt.xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\n# plt.savefig('dailydeathsgravseas_cluster_6.png'.format(), dpi=300)\nplt.show()\n#plt.savefig('cumDeathsSVENSTORP.png'.format(), dpi=300)\n</pre> %matplotlib inline  # Set up the data to fit time = parish_file['Days'].values  cumdeathData = parish_file['CumDeaths'].values  # Set the figsize for each subplot plt.figure(figsize=(10, 5))  tick_positions = parish_file['Days'].values tick_labels = parish_file['EndDate'].values  #Plot the obseved data plt.plot(time, cumdeathData, 'bo', label='Observed data')  # Plot daily deaths plt.plot(model_solution[\"D\"],                   color='orange', label=(model_input.parish_name)) plt.ylabel('Cummulative Deaths') plt.legend(loc='upper left') plt.xticks(tick_positions, tick_labels, rotation=70, fontsize=9)   # Adjust the layout to avoid overlapping plt.tight_layout() # plt.savefig('dailydeathsgravseas_cluster_6.png'.format(), dpi=300) plt.show() #plt.savefig('cumDeathsSVENSTORP.png'.format(), dpi=300) In\u00a0[78]: Copied! <pre>parish_file['Days'].values\n</pre> parish_file['Days'].values  Out[78]: <pre>array([ 30,  61,  92, 122, 153, 183, 214])</pre> <p>Plotting the daily deaths to check the model fit</p> In\u00a0[209]: Copied! <pre># %matplotlib inline\n\n# daily_deaths_model = daily_deaths(model_solution, observedTime, 0, model_input.maxDays())\n\n# # Set up the data to fit\n# time = parish_file['Days'].values\n# deathData = parish_file['Deaths'].values\n\n# # Set the figsize for each subplot\n# plt.figure(figsize=(10, 5))\n\n# tick_positions = parish_file['Days'].values\n# tick_labels = parish_file['EndDate'].values\n\n# #Plot the obseved data\n# plt.plot(time, deathData, 'bo', label='Observed data')\n\n# # Plot daily deaths\n# plt.plot(time,daily_deaths_model,\n#                  color='red', label=(model_input.parish_name))\n# plt.ylabel('Daily Deaths')\n# plt.legend(loc='upper right')\n# plt.xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\n#plt.tight_layout()\n# plt.savefig('dailydeathsgravseas_cluster_6.png'.format(), dpi=300)\n#plt.show()\n#plt.savefig('dailyDeathsYstad.png'.format(), dpi=300)\n</pre> # %matplotlib inline  # daily_deaths_model = daily_deaths(model_solution, observedTime, 0, model_input.maxDays())  # # Set up the data to fit # time = parish_file['Days'].values # deathData = parish_file['Deaths'].values  # # Set the figsize for each subplot # plt.figure(figsize=(10, 5))  # tick_positions = parish_file['Days'].values # tick_labels = parish_file['EndDate'].values  # #Plot the obseved data # plt.plot(time, deathData, 'bo', label='Observed data')  # # Plot daily deaths # plt.plot(time,daily_deaths_model, #                  color='red', label=(model_input.parish_name)) # plt.ylabel('Daily Deaths') # plt.legend(loc='upper right') # plt.xticks(tick_positions, tick_labels, rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping #plt.tight_layout() # plt.savefig('dailydeathsgravseas_cluster_6.png'.format(), dpi=300) #plt.show() #plt.savefig('dailyDeathsYstad.png'.format(), dpi=300)"},{"location":"PlagueProject/SEIRDSeasonMetapopModelPlague/","title":"SEIRDSeasonMetapopModelPlague","text":"In\u00a0[1]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline  In\u00a0[2]: Copied! <pre># Python 3.11.2\n# Import packages\nimport scipy.integrate as scipy\nimport scipy.optimize as optimize\nimport scipy.stats as stats\nimport pandas as pd\nimport numpy as np\nimport pylab as pl\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport json  # for pretty printing\n\nimport shutil\nimport sys\nimport os.path\n</pre> # Python 3.11.2 # Import packages import scipy.integrate as scipy import scipy.optimize as optimize import scipy.stats as stats import pandas as pd import numpy as np import pylab as pl import random import matplotlib.pyplot as plt from collections import defaultdict import json  # for pretty printing  import shutil import sys import os.path  <p>Initializing the population size and initial conditions:</p> In\u00a0[3]: Copied! <pre># vector of population sizes with lenght n\nn = 19\nN = np.array([204  # Bromma\n              , 217  # Oja\n              , 1895  # S Maria Ystad 1749\n              , 554  # Valleberga\n              , 693  # S Kopinge\n              , 403  # Horups\n              , 582  # Bj\u00e4resj\u00f6 1780\n              , 716  # Villie 1749\n              , 418  # Sn\u00e5restad 1775\n              , 519  # Sk\u00e5rby 1749\n              , 262  # Hammenh\u00f6gs 1749\n              , 560  # Glemminge 1775\n              , 236  # Balk\u00e5kra 1775\n              , 334  # Baldringe 1749\n              , 299  # Ovraby\n              , 761  # S\u00f6vestads 1749\n              , 776  # L\u00f6derups 1749\n              , 951  # Borrby 1775\n              , 358  # Tosterups 1775\n              ])\n# Initial conditions for each patch\nE0 = np.zeros(n)  # vector of initial exposed with length n\n\nI0 = np.zeros(n)  # vector of initial infecteds with length n\nI0[0] = 1.0  # the first element of the I0 vector is set to 1\n\nS0 = np.zeros(n)  # vector of initial susceptibles with length n\nfor i in range(n):\n    S0[i] = N[i] - I0[i]\n\nR0 = np.zeros(n)  # vector of initial removeds with length n\nD0 = np.zeros(n)  # vector of initial deaths with length n\n\n# print(S0,I0,R0,D0)\n</pre> # vector of population sizes with lenght n n = 19 N = np.array([204  # Bromma               , 217  # Oja               , 1895  # S Maria Ystad 1749               , 554  # Valleberga               , 693  # S Kopinge               , 403  # Horups               , 582  # Bj\u00e4resj\u00f6 1780               , 716  # Villie 1749               , 418  # Sn\u00e5restad 1775               , 519  # Sk\u00e5rby 1749               , 262  # Hammenh\u00f6gs 1749               , 560  # Glemminge 1775               , 236  # Balk\u00e5kra 1775               , 334  # Baldringe 1749               , 299  # Ovraby               , 761  # S\u00f6vestads 1749               , 776  # L\u00f6derups 1749               , 951  # Borrby 1775               , 358  # Tosterups 1775               ]) # Initial conditions for each patch E0 = np.zeros(n)  # vector of initial exposed with length n  I0 = np.zeros(n)  # vector of initial infecteds with length n I0[0] = 1.0  # the first element of the I0 vector is set to 1  S0 = np.zeros(n)  # vector of initial susceptibles with length n for i in range(n):     S0[i] = N[i] - I0[i]  R0 = np.zeros(n)  # vector of initial removeds with length n D0 = np.zeros(n)  # vector of initial deaths with length n  # print(S0,I0,R0,D0)  In\u00a0[4]: Copied! <pre># Defining the transmission rate matrix as a function of two parameters\n\ndef TransmissionRateMatrix(beta: float, p: float) -&gt; np.ndarray:\n    return (\n        np.array([\n            [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],\n            [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],\n            [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],\n            [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],\n            [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],\n            [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],\n            [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],\n            [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],\n            [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],\n            [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],\n            [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]\n        ])\n    )\n</pre> # Defining the transmission rate matrix as a function of two parameters  def TransmissionRateMatrix(beta: float, p: float) -&gt; np.ndarray:     return (         np.array([             [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],             [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],             [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],             [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],             [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],             [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],             [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],             [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],             [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]         ])     )  <p>Define the data. In this case we have: 1. Total number of deaths per patch 2. Initial day of the outbreak for each patch 3. Last day of the outbreak for each patch</p> <p>'0' indicates missing information</p> In\u00a0[5]: Copied! <pre>patchNames = [\"Bromma\", \"Oja\", \"Ystad\", \"Valleberga\", \"S. Kopinge\", \"Horups\", \"Bj\u00e4resj\u00f6\", \"Villie\", \"Sn\u00e5restad\",\n              \"Sk\u00e5rby\", \"Hammenh\u00f6gs\", \"Glemminge\", \"Balk\u00e5kra\", \"Baldringe\", \"Ovraby\", \"S\u00f6vestads\", \"L\u00f6derups\", \"Borrby\", \"Tosterups\"]\n# Deaths by parish. Zero means we don't have information\nTotalDeathsByParish = 1.0 * \\\n    np.array([0, 40, 740, 0, 80, 60, 0, 0, 38,\n             70, 0, 0, 45, 0, 0, 0, 270, 45, 0])\n# initial day for each parish\nBeginPlagueByParish: list[int] = [1, 61, 61, 91, 91, 91, 91,\n                                  121, 121, 121, 121, 121, 121, 121, 151, 181, 271, 331, 511]\n# final day for each parish. Zero means we don't have information\nEndPlagueByParish: list[int] = [60  # Bromma\n                                , 360  # Oja\n                                , 270  # Ystad\n                                , 0  # Valleberga\n                                , 300  # Kopinge\n                                , 0  # Horups\n                                , 0  # Bjaresjo\n                                , 150  # Villie\n                                , 510  # Snarestad\n                                , 270  # Skarby\n                                , 0  # Hammenhogs\n                                , 150  # Glemminge\n                                , 240  # Balkakra\n                                , 0  # Baldringe\n                                , 180  # Ovraby\n                                , 270  # Sovestads\n                                , 570  # Loderups\n                                , 570  # Borrby\n                                , 570  # Tosterups\n                                ]\n\n# Create a dictionary using the first list as the keys\ninfoByPatch = {}\nfor x in zip(patchNames, BeginPlagueByParish, EndPlagueByParish, TotalDeathsByParish):\n    infoByPatch[x[0]] = {'beginPlague': x[1],\n                         'endPlague': x[2],\n                         'totalDeaths': x[3]\n                         }\nprint(infoByPatch)\n\n# infoByPatch['Bromma']['beginPlague']\n</pre> patchNames = [\"Bromma\", \"Oja\", \"Ystad\", \"Valleberga\", \"S. Kopinge\", \"Horups\", \"Bj\u00e4resj\u00f6\", \"Villie\", \"Sn\u00e5restad\",               \"Sk\u00e5rby\", \"Hammenh\u00f6gs\", \"Glemminge\", \"Balk\u00e5kra\", \"Baldringe\", \"Ovraby\", \"S\u00f6vestads\", \"L\u00f6derups\", \"Borrby\", \"Tosterups\"] # Deaths by parish. Zero means we don't have information TotalDeathsByParish = 1.0 * \\     np.array([0, 40, 740, 0, 80, 60, 0, 0, 38,              70, 0, 0, 45, 0, 0, 0, 270, 45, 0]) # initial day for each parish BeginPlagueByParish: list[int] = [1, 61, 61, 91, 91, 91, 91,                                   121, 121, 121, 121, 121, 121, 121, 151, 181, 271, 331, 511] # final day for each parish. Zero means we don't have information EndPlagueByParish: list[int] = [60  # Bromma                                 , 360  # Oja                                 , 270  # Ystad                                 , 0  # Valleberga                                 , 300  # Kopinge                                 , 0  # Horups                                 , 0  # Bjaresjo                                 , 150  # Villie                                 , 510  # Snarestad                                 , 270  # Skarby                                 , 0  # Hammenhogs                                 , 150  # Glemminge                                 , 240  # Balkakra                                 , 0  # Baldringe                                 , 180  # Ovraby                                 , 270  # Sovestads                                 , 570  # Loderups                                 , 570  # Borrby                                 , 570  # Tosterups                                 ]  # Create a dictionary using the first list as the keys infoByPatch = {} for x in zip(patchNames, BeginPlagueByParish, EndPlagueByParish, TotalDeathsByParish):     infoByPatch[x[0]] = {'beginPlague': x[1],                          'endPlague': x[2],                          'totalDeaths': x[3]                          } print(infoByPatch)  # infoByPatch['Bromma']['beginPlague']  <pre>{'Bromma': {'beginPlague': 1, 'endPlague': 60, 'totalDeaths': 0.0}, 'Oja': {'beginPlague': 61, 'endPlague': 360, 'totalDeaths': 40.0}, 'Ystad': {'beginPlague': 61, 'endPlague': 270, 'totalDeaths': 740.0}, 'Valleberga': {'beginPlague': 91, 'endPlague': 0, 'totalDeaths': 0.0}, 'S. Kopinge': {'beginPlague': 91, 'endPlague': 300, 'totalDeaths': 80.0}, 'Horups': {'beginPlague': 91, 'endPlague': 0, 'totalDeaths': 60.0}, 'Bj\u00e4resj\u00f6': {'beginPlague': 91, 'endPlague': 0, 'totalDeaths': 0.0}, 'Villie': {'beginPlague': 121, 'endPlague': 150, 'totalDeaths': 0.0}, 'Sn\u00e5restad': {'beginPlague': 121, 'endPlague': 510, 'totalDeaths': 38.0}, 'Sk\u00e5rby': {'beginPlague': 121, 'endPlague': 270, 'totalDeaths': 70.0}, 'Hammenh\u00f6gs': {'beginPlague': 121, 'endPlague': 0, 'totalDeaths': 0.0}, 'Glemminge': {'beginPlague': 121, 'endPlague': 150, 'totalDeaths': 0.0}, 'Balk\u00e5kra': {'beginPlague': 121, 'endPlague': 240, 'totalDeaths': 45.0}, 'Baldringe': {'beginPlague': 121, 'endPlague': 0, 'totalDeaths': 0.0}, 'Ovraby': {'beginPlague': 151, 'endPlague': 180, 'totalDeaths': 0.0}, 'S\u00f6vestads': {'beginPlague': 181, 'endPlague': 270, 'totalDeaths': 0.0}, 'L\u00f6derups': {'beginPlague': 271, 'endPlague': 570, 'totalDeaths': 270.0}, 'Borrby': {'beginPlague': 331, 'endPlague': 570, 'totalDeaths': 45.0}, 'Tosterups': {'beginPlague': 511, 'endPlague': 570, 'totalDeaths': 0.0}}\n</pre> In\u00a0[6]: Copied! <pre># divmod(t,365) is the same as (t//365, t%365)\n\nimport matplotlib.pyplot as plt\n\n\ndef gaussian(x, mu, sigma):\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n\n\ndef seasonality(t, bump_center, bump_width, bump_height):\n    return bump_height * gaussian(t % 365, bump_center, bump_width) + bump_height * gaussian(t % 365 - 365, bump_center, bump_width) + bump_height * gaussian(t % 365 + 365, bump_center, bump_width)\n\n\n# Generate an array of day_of_year values from 1 to 365\ndays = np.arange(1, 365)\n\n# Calculate the corresponding temperature approximations\nseas = [seasonality(day, 255, 30, 0.2) for day in days]\n\n\ntick_positions = [16, 44, 75, 105, 136, 166, 197, 228, 258, 289, 319, 350]\ntick_labels = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\",\n               \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\n# Plot the temperature approximations over the year\n\n%matplotlib inline\nplt.plot(days, seas)\nplt.xlabel('Day of Year')\nplt.ylabel('transmission rate modifier')\nplt.title('Seasonality in transmission rate (vector abundance, other factors)')\nplt.xticks(tick_positions, tick_labels, rotation=70, fontsize=9)\nplt.show()\n</pre> # divmod(t,365) is the same as (t//365, t%365)  import matplotlib.pyplot as plt   def gaussian(x, mu, sigma):     return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))   def seasonality(t, bump_center, bump_width, bump_height):     return bump_height * gaussian(t % 365, bump_center, bump_width) + bump_height * gaussian(t % 365 - 365, bump_center, bump_width) + bump_height * gaussian(t % 365 + 365, bump_center, bump_width)   # Generate an array of day_of_year values from 1 to 365 days = np.arange(1, 365)  # Calculate the corresponding temperature approximations seas = [seasonality(day, 255, 30, 0.2) for day in days]   tick_positions = [16, 44, 75, 105, 136, 166, 197, 228, 258, 289, 319, 350] tick_labels = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\",                \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]  # Plot the temperature approximations over the year  %matplotlib inline plt.plot(days, seas) plt.xlabel('Day of Year') plt.ylabel('transmission rate modifier') plt.title('Seasonality in transmission rate (vector abundance, other factors)') plt.xticks(tick_positions, tick_labels, rotation=70, fontsize=9) plt.show()  <p>Generating the differential equations</p> In\u00a0[8]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n\n    parameters = model_parameters[0]\n    vecpeak: float = parameters['vecpeak']\n    vecwidth: float = parameters['vecwidth']\n    vecboost: float = parameters['vecboost']\n    beta: float = parameters['beta'] + \\\n        seasonality(t, vecpeak, vecwidth, vecboost)\n    sigma: float = parameters['sigma']  # transition from Exposed to Infected\n    # transition from Infected to Recovered / Dead\n    gamma: float = parameters['gamma']\n    p: float = parameters['p']\n    mu: float = parameters['mu']\n    N = parameters['N']\n    n: int = parameters['n']\n\n    S = defaultdict(float)\n    E = defaultdict(float)\n    I = defaultdict(float)\n    R = defaultdict(float)\n    D = defaultdict(float)\n\n    vars = tuple(sum([[S[i], E[i], I[i], R[i], D[i]] for i in range(n)], []))\n    vars = y\n\n   # Choosing the corresponding output for each subpopulation\n    def entryS(i):\n        return vars[5 * i]\n\n    def entryE(i):\n        return vars[5 * i + 1]\n\n    def entryI(i):\n        return vars[5 * i + 2]\n\n    def entryR(i):\n        return vars[5 * i + 3]\n\n    def entryD(i):\n        return vars[5 * i + 4]\n\n    # Initializando the directory for each subpopulation\n    dS = {}\n    dE = {}\n    dI = {}\n    dR = {}\n    dD = {}\n\n    # Defining the differential equations for each subpopulation\n    for i in range(n):\n        dS[i] = - entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)\n                                               [i][j] * entryI(j) for j in range(n))\n        dE[i] = entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)\n                                             [i][j] * entryI(j) for j in range(n)) - sigma * entryE(i)\n        dI[i] = sigma * entryE(i) - gamma * entryI(i)\n        dR[i] = gamma * (1 - mu) * entryI(i)\n        dD[i] = gamma * mu * entryI(i)\n    derivates = sum([[dS[i], dE[i], dI[i], dR[i], dD[i]]\n                    for i in range(n)], [])\n    return derivates   # For odeint\n\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n\n    # Initial conditions vector for the metapopulation model. len(initConditions) = 4*n\n    initConditions = tuple(sum(\n        [[init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i]] for i in range(n)], []))\n\n    # Time vector\n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    # Computing the numerical solution\n    model = genInput['model']\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {}\n    indexVar['S'] = 0\n    indexVar['E'] = 1\n    indexVar['I'] = 2\n    indexVar['R'] = 3\n    indexVar['D'] = 4\n\n    def varSol(patch, var):\n        return solution[:, 5*patch + indexVar[var]]\n\n    return {'S': {patch: varSol(patch, 'S') for patch in range(n)},\n            'E': {patch: varSol(patch, 'E') for patch in range(n)},\n            'I': {patch: varSol(patch, 'I') for patch in range(n)},\n            'R': {patch: varSol(patch, 'R') for patch in range(n)},\n            'D': {patch: varSol(patch, 'D') for patch in range(n)},\n            'N': genInput['N'],\n            'init': init,\n            'beta': genInput['beta'],\n            'p': genInput['p'],\n            'gamma': genInput['gamma'],\n            'mu':  genInput['mu'],\n            'sigma': genInput['sigma'],\n            't': t,\n            'n': n,\n            'vecpeak': genInput['vecpeak'],\n            'vecwidth': genInput['vecwidth'],\n            'vecboost': genInput['vecboost'],\n            'model': model,\n            'raw_solution': solution}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):      parameters = model_parameters[0]     vecpeak: float = parameters['vecpeak']     vecwidth: float = parameters['vecwidth']     vecboost: float = parameters['vecboost']     beta: float = parameters['beta'] + \\         seasonality(t, vecpeak, vecwidth, vecboost)     sigma: float = parameters['sigma']  # transition from Exposed to Infected     # transition from Infected to Recovered / Dead     gamma: float = parameters['gamma']     p: float = parameters['p']     mu: float = parameters['mu']     N = parameters['N']     n: int = parameters['n']      S = defaultdict(float)     E = defaultdict(float)     I = defaultdict(float)     R = defaultdict(float)     D = defaultdict(float)      vars = tuple(sum([[S[i], E[i], I[i], R[i], D[i]] for i in range(n)], []))     vars = y     # Choosing the corresponding output for each subpopulation     def entryS(i):         return vars[5 * i]      def entryE(i):         return vars[5 * i + 1]      def entryI(i):         return vars[5 * i + 2]      def entryR(i):         return vars[5 * i + 3]      def entryD(i):         return vars[5 * i + 4]      # Initializando the directory for each subpopulation     dS = {}     dE = {}     dI = {}     dR = {}     dD = {}      # Defining the differential equations for each subpopulation     for i in range(n):         dS[i] = - entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)                                                [i][j] * entryI(j) for j in range(n))         dE[i] = entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)                                              [i][j] * entryI(j) for j in range(n)) - sigma * entryE(i)         dI[i] = sigma * entryE(i) - gamma * entryI(i)         dR[i] = gamma * (1 - mu) * entryI(i)         dD[i] = gamma * mu * entryI(i)     derivates = sum([[dS[i], dE[i], dI[i], dR[i], dD[i]]                     for i in range(n)], [])     return derivates   # For odeint   def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']      # Initial conditions vector for the metapopulation model. len(initConditions) = 4*n     initConditions = tuple(sum(         [[init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i]] for i in range(n)], []))      # Time vector     T = genInput['T']     t = np.linspace(0, T, T+1)      # Computing the numerical solution     model = genInput['model']     solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {}     indexVar['S'] = 0     indexVar['E'] = 1     indexVar['I'] = 2     indexVar['R'] = 3     indexVar['D'] = 4      def varSol(patch, var):         return solution[:, 5*patch + indexVar[var]]      return {'S': {patch: varSol(patch, 'S') for patch in range(n)},             'E': {patch: varSol(patch, 'E') for patch in range(n)},             'I': {patch: varSol(patch, 'I') for patch in range(n)},             'R': {patch: varSol(patch, 'R') for patch in range(n)},             'D': {patch: varSol(patch, 'D') for patch in range(n)},             'N': genInput['N'],             'init': init,             'beta': genInput['beta'],             'p': genInput['p'],             'gamma': genInput['gamma'],             'mu':  genInput['mu'],             'sigma': genInput['sigma'],             't': t,             'n': n,             'vecpeak': genInput['vecpeak'],             'vecwidth': genInput['vecwidth'],             'vecboost': genInput['vecboost'],             'model': model,             'raw_solution': solution}  In\u00a0[9]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters, beginTime, endTime, deathData):\n    beta, p, vecpeak, vecwidth, vecboost = parameters\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': S0,\n                      'E': E0,\n                      'I': I0,\n                      'R': R0,\n                      'D': D0,\n                  },  # defining the initial values for the model\n                  'beta': beta,\n                  'p': p,\n                  'gamma': 0.06,\n                  'sigma': 0.02,\n                  'mu': 0.2,\n                  'vecpeak': vecpeak,\n                  'vecwidth': vecwidth,\n                  'vecboost': vecboost,\n                  'N': N,\n                  'n': 19,\n                  'T': 700}\n\n    model_sol = generate_sol(model_info)\n    totalError = 0\n    n = model_info['n']\n    for i in range(n):\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        if (deathData[i] != 0 and final_position != 0):\n            try:\n                val1 = model_sol['D'][i]\n                val2 = val1[final_position]\n                val3 = deathData[i]\n                totalError += 0.25*(model_sol['D'][i][initial_position] - 1.0)**2 + 0.75*(\n                    model_sol['D'][i][final_position] - deathData[i])**2\n            except:\n                print(n)\n                print(i)\n                print(final_position)\n                print(len(model_sol['D']))\n                print(model_sol['D'][i])\n                print(deathData[i])\n        else:\n            totalError += (model_sol['D'][i][initial_position] - 1.0)**2\n    return totalError\n</pre>  # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters, beginTime, endTime, deathData):     beta, p, vecpeak, vecwidth, vecboost = parameters     model_info = {'model': SEIRD_model,                   'init': {                       'S': S0,                       'E': E0,                       'I': I0,                       'R': R0,                       'D': D0,                   },  # defining the initial values for the model                   'beta': beta,                   'p': p,                   'gamma': 0.06,                   'sigma': 0.02,                   'mu': 0.2,                   'vecpeak': vecpeak,                   'vecwidth': vecwidth,                   'vecboost': vecboost,                   'N': N,                   'n': 19,                   'T': 700}      model_sol = generate_sol(model_info)     totalError = 0     n = model_info['n']     for i in range(n):         initial_position = beginTime[i]         final_position = endTime[i]         if (deathData[i] != 0 and final_position != 0):             try:                 val1 = model_sol['D'][i]                 val2 = val1[final_position]                 val3 = deathData[i]                 totalError += 0.25*(model_sol['D'][i][initial_position] - 1.0)**2 + 0.75*(                     model_sol['D'][i][final_position] - deathData[i])**2             except:                 print(n)                 print(i)                 print(final_position)                 print(len(model_sol['D']))                 print(model_sol['D'][i])                 print(deathData[i])         else:             totalError += (model_sol['D'][i][initial_position] - 1.0)**2     return totalError  In\u00a0[10]: Copied! <pre># Set up the data to fit\nbeginTime = BeginPlagueByParish\nendTime = EndPlagueByParish\ndeathData = TotalDeathsByParish\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = 0.3\np_guess = 0.1\nvecboost_guess = 0.1\nvecpeak_guess = 180.0\nvecwidth_guess = 30.0\n</pre> # Set up the data to fit beginTime = BeginPlagueByParish endTime = EndPlagueByParish deathData = TotalDeathsByParish  # Choose initial guesses for the parameters to fit beta_guess = 0.3 p_guess = 0.1 vecboost_guess = 0.1 vecpeak_guess = 180.0 vecwidth_guess = 30.0  In\u00a0[11]: Copied! <pre># Minimize the objective function to obtain estimates for beta and gamma\nresult = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, vecboost_guess, vecpeak_guess, vecwidth_guess), args=(beginTime, endTime, deathData),\n                           method='L-BFGS-B'\n                           # , bounds=[(0,1),(0,1),(0, 10), (-2, 2), (-10, 10)]\n                           )\nbeta_estimated, p_estimated, vecboost_estimated, vecpeak_estimated, vecwidth_estimated = result.x\n\nprint(\"beta = \", beta_estimated)\nprint(\"p = \", p_estimated)\nprint(\"vecboost = \", vecboost_estimated)\nprint(\"vecpeak = \", vecpeak_estimated)\nprint(\"vecwidth = \", vecwidth_estimated)\n</pre>  # Minimize the objective function to obtain estimates for beta and gamma result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, vecboost_guess, vecpeak_guess, vecwidth_guess), args=(beginTime, endTime, deathData),                            method='L-BFGS-B'                            # , bounds=[(0,1),(0,1),(0, 10), (-2, 2), (-10, 10)]                            ) beta_estimated, p_estimated, vecboost_estimated, vecpeak_estimated, vecwidth_estimated = result.x  print(\"beta = \", beta_estimated) print(\"p = \", p_estimated) print(\"vecboost = \", vecboost_estimated) print(\"vecpeak = \", vecpeak_estimated) print(\"vecwidth = \", vecwidth_estimated)  <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[11], line 2\n      1 # Minimize the objective function to obtain estimates for beta and gamma\n----&gt; 2 result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, vecboost_guess, vecpeak_guess, vecwidth_guess), args=(beginTime, endTime, deathData),\n      3                            method='L-BFGS-B'\n      4                            # , bounds=[(0,1),(0,1),(0, 10), (-2, 2), (-10, 10)]\n      5                            )\n      6 beta_estimated, p_estimated, vecboost_estimated, vecpeak_estimated, vecwidth_estimated = result.x\n      8 print(\"beta = \", beta_estimated)\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_minimize.py:696, in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\n    693     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n    694                              **options)\n    695 elif meth == 'l-bfgs-b':\n--&gt; 696     res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n    697                            callback=callback, **options)\n    698 elif meth == 'tnc':\n    699     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n    700                         **options)\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:305, in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\n    302     else:\n    303         iprint = disp\n--&gt; 305 sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n    306                               bounds=new_bounds,\n    307                               finite_diff_rel_step=finite_diff_rel_step)\n    309 func_and_grad = sf.fun_and_grad\n    311 fortran_int = _lbfgsb.types.intvar.dtype\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_optimize.py:332, in _prepare_scalar_function(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\n    328     bounds = (-np.inf, np.inf)\n    330 # ScalarFunction caches. Reuse of fun(x) during grad\n    331 # calculation reduces overall function evaluations.\n--&gt; 332 sf = ScalarFunction(fun, x0, args, grad, hess,\n    333                     finite_diff_rel_step, bounds, epsilon=epsilon)\n    335 return sf\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:158, in ScalarFunction.__init__(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\n    155     self.f = fun_wrapped(self.x)\n    157 self._update_fun_impl = update_fun\n--&gt; 158 self._update_fun()\n    160 # Gradient evaluation\n    161 if callable(grad):\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251, in ScalarFunction._update_fun(self)\n    249 def _update_fun(self):\n    250     if not self.f_updated:\n--&gt; 251         self._update_fun_impl()\n    252         self.f_updated = True\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155, in ScalarFunction.__init__.&lt;locals&gt;.update_fun()\n    154 def update_fun():\n--&gt; 155     self.f = fun_wrapped(self.x)\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137, in ScalarFunction.__init__.&lt;locals&gt;.fun_wrapped(x)\n    133 self.nfev += 1\n    134 # Send a copy because the user may overwrite it.\n    135 # Overwriting results in undefined behaviour because\n    136 # fun(self.x) will change self.x, with the two no longer linked.\n--&gt; 137 fx = fun(np.copy(x), *args)\n    138 # Make sure the function returns a true scalar\n    139 if not np.isscalar(fx):\n\nCell In[9], line 24, in objectiveFunction(parameters, beginTime, endTime, deathData)\n      3 beta, p, vecpeak, vecwidth, vecboost = parameters\n      4 model_info = {'model': SEIRD_model,\n      5               'init': {\n      6                   'S': S0,\n   (...)\n     21               'n': 19,\n     22               'T': 700}\n---&gt; 24 model_sol = generate_sol(model_info)\n     25 totalError = 0\n     26 n = model_info['n']\n\nCell In[8], line 77, in generate_sol(genInput)\n     75 # Computing the numerical solution\n     76 model = genInput['model']\n---&gt; 77 solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n     79 indexVar = {}\n     80 indexVar['S'] = 0\n\nFile /opt/homebrew/lib/python3.11/site-packages/scipy/integrate/_odepack_py.py:242, in odeint(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg, tfirst)\n    240 t = copy(t)\n    241 y0 = copy(y0)\n--&gt; 242 output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n    243                          full_output, rtol, atol, tcrit, h0, hmax, hmin,\n    244                          ixpr, mxstep, mxhnil, mxordn, mxords,\n    245                          int(bool(tfirst)))\n    246 if output[-1] &lt; 0:\n    247     warning_msg = _msgs[output[-1]] + \" Run with full_output = 1 to get quantitative information.\"\n\nCell In[8], line 53, in SEIRD_model(y, t, model_parameters)\n     50 for i in range(n):\n     51     dS[i] = - entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)\n     52                                            [i][j] * entryI(j) for j in range(n))\n---&gt; 53     dE[i] = entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)\n     54                                          [i][j] * entryI(j) for j in range(n)) - sigma * entryE(i)\n     55     dI[i] = sigma * entryE(i) - gamma * entryI(i)\n     56     dR[i] = gamma * (1 - mu) * entryI(i)\n\nCell In[8], line 53, in &lt;genexpr&gt;(.0)\n     50 for i in range(n):\n     51     dS[i] = - entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)\n     52                                            [i][j] * entryI(j) for j in range(n))\n---&gt; 53     dE[i] = entryS(i) / (N[i]*1.0) * sum(TransmissionRateMatrix(beta, p)\n     54                                          [i][j] * entryI(j) for j in range(n)) - sigma * entryE(i)\n     55     dI[i] = sigma * entryE(i) - gamma * entryI(i)\n     56     dR[i] = gamma * (1 - mu) * entryI(i)\n\nCell In[5], line 5, in TransmissionRateMatrix(beta, p)\n      3 def TransmissionRateMatrix(beta: float, p: float) -&gt; np.ndarray:\n      4     return (\n----&gt; 5         np.array([\n      6             [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],\n      7             [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      8             [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      9             [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],\n     10             [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],\n     11             [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],\n     12             [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],\n     13             [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n     14             [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],\n     15             [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],\n     16             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],\n     17             [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],\n     18             [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],\n     19             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],\n     20             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],\n     21             [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],\n     22             [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],\n     23             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],\n     24             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]\n     25         ])\n     26     )\n\nKeyboardInterrupt: </pre> In\u00a0[\u00a0]: Copied! <pre># option for objective function\n# (sum(model_sol['I'][i][position] for position in range(initial_position, final_position))-0.6*deathData[i])**2\n\n# Set up the data to fit\nbeginTime = BeginPlagueByParish\nendTime = EndPlagueByParish\ndeathData = TotalDeathsByParish\n</pre> # option for objective function # (sum(model_sol['I'][i][position] for position in range(initial_position, final_position))-0.6*deathData[i])**2  # Set up the data to fit beginTime = BeginPlagueByParish endTime = EndPlagueByParish deathData = TotalDeathsByParish  In\u00a0[\u00a0]: Copied! <pre># Manually fit the model\nmodel_estimation = {'model': SEIRD_model,\n                    'init': {\n                        'S': S0,\n                        'E': E0,\n                        'I': I0,\n                        'R': R0,\n                        'D': D0,\n                    },  # defining the initial values for the model\n                    'beta': 0.1,\n                    'vecboost': 0.1,\n                    'p': 0.05,  # 0.000000004,\n                    'vecpeak': 180,\n                    'vecwidth': 30,\n                    'sigma': 0.02,\n                    'gamma': 0.06,\n                    'mu': 0.2,\n                    'N': N,\n                    'n': 19,\n                    'T': 700}\n\nmodel_dict = generate_sol(model_estimation)\n\n# Number of patches\nn = 8  # model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ntick_positions = [30, 60, 90, 120, 150, 180, 210, 240,\n                  270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]\ntick_labels = [\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"\n               ]\n\n# Dictionary that reduces the plotting to those plots with data\nlookup_index = [1, 2, 4, 8, 9, 12, 16, 17]\n\n# Plot daily deaths for each patch i\nfor i in range(n):\n    if TotalDeathsByParish[lookup_index[i]] != 0 and EndPlagueByParish[lookup_index[i]] != 0:\n        initial_position = beginTime[lookup_index[i]]\n        final_position = EndPlagueByParish[lookup_index[i]]\n        axes[i].plot(initial_position, 0, 'bo')\n        axes[i].plot(final_position,\n                     TotalDeathsByParish[lookup_index[i]], 'bo')\n        axes[i].plot(model_dict['D'][lookup_index[i]], color='orange')\n        # axes[i].plot(model_dict['D'][lookup_index[i]], color='orange', label=(patchNames[lookup_index[i]]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        # axes[i].legend(loc = 'lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n    else:\n        axes[i].plot(model_dict['D'][lookup_index[i]],\n                     color='orange', label=(patchNames[lookup_index[i]]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> # Manually fit the model model_estimation = {'model': SEIRD_model,                     'init': {                         'S': S0,                         'E': E0,                         'I': I0,                         'R': R0,                         'D': D0,                     },  # defining the initial values for the model                     'beta': 0.1,                     'vecboost': 0.1,                     'p': 0.05,  # 0.000000004,                     'vecpeak': 180,                     'vecwidth': 30,                     'sigma': 0.02,                     'gamma': 0.06,                     'mu': 0.2,                     'N': N,                     'n': 19,                     'T': 700}  model_dict = generate_sol(model_estimation)  # Number of patches n = 8  # model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  tick_positions = [30, 60, 90, 120, 150, 180, 210, 240,                   270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570] tick_labels = [\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"                ]  # Dictionary that reduces the plotting to those plots with data lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]  # Plot daily deaths for each patch i for i in range(n):     if TotalDeathsByParish[lookup_index[i]] != 0 and EndPlagueByParish[lookup_index[i]] != 0:         initial_position = beginTime[lookup_index[i]]         final_position = EndPlagueByParish[lookup_index[i]]         axes[i].plot(initial_position, 0, 'bo')         axes[i].plot(final_position,                      TotalDeathsByParish[lookup_index[i]], 'bo')         axes[i].plot(model_dict['D'][lookup_index[i]], color='orange')         # axes[i].plot(model_dict['D'][lookup_index[i]], color='orange', label=(patchNames[lookup_index[i]]))         axes[i].set_ylabel('Cumulative Deaths')         # axes[i].legend(loc = 'lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)     else:         axes[i].plot(model_dict['D'][lookup_index[i]],                      color='orange', label=(patchNames[lookup_index[i]]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()  In\u00a0[\u00a0]: Copied! <pre>model_estimation = {'model': SIRD_model,\n                    'init': {\n                        'S': S0,\n                        'I': I0,\n                        'R': R0,\n                        'D': D0,\n                    },  # defining the initial values for the model\n                    'beta': beta_estimated,\n                    'p': p_estimated,\n                    'beta0': beta0_estimated,\n                    'beta1': beta1_estimated,\n                    'a': a_estimated,\n                    'gamma': 0.32,\n                    'mu': 0.6,\n                    'N': N,\n                    'n': 19,\n                    'T': 700}\nmodel_dict = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SIRD_model,                     'init': {                         'S': S0,                         'I': I0,                         'R': R0,                         'D': D0,                     },  # defining the initial values for the model                     'beta': beta_estimated,                     'p': p_estimated,                     'beta0': beta0_estimated,                     'beta1': beta1_estimated,                     'a': a_estimated,                     'gamma': 0.32,                     'mu': 0.6,                     'N': N,                     'n': 19,                     'T': 700} model_dict = generate_sol(model_estimation)  <p>Plotting the solutions</p> In\u00a0[\u00a0]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n\n# Plotting the solution\n\n\ndef plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):\n\n    for key in state:\n        if key not in model:\n            raise ValueError(f\"Invalid state: {key}\")\n        for i in range(model['n']):\n            plt.plot(model['t'], model[key][i],\n                     label=f'{key} - {patchNames[i]}')\n    plt.xlabel('Time')\n    plt.ylabel('Infectious')\n    plt.title('SIRD model')\n    # plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))\n    plt.show()\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)]  # Plotting the solution   def plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):      for key in state:         if key not in model:             raise ValueError(f\"Invalid state: {key}\")         for i in range(model['n']):             plt.plot(model['t'], model[key][i],                      label=f'{key} - {patchNames[i]}')     plt.xlabel('Time')     plt.ylabel('Infectious')     plt.title('SIRD model')     # plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))     plt.show()  In\u00a0[\u00a0]: Copied! <pre># Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\ntick_labels = [\"Mar 1712\", \"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"\n               ]\ntick_positions = [0, 30, 60, 90, 120, 150, 180, 210, 240,\n                  270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_dict, i, 0, 570),\n                 color='blue', label=(patchNames[i]))\n    axes[i].set_ylabel('Daily Deaths', font='Helvetica')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, font='Helvetica', fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  tick_labels = [\"Mar 1712\", \"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"                ] tick_positions = [0, 30, 60, 90, 120, 150, 180, 210, 240,                   270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_dict, i, 0, 570),                  color='blue', label=(patchNames[i]))     axes[i].set_ylabel('Daily Deaths', font='Helvetica')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, font='Helvetica', fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()"},{"location":"PlagueProject/SIRClusterFitting/","title":"SIRClusterFitting","text":"In\u00a0[30]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[31]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[32]: Copied! <pre>import sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nmain_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else ''\ndocs_dir = os.path.join(main_dir, 'docs')\nproject_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else ''\ndata_private_folder = os.path.join(project_dir, 'data', 'private')\n\n# GitHub credentials for Colab\n\nUSERNAME=\"\" # Your GitHub username\nGH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)\n\nif IN_COLAB:\n  ! rm -rf *\n  ! git config --system credential.helper store\n  ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials\n  ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling\n  sys.path += [main_dir, docs_dir, project_dir, data_private_folder]\n</pre> import sys import os  IN_COLAB = 'google.colab' in sys.modules  main_dir = '/content/PythonMathematicalModeling/' if IN_COLAB else '' docs_dir = os.path.join(main_dir, 'docs') project_dir = os.path.join(docs_dir, 'PlagueProject') if IN_COLAB else '' data_private_folder = os.path.join(project_dir, 'data', 'private')  # GitHub credentials for Colab  USERNAME=\"\" # Your GitHub username GH_TOKEN=\"\" # Your github token (Settings -&gt; Developer Settings -&gt; Personal Access Tokens -&gt; Generate new token)  if IN_COLAB:   ! rm -rf *   ! git config --system credential.helper store   ! echo \"https://{USERNAME}:{GH_TOKEN}@github.com\" &gt; ~/.git-credentials   ! git clone --recurse-submodules https://github.com/polislizarralde/PythonMathematicalModeling   sys.path += [main_dir, docs_dir, project_dir, data_private_folder] In\u00a0[33]: Copied! <pre>try:\n  from funct_process_data import *  # Import all functions from funct_process_data.py\nexcept ImportError:\n  if IN_COLAB:\n    ! pip3 install -r PythonMathematicalModeling/requirements.txt\n    print('Stopping RUNTIME! Please run again.')\n    os.kill(os.getpid(), 9)\n</pre> try:   from funct_process_data import *  # Import all functions from funct_process_data.py except ImportError:   if IN_COLAB:     ! pip3 install -r PythonMathematicalModeling/requirements.txt     print('Stopping RUNTIME! Please run again.')     os.kill(os.getpid(), 9)      In\u00a0[34]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[35]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] In\u00a0[36]: Copied! <pre># Set the working directory for private files\n\n# Southeast Scania\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n# Middle Scania\nmiddle_path = os.path.join(data_private_folder, 'middleScania.csv')\nmiddleScania = pd.read_csv(middle_path, sep=',')\n# Southwest Scania\nsouthwest_path = os.path.join(data_private_folder, 'southwestScania.csv')\nsouthwestScania = pd.read_csv(southwest_path, sep=',')\n\n# Concatenate all the South Scania data into one dataframe and reset the index\nsouthScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True)\n</pre> # Set the working directory for private files  # Southeast Scania southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',') # Middle Scania middle_path = os.path.join(data_private_folder, 'middleScania.csv') middleScania = pd.read_csv(middle_path, sep=',') # Southwest Scania southwest_path = os.path.join(data_private_folder, 'southwestScania.csv') southwestScania = pd.read_csv(southwest_path, sep=',')  # Concatenate all the South Scania data into one dataframe and reset the index southScania = pd.concat([southeastScania, middleScania, southwestScania], ignore_index=True) In\u00a0[37]: Copied! <pre># Convert WKT (Well-Known Text) geometry to Shapely geometry\nsouthScania['geometry'] = southScania['geometry'].apply(wkt.loads)\n# Create a GeoDataFrame from the DataFrame\nsouthScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry')\n\n# Assigning the coordinate reference system (CRS) to the GeoDataFrame\nsouthScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")\n\n# For checking the CRS\n# southScaniaMap.crs\n</pre> # Convert WKT (Well-Known Text) geometry to Shapely geometry southScania['geometry'] = southScania['geometry'].apply(wkt.loads) # Create a GeoDataFrame from the DataFrame southScaniaMap = gpd.GeoDataFrame(southScania, geometry='geometry')  # Assigning the coordinate reference system (CRS) to the GeoDataFrame southScaniaMap = southScaniaMap.set_crs(\"EPSG:3034\")  # For checking the CRS # southScaniaMap.crs <p>We will focus only in the parishes affected by the plague. To do so, first we filter the data frame.</p> In\u00a0[38]: Copied! <pre>colorByColumn(southScaniaMap, 'EndPlaguePeriod')\nplagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red']\nlen(plagueSouthScania)\n</pre> colorByColumn(southScaniaMap, 'EndPlaguePeriod') plagueSouthScania = southScaniaMap[southScaniaMap['color'] == 'red'] len(plagueSouthScania) Out[38]: <pre>87</pre> <p>First, we replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe. Then, we add new columns to the dataframe where each element is the type pandas._libs.tslibs.timestamps.Timestamp.</p> In\u00a0[39]: Copied! <pre>plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan)\nplagueSouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['BeginPlaguePeriod'], format='%b %Y')\nplagueSouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(\n    plagueSouthScania['EndPlaguePeriod'], format='%b %Y')\n</pre> plagueSouthScania = plagueSouthScania.replace(['UNDEFINED', '?'], np.nan) plagueSouthScania['new_format_BeginPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['BeginPlaguePeriod'], format='%b %Y') plagueSouthScania['new_format_EndPlaguePeriod'] = pd.to_datetime(     plagueSouthScania['EndPlaguePeriod'], format='%b %Y') <p>Doing the clusters under the condition of shared borders</p> In\u00a0[40]: Copied! <pre># Create a graph\nG = nx.Graph()\n\n# Add nodes\nfor index, row in plagueSouthScania.iterrows():\n    G.add_node(index, polygon=row['geometry'])\n\n# Add edges\nfor i, row_i in plagueSouthScania.iterrows():\n    for j, row_j in plagueSouthScania.iterrows():\n        if i != j and row_i['geometry'].touches(row_j['geometry']):\n            G.add_edge(i, j)\n# Find connected components\nconnected_components = list(nx.connected_components(G))\n\n# Create new geodataframes for each subset of connected polygons\ngdfs = [gpd.GeoDataFrame(plagueSouthScania.loc[list(component)], crs=plagueSouthScania.crs) for component in connected_components]\n\nplagueSouthScania = plagueSouthScania.copy()\n\n# Initialize a new column in the original dataframe\nplagueSouthScania['component'] = -1\n\n# Loop over the list of connected components\nfor i, component in enumerate(connected_components):\n    # For each component, set the 'component' value of the corresponding rows to the current component number\n    plagueSouthScania.loc[list(component), 'component'] = i\n</pre> # Create a graph G = nx.Graph()  # Add nodes for index, row in plagueSouthScania.iterrows():     G.add_node(index, polygon=row['geometry'])  # Add edges for i, row_i in plagueSouthScania.iterrows():     for j, row_j in plagueSouthScania.iterrows():         if i != j and row_i['geometry'].touches(row_j['geometry']):             G.add_edge(i, j) # Find connected components connected_components = list(nx.connected_components(G))  # Create new geodataframes for each subset of connected polygons gdfs = [gpd.GeoDataFrame(plagueSouthScania.loc[list(component)], crs=plagueSouthScania.crs) for component in connected_components]  plagueSouthScania = plagueSouthScania.copy()  # Initialize a new column in the original dataframe plagueSouthScania['component'] = -1  # Loop over the list of connected components for i, component in enumerate(connected_components):     # For each component, set the 'component' value of the corresponding rows to the current component number     plagueSouthScania.loc[list(component), 'component'] = i In\u00a0[41]: Copied! <pre>color_list = ['steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen', 'coral'\n                , 'steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen']\nfig, ax = plt.subplots(figsize=(7, 4))\nSkaneMap.plot(ax=ax, color = 'whitesmoke', edgecolor='black',\n              legend=False)\nsouthScaniaMap.plot(ax=ax, color = 'azure',\n                        edgecolor='darkgray', legend=False)\nfor i in range(len(gdfs)):\n    cluster_i = plagueSouthScania[plagueSouthScania['component'] == i]\n    if len(cluster_i) &lt; 3:\n        cluster_i.plot(ax=ax, color = 'peachpuff',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) == 4:\n        cluster_i.plot(ax=ax, color = 'deepskyblue',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) == 9:\n        cluster_i.plot(ax=ax, color = 'salmon',\n                        edgecolor='black', legend=False)\n    elif len(cluster_i) &gt; 9:\n        cluster_i.plot(ax=ax, color = 'aquamarine',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> color_list = ['steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen', 'coral'                 , 'steelblue', 'springgreen', 'aquamarine', 'darkcyan', 'skyblue', 'dodgerblue', 'seagreen'] fig, ax = plt.subplots(figsize=(7, 4)) SkaneMap.plot(ax=ax, color = 'whitesmoke', edgecolor='black',               legend=False) southScaniaMap.plot(ax=ax, color = 'azure',                         edgecolor='darkgray', legend=False) for i in range(len(gdfs)):     cluster_i = plagueSouthScania[plagueSouthScania['component'] == i]     if len(cluster_i) &lt; 3:         cluster_i.plot(ax=ax, color = 'peachpuff',                         edgecolor='black', legend=False)     elif len(cluster_i) == 4:         cluster_i.plot(ax=ax, color = 'deepskyblue',                         edgecolor='black', legend=False)     elif len(cluster_i) == 9:         cluster_i.plot(ax=ax, color = 'salmon',                         edgecolor='black', legend=False)     elif len(cluster_i) &gt; 9:         cluster_i.plot(ax=ax, color = 'aquamarine',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[42]: Copied! <pre>cluster_0 = plagueSouthScania[plagueSouthScania['component'] == 0]\ncluster_1 = plagueSouthScania[plagueSouthScania['component'] == 1]\ncluster_2 = plagueSouthScania[plagueSouthScania['component'] == 2]\ncluster_3 = plagueSouthScania[plagueSouthScania['component'] == 3]  \ncluster_4 = plagueSouthScania[plagueSouthScania['component'] == 4]\ncluster_5 = plagueSouthScania[plagueSouthScania['component'] == 5]\ncluster_6 = plagueSouthScania[plagueSouthScania['component'] == 6]\ncluster_7 = plagueSouthScania[plagueSouthScania['component'] == 7]\ncluster_8 = plagueSouthScania[plagueSouthScania['component'] == 8]\ncluster_9 = plagueSouthScania[plagueSouthScania['component'] == 9]\ncluster_10 = plagueSouthScania[plagueSouthScania['component'] == 10]\ncluster_11 = plagueSouthScania[plagueSouthScania['component'] == 11]\ncluster_12 = plagueSouthScania[plagueSouthScania['component'] == 12]\ncluster_13 = plagueSouthScania[plagueSouthScania['component'] == 13]\ncluster_14 = plagueSouthScania[plagueSouthScania['component'] == 14]  \nbig_cluster = pd.concat([cluster_2,cluster_7], ignore_index=True) \nsmall_cluster = pd.concat([cluster_11, cluster_12], ignore_index=True)\n</pre> cluster_0 = plagueSouthScania[plagueSouthScania['component'] == 0] cluster_1 = plagueSouthScania[plagueSouthScania['component'] == 1] cluster_2 = plagueSouthScania[plagueSouthScania['component'] == 2] cluster_3 = plagueSouthScania[plagueSouthScania['component'] == 3]   cluster_4 = plagueSouthScania[plagueSouthScania['component'] == 4] cluster_5 = plagueSouthScania[plagueSouthScania['component'] == 5] cluster_6 = plagueSouthScania[plagueSouthScania['component'] == 6] cluster_7 = plagueSouthScania[plagueSouthScania['component'] == 7] cluster_8 = plagueSouthScania[plagueSouthScania['component'] == 8] cluster_9 = plagueSouthScania[plagueSouthScania['component'] == 9] cluster_10 = plagueSouthScania[plagueSouthScania['component'] == 10] cluster_11 = plagueSouthScania[plagueSouthScania['component'] == 11] cluster_12 = plagueSouthScania[plagueSouthScania['component'] == 12] cluster_13 = plagueSouthScania[plagueSouthScania['component'] == 13] cluster_14 = plagueSouthScania[plagueSouthScania['component'] == 14]   big_cluster = pd.concat([cluster_2,cluster_7], ignore_index=True)  small_cluster = pd.concat([cluster_11, cluster_12], ignore_index=True) <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[43]: Copied! <pre>class Initial_Model:\n    def __init__(self, gdf):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].values\n\n    def numPatches(self):\n        return len(self.patchNames())\n\n    def patchPop(self, column_pop: str = 'BEF1699'):\n        return self.gdf[column_pop].values\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Initial_Model:     def __init__(self, gdf):         self.gdf = gdf         self.n = self.numPatches()         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].values      def numPatches(self):         return len(self.patchNames())      def patchPop(self, column_pop: str = 'BEF1699'):         return self.gdf[column_pop].values      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()              <p>Generating the differential equations</p> In\u00a0[44]: Copied! <pre>SEASONALITY = False\n</pre> SEASONALITY = False In\u00a0[45]: Copied! <pre>def SIRD_model(y, t, model_parameters: tuple[dict]):\n    global SEASONALITY\n    parameters = model_parameters[0]\n    gdf = parameters['gdf']\n    beta = parameters['beta']\n    p = parameters['p']\n    gamma = parameters['gamma']\n    mu = parameters['mu']\n    N = parameters['N']\n    n = parameters['n']\n    \n    # Create a vector of variables\n    vars = y\n    def entryfun(i, offset): return vars[4 * i + offset]\n    # Create an array from the entry function\n    entry = np.array([[entryfun(i, j) for j in range(4)] for i in range(len(vars) // 4)])\n  \n    beta_matrix =  transmission_matrix_beta(gdf)\n    p_matrix = transmission_matrix_p(gdf)\n\n    # For including a seasonal transmission rate\n    if SEASONALITY:\n        bump_center = parameters['bump_center']\n        bump_width = parameters['bump_width']\n        bump_height = parameters['bump_height']\n        seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)\n    else:\n        seasonal_rate = lambda w : 0\n        \n    matrix = lambda w : (beta + seasonal_rate(w)) * beta_matrix + (p + seasonal_rate(w))  * p_matrix\n    sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 1], axis=1)\n\n    dS = -entry[:, 0] / N * sum_transmission(t)\n    dI = entry[:, 0] / N * sum_transmission(t) - gamma * entry[:, 1]\n    dR = gamma * (1 - mu) * entry[:, 1]\n    dD = gamma * mu * entry[:, 1]\n    derivatives = np.stack((dS, dI, dR, dD), axis=1).flatten()\n\n    return derivatives\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n    initConditions = [val for i in range(n) for val in (\n        init['S'][i], init['I'][i], init['R'][i], init['D'][i])]\n\n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    model = genInput['model']\n    output = scipy.odeint(func=model, y0=initConditions, t = t, args=((genInput,),), full_output=1)\n    # output is a tuple with two elements, the first element is the solution\n    # array and the second element is an object with additional information\n    solution = output[0]\n\n    indexVar = {'S': 0, 'I': 1, 'R': 2, 'D': 3}\n    def varSol(patch : int, var : str): \n        indice : int = 4*patch + indexVar[var]\n        return solution[:, indice]\n\n    return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()}\n</pre> def SIRD_model(y, t, model_parameters: tuple[dict]):     global SEASONALITY     parameters = model_parameters[0]     gdf = parameters['gdf']     beta = parameters['beta']     p = parameters['p']     gamma = parameters['gamma']     mu = parameters['mu']     N = parameters['N']     n = parameters['n']          # Create a vector of variables     vars = y     def entryfun(i, offset): return vars[4 * i + offset]     # Create an array from the entry function     entry = np.array([[entryfun(i, j) for j in range(4)] for i in range(len(vars) // 4)])        beta_matrix =  transmission_matrix_beta(gdf)     p_matrix = transmission_matrix_p(gdf)      # For including a seasonal transmission rate     if SEASONALITY:         bump_center = parameters['bump_center']         bump_width = parameters['bump_width']         bump_height = parameters['bump_height']         seasonal_rate = lambda w : seasonal_transmission_rate(w, bump_center, bump_width, bump_height)     else:         seasonal_rate = lambda w : 0              matrix = lambda w : (beta + seasonal_rate(w)) * beta_matrix + (p + seasonal_rate(w))  * p_matrix     sum_transmission = lambda w : np.sum(matrix(w) * entry[:, 1], axis=1)      dS = -entry[:, 0] / N * sum_transmission(t)     dI = entry[:, 0] / N * sum_transmission(t) - gamma * entry[:, 1]     dR = gamma * (1 - mu) * entry[:, 1]     dD = gamma * mu * entry[:, 1]     derivatives = np.stack((dS, dI, dR, dD), axis=1).flatten()      return derivatives  def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']     initConditions = [val for i in range(n) for val in (         init['S'][i], init['I'][i], init['R'][i], init['D'][i])]      T = genInput['T']     t = np.linspace(0, T, T+1)      model = genInput['model']     output = scipy.odeint(func=model, y0=initConditions, t = t, args=((genInput,),), full_output=1)     # output is a tuple with two elements, the first element is the solution     # array and the second element is an object with additional information     solution = output[0]      indexVar = {'S': 0, 'I': 1, 'R': 2, 'D': 3}     def varSol(patch : int, var : str):          indice : int = 4*patch + indexVar[var]         return solution[:, indice]      return {var: {patch: varSol(patch, var) for patch in range(n)} for var in indexVar.keys()} <p>Trying a small dataframe</p> In\u00a0[46]: Copied! <pre>cluster = cluster_2\ncluster = cluster.loc[(cluster['BeginPlaguePeriod']!= 'JAN 1711') \n                      &amp; (cluster['BeginPlaguePeriod']!= 'JUN 1711')\n                      &amp; (cluster['BeginPlaguePeriod']!= 'OCT 1711') \n                      &amp; (cluster['BeginPlaguePeriod']!= 'DEC 1711')  \n                      &amp; (cluster['BeginPlaguePeriod']!= 'FEB 1715')]\n</pre> cluster = cluster_2 cluster = cluster.loc[(cluster['BeginPlaguePeriod']!= 'JAN 1711')                        &amp; (cluster['BeginPlaguePeriod']!= 'JUN 1711')                       &amp; (cluster['BeginPlaguePeriod']!= 'OCT 1711')                        &amp; (cluster['BeginPlaguePeriod']!= 'DEC 1711')                         &amp; (cluster['BeginPlaguePeriod']!= 'FEB 1715')] <p>Defining the three connected groups that seem to spread the plague between them.</p> In\u00a0[47]: Copied! <pre>group1 = cluster[(cluster['ParishName'] == 'GENARP')\n                 #| (cluster['ParishName'] == 'GR\u00d6NBY')\n                 | (cluster['ParishName'] == 'VEBER\u00d6D')\n                 | (cluster['ParishName'] == 'VOMB') \n                 | (cluster['ParishName'] == 'SLIMMINGE')\n                 | (cluster['ParishName'] == 'SKURUP')\n                 | (cluster['ParishName'] == 'B\u00d6RRINGE')\n                 | (cluster['ParishName'] == 'SVEDALA')\n                 | (cluster['ParishName'] == 'SKABERSJ\u00d6')\n                 | (cluster['ParishName'] == 'T\u00d6RRINGE')\n                 | (cluster['ParishName'] == 'S\u00d6DRA AKARP')\n                 | (cluster['ParishName'] == 'ARRIE')\n]     \ngroup1 = group1.reset_index(drop=True)\n</pre> group1 = cluster[(cluster['ParishName'] == 'GENARP')                  #| (cluster['ParishName'] == 'GR\u00d6NBY')                  | (cluster['ParishName'] == 'VEBER\u00d6D')                  | (cluster['ParishName'] == 'VOMB')                   | (cluster['ParishName'] == 'SLIMMINGE')                  | (cluster['ParishName'] == 'SKURUP')                  | (cluster['ParishName'] == 'B\u00d6RRINGE')                  | (cluster['ParishName'] == 'SVEDALA')                  | (cluster['ParishName'] == 'SKABERSJ\u00d6')                  | (cluster['ParishName'] == 'T\u00d6RRINGE')                  | (cluster['ParishName'] == 'S\u00d6DRA AKARP')                  | (cluster['ParishName'] == 'ARRIE') ]      group1 = group1.reset_index(drop=True) In\u00a0[48]: Copied! <pre>group2 = cluster[(cluster['ParishName'] == 'YSTAD')\n                 | (cluster['ParishName'] == '\u00d6JA')\n                 | (cluster['ParishName'] == 'BROMMA')\n                 | (cluster['ParishName'] == 'BJ\u00c4RESJ\u00d6') \n                 | (cluster['ParishName'] == 'STORA K\u00d6PINGE')\n                 | (cluster['ParishName'] == 'VALLEBERGA')\n                 | ((cluster['ParishName'] == 'H\u00d6RUP') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))\n                 | ((cluster['ParishName'] == 'GLEMMINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))\n                 | (cluster['ParishName'] == 'INGELSTORP')\n                 | (cluster['ParishName'] == 'HAMMENH\u00d6G')\n                 | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))\n                 | (cluster['ParishName'] == 'HEDESKOGA')\n                 | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'NOV 1712'))\n]     \ngroup2 = group2.reset_index(drop=True)\n</pre> group2 = cluster[(cluster['ParishName'] == 'YSTAD')                  | (cluster['ParishName'] == '\u00d6JA')                  | (cluster['ParishName'] == 'BROMMA')                  | (cluster['ParishName'] == 'BJ\u00c4RESJ\u00d6')                   | (cluster['ParishName'] == 'STORA K\u00d6PINGE')                  | (cluster['ParishName'] == 'VALLEBERGA')                  | ((cluster['ParishName'] == 'H\u00d6RUP') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))                  | ((cluster['ParishName'] == 'GLEMMINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1712'))                  | (cluster['ParishName'] == 'INGELSTORP')                  | (cluster['ParishName'] == 'HAMMENH\u00d6G')                  | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))                  | (cluster['ParishName'] == 'HEDESKOGA')                  | ((cluster['ParishName'] == '\u00d6VRABY') &amp; (cluster['BeginPlaguePeriod']== 'NOV 1712')) ]      group2 = group2.reset_index(drop=True) In\u00a0[49]: Copied! <pre>group2.at[1, 'BeginPlaguePeriod'] = 'AUG 1712'\ngroup2.at[1, 'EndPlaguePeriod'] = np.NaN\n</pre> group2.at[1, 'BeginPlaguePeriod'] = 'AUG 1712' group2.at[1, 'EndPlaguePeriod'] = np.NaN In\u00a0[50]: Copied! <pre>group3 = cluster[(cluster['ParishName'] == 'S\u00d6DRA \u00c5SUM')\n                 | (cluster['ParishName'] == 'S\u00d6VDE')\n                 | (cluster['ParishName'] == 'BRANDSTARD')\n                 | (cluster['ParishName'] == 'BALDRINGE') \n                 | (cluster['ParishName'] == 'RAMS\u00c5SA')\n                 | (cluster['ParishName'] == 'VALLEBERGA')\n                 | (cluster['ParishName'] == 'TRYDE') \n                 | (cluster['ParishName'] == 'TRAN\u00c5S')\n                 | (cluster['ParishName'] == 'LOVESTAD')\n                 | (cluster['ParishName'] == 'VANSTAD')\n                 | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))\n                 | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1713'))\n                 | (cluster['ParishName'] == 'SK\u00c5RBY')\n                 | (cluster['ParishName'] == 'VILLIE')\n                 | (cluster['ParishName'] == 'BALK\u00c5KRA')\n                 | (cluster['ParishName'] == 'SNARESTAD')\n                 | (cluster['ParishName'] == 'VASTRA N\u00d6BBEL\u00d6V')\n                 | (cluster['ParishName'] == '\u00d6STRA VEMMENH\u00d6G')\n                 | (cluster['ParishName'] == 'SVENSTORP')\n\n]     \ngroup3 = group3.reset_index(drop=True)\n</pre> group3 = cluster[(cluster['ParishName'] == 'S\u00d6DRA \u00c5SUM')                  | (cluster['ParishName'] == 'S\u00d6VDE')                  | (cluster['ParishName'] == 'BRANDSTARD')                  | (cluster['ParishName'] == 'BALDRINGE')                   | (cluster['ParishName'] == 'RAMS\u00c5SA')                  | (cluster['ParishName'] == 'VALLEBERGA')                  | (cluster['ParishName'] == 'TRYDE')                   | (cluster['ParishName'] == 'TRAN\u00c5S')                  | (cluster['ParishName'] == 'LOVESTAD')                  | (cluster['ParishName'] == 'VANSTAD')                  | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'SEP 1712'))                  | ((cluster['ParishName'] == 'R\u00d6DDINGE') &amp; (cluster['BeginPlaguePeriod']== 'JUL 1713'))                  | (cluster['ParishName'] == 'SK\u00c5RBY')                  | (cluster['ParishName'] == 'VILLIE')                  | (cluster['ParishName'] == 'BALK\u00c5KRA')                  | (cluster['ParishName'] == 'SNARESTAD')                  | (cluster['ParishName'] == 'VASTRA N\u00d6BBEL\u00d6V')                  | (cluster['ParishName'] == '\u00d6STRA VEMMENH\u00d6G')                  | (cluster['ParishName'] == 'SVENSTORP')  ]      group3 = group3.reset_index(drop=True) In\u00a0[22]: Copied! <pre># Getting the centroid of each polygon for defining the transmission matrix\ngroup = get_centroid(add_Begin_End_days(sort_by_date(group1), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber'))\nmodel_input = Initial_Model(group)\n</pre> # Getting the centroid of each polygon for defining the transmission matrix group = get_centroid(add_Begin_End_days(sort_by_date(group1), 'new_format_BeginPlaguePeriod', 'new_format_EndPlaguePeriod', 'VictimsNumber')) model_input = Initial_Model(group) In\u00a0[25]: Copied! <pre># Model_test = {'model': SIRD_model,\n#               'init': {\n#                   'S': model_input.S0,\n#                   'I': model_input.I0,\n#                   'R': model_input.R0,\n#                   'D': model_input.D0,\n#               },  # defining the initial values for the model\n#               'gdf': group,  # defining the dataframe to work with\n#               'beta': 0.3,\n#               'p': 0.5,\n#               'bump_center': 0.1,\n#               'bump_width': 180.0,\n#               'bump_height': 30.0,\n#               'gamma': 0.32,\n#               'mu': 0.4,\n#               'N': model_input.patchPop(),\n#               'n': model_input.n,\n#               'T': model_input.maxDays()}\n\n# model_dict = generate_sol(Model_test)\n</pre> # Model_test = {'model': SIRD_model, #               'init': { #                   'S': model_input.S0, #                   'I': model_input.I0, #                   'R': model_input.R0, #                   'D': model_input.D0, #               },  # defining the initial values for the model #               'gdf': group,  # defining the dataframe to work with #               'beta': 0.3, #               'p': 0.5, #               'bump_center': 0.1, #               'bump_width': 180.0, #               'bump_height': 30.0, #               'gamma': 0.32, #               'mu': 0.4, #               'N': model_input.patchPop(), #               'n': model_input.n, #               'T': model_input.maxDays()}  # model_dict = generate_sol(Model_test) In\u00a0[27]: Copied! <pre># %matplotlib inline\n\n# # Set up the data to fit\n# beginTime = group['BeginDaysPlague'].values\n# endTime = group['EndDaysPlague'].values\n# deathData = group['VictimsNumber'].values\n\n# # Number of patches\n# n = Model_test['n']\n\n# # Set the figsize for each subplot\n# figsize_single_subplot = (8, 2)\n\n# # Calculate the total figure height based on the number of subplots and their height\n# fig_height = figsize_single_subplot[1] * n\n\n# # Create a figure and an array of axes with nrows=n and ncols=1\n# fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n#     figsize_single_subplot[0], fig_height), sharex=False)\n\n# # Convert axes to a list if it's not already one\n# if n == 1:\n#     axes = [axes]\n\n# # Plot model solution D for each patch\n# for i in range(n):\n#     if deathData[i] != 0 and endTime[i] != 0:\n#         # initial_position = beginTime[i]\n#         # final_position = endTime[i]\n#         # axes[i].plot(initial_position, 0, 'bo')\n#         # axes[i].plot(final_position,\n#         #              deathData[i], 'bo')\n#         axes[i].plot(model_dict['I'][i], color='darkred', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n#         axes[i].legend(loc='lower right')\n       \n#     else:\n#         axes[i].plot(model_dict['I'][i],\n#                      color='darkred', label=(model_input.patchNames()[i]))\n#         axes[i].set_ylabel('Cumulative Deaths')\n#         axes[i].legend(loc='upper right')\n        \n# # Adjust the layout to avoid overlapping\n# plt.tight_layout()\n# plt.show()\n</pre> # %matplotlib inline  # # Set up the data to fit # beginTime = group['BeginDaysPlague'].values # endTime = group['EndDaysPlague'].values # deathData = group['VictimsNumber'].values  # # Number of patches # n = Model_test['n']  # # Set the figsize for each subplot # figsize_single_subplot = (8, 2)  # # Calculate the total figure height based on the number of subplots and their height # fig_height = figsize_single_subplot[1] * n  # # Create a figure and an array of axes with nrows=n and ncols=1 # fig, axes = plt.subplots(nrows=n, ncols=1, figsize=( #     figsize_single_subplot[0], fig_height), sharex=False)  # # Convert axes to a list if it's not already one # if n == 1: #     axes = [axes]  # # Plot model solution D for each patch # for i in range(n): #     if deathData[i] != 0 and endTime[i] != 0: #         # initial_position = beginTime[i] #         # final_position = endTime[i] #         # axes[i].plot(initial_position, 0, 'bo') #         # axes[i].plot(final_position, #         #              deathData[i], 'bo') #         axes[i].plot(model_dict['I'][i], color='darkred', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths') #         axes[i].legend(loc='lower right')         #     else: #         axes[i].plot(model_dict['I'][i], #                      color='darkred', label=(model_input.patchNames()[i])) #         axes[i].set_ylabel('Cumulative Deaths') #         axes[i].legend(loc='upper right')          # # Adjust the layout to avoid overlapping # plt.tight_layout() # plt.show() <p>Defining the optimization problem:</p> In\u00a0[51]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters, beginTime, endTime, deathData):\n    beta, p, bump_center, bump_width, bump_height  = parameters\n    model_info = {'model': SIRD_model,\n                  'init': {\n                      'S': model_input.S0,\n                      'I': model_input.I0,\n                      'R': model_input.R0,\n                      'D': model_input.D0,\n                  },\n                  'gdf': group,\n                  # defining the initial values for the model\n                  'beta': beta,\n                  'p': p,\n                  'bump_center': bump_center,\n                  'bump_width': bump_width,\n                  'bump_height': bump_height,\n                  'gamma': 0.27,\n                  'mu': 0.04,\n                  'N': model_input.patchPop(),\n                  'n': model_input.n,\n                  'T': model_input.maxDays()}\n    model_sol = generate_sol(model_info)\n    totalError = 0\n    n = model_info['n']\n\n    # Calculate the error for each patch\n    errors = np.zeros(n)\n    for i in range(n):\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        if (deathData[i] != 0 and final_position != 0):\n            errors[i] =  ((model_sol['D'][i][initial_position] - 1.0)**2 + (\n                    model_sol['D'][i][final_position] - deathData[i])**2)\n        # elif (deathData[i] != 0 and final_position == 0):\n        #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (\n        #         (model_sol['D'][i][initial_position + 30] - deathData[i])**2)\n        # elif (deathData[i] == 0 and final_position != 0):\n        #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (\n        #         (model_sol['D'][i][final_position+1] - 0.0)**2)\n        else:\n            errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)\n\n    # Calculate the total error\n    totalError = np.sum(errors)\n    return totalError\n</pre> # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters, beginTime, endTime, deathData):     beta, p, bump_center, bump_width, bump_height  = parameters     model_info = {'model': SIRD_model,                   'init': {                       'S': model_input.S0,                       'I': model_input.I0,                       'R': model_input.R0,                       'D': model_input.D0,                   },                   'gdf': group,                   # defining the initial values for the model                   'beta': beta,                   'p': p,                   'bump_center': bump_center,                   'bump_width': bump_width,                   'bump_height': bump_height,                   'gamma': 0.27,                   'mu': 0.04,                   'N': model_input.patchPop(),                   'n': model_input.n,                   'T': model_input.maxDays()}     model_sol = generate_sol(model_info)     totalError = 0     n = model_info['n']      # Calculate the error for each patch     errors = np.zeros(n)     for i in range(n):         initial_position = beginTime[i]         final_position = endTime[i]         if (deathData[i] != 0 and final_position != 0):             errors[i] =  ((model_sol['D'][i][initial_position] - 1.0)**2 + (                     model_sol['D'][i][final_position] - deathData[i])**2)         # elif (deathData[i] != 0 and final_position == 0):         #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (         #         (model_sol['D'][i][initial_position + 30] - deathData[i])**2)         # elif (deathData[i] == 0 and final_position != 0):         #     errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2) + (         #         (model_sol['D'][i][final_position+1] - 0.0)**2)         else:             errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)      # Calculate the total error     totalError = np.sum(errors)     return totalError  <p>Parameter estimation</p> In\u00a0[52]: Copied! <pre># Set up the data to fit\nbeginTime = group['BeginDaysPlague'].values\nendTime = group['EndDaysPlague'].values\ndeathData = group['VictimsNumber'].values\n\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = 0.3\np_guess = 0.3\nbump_center_guess = 0.1\nbump_width_guess = 180.0\nbump_height_guess = 30.0\n\n\n# Minimize the objective function to obtain estimates for beta and gamma\nresult = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess ), args=(beginTime, endTime, deathData),\n                           method='L-BFGS-B'\n                           , bounds=[(0, 1), (0, 1), (0,1),(0, 365), (0, 100)]\n                           )\n\nbeta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x\n\nprint(\"beta = \", beta_estimated)\nprint(\"p = \", p_estimated)\n# print(\"gamma = \", gamma_estimated)\n# print(\"sigma = \", sigma_estimated)\n# print(\"mu = \", mu_estimated)\nprint(\"bump_center = \", bump_center_estimated)\nprint(\"bump_width = \", bump_width_estimated)\nprint(\"bump_height = \", bump_height_estimated)\n</pre> # Set up the data to fit beginTime = group['BeginDaysPlague'].values endTime = group['EndDaysPlague'].values deathData = group['VictimsNumber'].values   # Choose initial guesses for the parameters to fit beta_guess = 0.3 p_guess = 0.3 bump_center_guess = 0.1 bump_width_guess = 180.0 bump_height_guess = 30.0   # Minimize the objective function to obtain estimates for beta and gamma result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess ), args=(beginTime, endTime, deathData),                            method='L-BFGS-B'                            , bounds=[(0, 1), (0, 1), (0,1),(0, 365), (0, 100)]                            )  beta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x  print(\"beta = \", beta_estimated) print(\"p = \", p_estimated) # print(\"gamma = \", gamma_estimated) # print(\"sigma = \", sigma_estimated) # print(\"mu = \", mu_estimated) print(\"bump_center = \", bump_center_estimated) print(\"bump_width = \", bump_width_estimated) print(\"bump_height = \", bump_height_estimated) <pre>beta =  0.13036713438770448\np =  0.12141185269733443\nbump_center =  0.1\nbump_width =  180.0\nbump_height =  30.0\n</pre> <p>Results from estimations</p> In\u00a0[53]: Copied! <pre># Set up the data to fit\nbeginTime = group['BeginDaysPlague'].values\nendTime = group['EndDaysPlague'].values\ndeathData = group['VictimsNumber'].values\n\n#Estimated parameters for group1 without seasonality and p = 0 or 1 old objective function\n# beta_estimated =  0.620810007807504\n# p_estimated =  0.0009426896729423059\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n# #Estimated parameters for group1 without seasonality and p = 0 or 1 new objective function\n# beta_estimated =  0.6203556824484048\n# p_estimated =  0.0009204636123439495\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n\n\n# #Estimated parameters for group1 without seasonality and without gronby and p = 0 or 1 new objective function\n# beta_estimated =  0.6203556824484048\n# p_estimated =  0.0009204636123439495\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n\n#Estimated parameters for group1 without seasonality and p = 0 or 1 old objective function and SIR model\nbeta_estimated =  0.13036713438770448\np_estimated =  0.12141185269733443\nbump_center_estimated =  0.1\nbump_width_estimated =  180.0\nbump_height_estimated =  30.0\n\n#Estimated parameters for group2 without seasonality and p = 0 or 1\n# beta_estimated =  1.0\n# p_estimated =  0.05031279957398098\n# bump_center_estimated =  0.1\n# bump_width_estimated =  180.0\n# bump_height_estimated =  30.0\n</pre> # Set up the data to fit beginTime = group['BeginDaysPlague'].values endTime = group['EndDaysPlague'].values deathData = group['VictimsNumber'].values  #Estimated parameters for group1 without seasonality and p = 0 or 1 old objective function # beta_estimated =  0.620810007807504 # p_estimated =  0.0009426896729423059 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  # #Estimated parameters for group1 without seasonality and p = 0 or 1 new objective function # beta_estimated =  0.6203556824484048 # p_estimated =  0.0009204636123439495 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0    # #Estimated parameters for group1 without seasonality and without gronby and p = 0 or 1 new objective function # beta_estimated =  0.6203556824484048 # p_estimated =  0.0009204636123439495 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0  #Estimated parameters for group1 without seasonality and p = 0 or 1 old objective function and SIR model beta_estimated =  0.13036713438770448 p_estimated =  0.12141185269733443 bump_center_estimated =  0.1 bump_width_estimated =  180.0 bump_height_estimated =  30.0  #Estimated parameters for group2 without seasonality and p = 0 or 1 # beta_estimated =  1.0 # p_estimated =  0.05031279957398098 # bump_center_estimated =  0.1 # bump_width_estimated =  180.0 # bump_height_estimated =  30.0 <p>Substituting the estimated values into the model and solving it</p> In\u00a0[54]: Copied! <pre>model_estimation = {'model': SIRD_model,\n                    'init': {\n                        'S': model_input.S0,\n                        'I': model_input.I0,\n                        'R': model_input.R0,\n                        'D': model_input.D0,\n                    },\n                    'gdf': group,\n                    # defining the initial values for the model\n                    'beta': beta_estimated,\n                    'p': p_estimated,\n                    'bump_center': bump_center_estimated,\n                    'bump_width': bump_width_estimated,\n                    'bump_height': bump_height_estimated,\n                    'gamma': 0.27,\n                    'mu': 0.04,\n                    'N': model_input.patchPop(),\n                    'n': model_input.n,\n                    'T': model_input.maxDays()}\nmodel_solution = generate_sol(model_estimation)\n</pre> model_estimation = {'model': SIRD_model,                     'init': {                         'S': model_input.S0,                         'I': model_input.I0,                         'R': model_input.R0,                         'D': model_input.D0,                     },                     'gdf': group,                     # defining the initial values for the model                     'beta': beta_estimated,                     'p': p_estimated,                     'bump_center': bump_center_estimated,                     'bump_width': bump_width_estimated,                     'bump_height': bump_height_estimated,                     'gamma': 0.27,                     'mu': 0.04,                     'N': model_input.patchPop(),                     'n': model_input.n,                     'T': model_input.maxDays()} model_solution = generate_sol(model_estimation)  <p>Plotting the cumulative number of deaths to check the model fit</p> In\u00a0[55]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\n#tick_positions = group['BeginDaysPlague'].values\ntick_positions = group['BeginDaysPlague'].values\ntick_labels = group['BeginPlaguePeriod'].values\n#tick_labels = group['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values\n\n# Dictionary that reduces the plotting to those plots with data\n#lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]\n\n# Plot model solution D for each patch\nfor i in range(n):\n    if deathData[i] != 0 and endTime[i] != 0:\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        axes[i].plot(initial_position, 0, 'bo')\n        axes[i].plot(final_position,\n                     deathData[i], 'bo')\n        axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc = 'lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n    else:\n        axes[i].plot(model_solution['D'][i],\n                     color='orange', label=(model_input.patchNames()[i]))\n        axes[i].set_ylabel('Cumulative Deaths')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  #tick_positions = group['BeginDaysPlague'].values tick_positions = group['BeginDaysPlague'].values tick_labels = group['BeginPlaguePeriod'].values #tick_labels = group['BeginPlaguePeriod'].apply(lambda x: x.strftime('%b %Y') if not pd.isna(x) else None).values  # Dictionary that reduces the plotting to those plots with data #lookup_index = [1, 2, 4, 8, 9, 12, 16, 17]  # Plot model solution D for each patch for i in range(n):     if deathData[i] != 0 and endTime[i] != 0:         initial_position = beginTime[i]         final_position = endTime[i]         axes[i].plot(initial_position, 0, 'bo')         axes[i].plot(final_position,                      deathData[i], 'bo')         axes[i].plot(model_solution['D'][i], color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc = 'lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)     else:         axes[i].plot(model_solution['D'][i],                      color='orange', label=(model_input.patchNames()[i]))         axes[i].set_ylabel('Cumulative Deaths')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() <p>Plotting the daily deaths by parish</p> In\u00a0[56]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)] In\u00a0[57]: Copied! <pre>%matplotlib inline\n\n# Number of patches\nn = model_estimation['n']\n\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\ntick_positions = group['BeginDaysPlague'].values\ntick_labels = group['BeginPlaguePeriod'].values\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),\n                 color='blue', label=(model_input.patchNames()[i]))\n    axes[i].set_ylabel('Daily Deaths')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> %matplotlib inline  # Number of patches n = model_estimation['n']   # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  tick_positions = group['BeginDaysPlague'].values tick_labels = group['BeginPlaguePeriod'].values   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_solution, i, 0, model_input.maxDays()),                  color='blue', label=(model_input.patchNames()[i]))     axes[i].set_ylabel('Daily Deaths')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show() In\u00a0[94]: Copied! <pre>group\n</pre> group Out[94]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry color new_format_BeginPlaguePeriod new_format_EndPlaguePeriod component BeginDaysPlague EndDaysPlague centroid 0 SOUTHWEST BARA GENARP GENARPS 745 841 793.0 1 4 MAY 1712 AUG 1712 70 POLYGON ((4213130.585 3190804.364, 4213215.648... red 1712-05-01 1712-08-01 2 0 122 POINT (4209770.328045825 3188830.2528020446) 1 SOUTHWEST TORNA VEBER\u00d6D VEBER\u00d6DS 519 586 552.5 3 4 JUN 1712 AUG 1712 0 POLYGON ((4213130.585 3190804.364, 4212952.360... red 1712-06-01 1712-08-01 2 31 122 POINT (4213592.589770676 3194680.188944844) 2 SOUTHWEST VEMMENH\u00d6G SLIMMINGE SLIMMINGE 550 621 585.5 3 4 JUN 1712 JUN 1712 0 POLYGON ((4214392.138 3182887.177, 4214363.468... red 1712-06-01 1712-06-01 2 31 60 POINT (4216637.909781777 3185001.1490382226) 3 SOUTHWEST VEMMENH\u00d6G SKURUP SKURUPS 547 618 582.5 2 4 JUL 1712 JUL 1712 0 POLYGON ((4214392.138 3182887.177, 4214576.093... red 1712-07-01 1712-07-01 2 61 91 POINT (4214330.468931566 3180670.917955833) 4 SOUTHWEST BARA SKABERSJ\u00d6 SKABERSJ\u00d6 485 548 516.5 1 4 AUG 1712 NOV 1712 0 POLYGON ((4193435.714 3182066.598, 4193378.045... red 1712-08-01 1712-11-01 2 92 213 POINT (4194529.083288759 3185782.2929709186) 5 SOUTHWEST OXIE SVEDALA SVEDALA 450 508 479.0 1 5 AUG 1712 NOV 1712 15 POLYGON ((4200801.082 3181487.130, 4200913.270... red 1712-08-01 1712-11-01 2 92 213 POINT (4198872.554955849 3181584.7684801435) 6 SOUTHWEST VEMMENH\u00d6G B\u00d6RRINGE B\u00d6RRINGE 359 405 382.0 2 5 AUG 1712 AUG 1712 5 POLYGON ((4204085.740 3178127.746, 4203815.385... red 1712-08-01 1712-08-01 2 92 122 POINT (4206443.916011617 3182376.392602373) 7 MIDDLE F\u00c4RS VOMB VOMBS 269 303 286.0 1 5 SEP 1712 NOV 1712 9 POLYGON ((4214955.556 3199380.156, 4214967.070... red 1712-09-01 1712-11-01 2 123 213 POINT (4217539.842033497 3199629.814443183) 8 SOUTHWEST OXIE T\u00d6RRINGE T\u00d6RRINGE 93 105 99.0 1 5 SEP 1712 NOV 1712 0 POLYGON ((4192313.360 3184522.040, 4192406.262... red 1712-09-01 1712-11-01 2 123 213 POINT (4191781.1845232085 3183045.723210506) 9 SOUTHWEST OXIE ARRIE ARRIE 113 127 120.0 2 4 OCT 1712 DEC 1712 15 POLYGON ((4187185.738 3182794.083, 4188296.110... red 1712-10-01 1712-12-01 2 153 244 POINT (4188980.1080080583 3182035.982453025)"},{"location":"PlagueProject/SeasMetapopModelPlague/","title":"SeasonalMetapopModelPlague","text":"In\u00a0[1]: Copied! <pre>#Python 3.11.2\n#Import packages\nimport scipy.integrate as scipy\nimport numpy as np\nimport pylab as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport json # for pretty printing\n</pre> #Python 3.11.2 #Import packages import scipy.integrate as scipy import numpy as np import pylab as pl import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from collections import defaultdict import json # for pretty printing <p>Initializing the population size, the initial conditions and the number of patches:</p> In\u00a0[2]: Copied! <pre># Number of patches\nn = 19\n\n # vector of population sizes with lenght n\nN = np.array([204  # Bromma\n              , 217  # Oja\n              , 1895  # S Maria Ystad 1749\n              , 554  # Valleberga\n              , 693  # S Kopinge\n              , 403  # Horups\n              , 582  # Bj\u00e4resj\u00f6 1780\n              , 716  # Villie 1749\n              , 418  # Sn\u00e5restad 1775\n              , 519  # Sk\u00e5rby 1749\n              , 262  # Hammenh\u00f6gs 1749\n              , 560  # Glemminge 1775\n              , 236  # Balk\u00e5kra 1775\n              , 334  # Baldringe 1749\n              , 299  # Ovraby\n              , 761  # S\u00f6vestads 1749\n              , 776  # L\u00f6derups 1749\n              , 951  # Borrby 1775\n              , 358  # Tosterups 1775\n              ]) \n\n# Initial conditions for each patch\n\nI0 = np.zeros(n)  # vector of initial infecteds with lenght n\nI0[0] = 1.0  # the first element of the I0 vector is set to 1\n\nS0 = np.zeros(n)  # vector of initial susceptibles with lenght n\nfor i in range(n):\n    S0[i] = N[i] - I0[i]\n\nR0 = np.zeros(n)  # vector of initial removeds with lenght n\nD0 = np.zeros(n)  # vector of initial deaths with lenght n\n\n#print(S0,I0,R0,D0)\n</pre> # Number of patches n = 19   # vector of population sizes with lenght n N = np.array([204  # Bromma               , 217  # Oja               , 1895  # S Maria Ystad 1749               , 554  # Valleberga               , 693  # S Kopinge               , 403  # Horups               , 582  # Bj\u00e4resj\u00f6 1780               , 716  # Villie 1749               , 418  # Sn\u00e5restad 1775               , 519  # Sk\u00e5rby 1749               , 262  # Hammenh\u00f6gs 1749               , 560  # Glemminge 1775               , 236  # Balk\u00e5kra 1775               , 334  # Baldringe 1749               , 299  # Ovraby               , 761  # S\u00f6vestads 1749               , 776  # L\u00f6derups 1749               , 951  # Borrby 1775               , 358  # Tosterups 1775               ])   # Initial conditions for each patch  I0 = np.zeros(n)  # vector of initial infecteds with lenght n I0[0] = 1.0  # the first element of the I0 vector is set to 1  S0 = np.zeros(n)  # vector of initial susceptibles with lenght n for i in range(n):     S0[i] = N[i] - I0[i]  R0 = np.zeros(n)  # vector of initial removeds with lenght n D0 = np.zeros(n)  # vector of initial deaths with lenght n  #print(S0,I0,R0,D0) In\u00a0[3]: Copied! <pre># Defining the transmission rate matrix as a function of two parameters\n\ndef TransmissionRateMatrix(beta: float, p: float)-&gt; np.ndarray:\n    return(\n        np.array([\n            [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],\n            [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],\n            [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],\n            [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],\n            [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],\n            [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],\n            [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],\n            [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],\n            [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],\n            [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],\n            [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],\n            [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]\n     ] )\n    )\n    \nprint(TransmissionRateMatrix(2, 1)[0][2])\nsize = TransmissionRateMatrix(2, 1).shape\nprint(size)\n</pre> # Defining the transmission rate matrix as a function of two parameters  def TransmissionRateMatrix(beta: float, p: float)-&gt; np.ndarray:     return(         np.array([             [beta, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, p, 0, 0, 0],             [p, beta, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, p, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0],             [0, 0, p, 0, beta, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p],             [0, 0, 0, p, 0, beta, 0, 0, 0, 0, p, p, 0, 0, 0, 0, p, p, 0],             [p, 0, 0, 0, 0, 0, beta, 0, 0, p, 0, 0, p, 0, 0, p, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, p, p, 0, beta, 0, 0, p, 0, 0, p, 0, 0, 0],             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p, 0],             [0, 0, 0, p, p, p, 0, 0, 0, 0, 0, beta, 0, 0, 0, 0, 0, 0, p],             [0, 0, 0, 0, 0, 0, p, 0, p, p, 0, 0, beta, 0, 0, 0, 0, 0, 0],             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, p, 0, 0, 0],             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, 0, 0, 0, p],             [p, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, p, 0, beta, 0, 0, 0],             [0, 0, 0, p, 0, p, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, beta, p, 0],             [0, 0, 0, 0, 0, p, 0, 0, 0, 0, p, 0, 0, 0, 0, 0, p, beta, 0],             [0, 0, 0, 0, p, 0, 0, 0, 0, 0, 0, p, 0, 0, p, 0, 0, 0, beta]      ] )     )      print(TransmissionRateMatrix(2, 1)[0][2]) size = TransmissionRateMatrix(2, 1).shape print(size)  <pre>0\n(19, 19)\n</pre> <p>Generating the seasonality function</p> In\u00a0[4]: Copied! <pre>def seasonal_transmission_rate(t, beta_0, beta_1, a, frequency=1/(365*1.0)):\n    return beta_0 * (1 + beta_1 * np.sin((2 * np.pi)* frequency * t - a))\n    \n\n# Generate some example data with seasonality\nt = np.arange(0, 365,1)\nbeta_0 = 6.91\nbeta_1 = 1.15\na = 1.78\n\n\n# Plot beta \nplt.plot(t, seasonal_transmission_rate(t, beta_0, beta_1, a))\n\ntick_positions = [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360]\ntick_labels = [\"Jan 1712\", \"Feb 1712\", \"Mar 1712\",\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\"\n              , \"Nov 1712\", \"Dec 1712\", \"Jan 1713\"\n              ]\nplt.xticks(tick_positions, tick_labels, rotation=45)\nplt.show()\n</pre>  def seasonal_transmission_rate(t, beta_0, beta_1, a, frequency=1/(365*1.0)):     return beta_0 * (1 + beta_1 * np.sin((2 * np.pi)* frequency * t - a))       # Generate some example data with seasonality t = np.arange(0, 365,1) beta_0 = 6.91 beta_1 = 1.15 a = 1.78   # Plot beta  plt.plot(t, seasonal_transmission_rate(t, beta_0, beta_1, a))  tick_positions = [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360] tick_labels = [\"Jan 1712\", \"Feb 1712\", \"Mar 1712\",\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\"               , \"Nov 1712\", \"Dec 1712\", \"Jan 1713\"               ] plt.xticks(tick_positions, tick_labels, rotation=45) plt.show()   <p>Generating the differential equations</p> In\u00a0[5]: Copied! <pre>def SIRD_model(y, t,\n               model_parameters: tuple[dict]):\n\n    parameters = model_parameters[0]\n    beta: float = parameters['beta'] \n    p: float = parameters['p']\n    gamma: float = parameters['gamma']\n    mu: float = parameters['mu']\n    N = parameters['N']\n    n: int = parameters['n']\n\n    S = defaultdict(float)\n    I = defaultdict(float)\n    R = defaultdict(float)\n    D = defaultdict(float)\n\n    vars = tuple(sum([[S[i], I[i], R[i], D[i]] for i in range(n)], []))\n    vars = y\n\n   # Choosing the corresponding output for each subpopulation\n    def entryS(i):\n        return vars[4 * i]\n\n    def entryI(i):\n        return vars[4 * i + 1]\n\n    def entryR(i):\n        return vars[4 * i + 2]\n\n    def entryD(i):\n        return vars[4 * i + 3]\n\n    # Initializando the directory for each subpopulation\n    dS = {}\n    dI = {}\n    dR = {}\n    dD = {}\n\n    # Defining the seasonal transmission rate\n    def seasonal_transmission_rate(t, beta_0 = 1.5, beta_1 = 0.1, a = 120, frequency = 1/(360*1.0)):\n        return beta_0 * (1 + beta_1 * np.sin((2 * np.pi) * frequency * t - a))\n\n    # Defining the differential equations for each subpopulation\n    for i in range(n):\n        dS[i] = (- (entryS(i) / (N[i]*1.0)) *\n                 sum(TransmissionRateMatrix(beta, p)[i][j] *\n                    seasonal_transmission_rate(t) * entryI(j) \n                    for j in range(n))\n                 )\n        dI[i] = ((entryS(i) / (N[i]*1.0)) * \n                  sum(TransmissionRateMatrix(beta, p)[i][j] *\n                      seasonal_transmission_rate(t) * entryI(j) \n                      for j in range(n)) \n                  - gamma * entryI(i)   \n                )\n        dR[i] = gamma * mu * entryI(i)\n        dD[i] = gamma * (1 - mu) * entryI(i)\n\n    derivates = sum([[dS[i], dI[i], dR[i], dD[i]] for i in range(n)], [])\n    return derivates   # For odeint\n\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    model = genInput['model']\n    init = genInput['init']\n    beta = genInput['beta']\n    gamma = genInput['gamma']\n    mu = genInput['mu']\n    n = genInput['n']\n    p = genInput['p']\n    N = genInput['N']\n    T = genInput['T']\n\n    # Initial conditions vector for the metapopulation model. len(initConditions) = 4*n\n    initConditions = tuple(sum(\n        [[init['S'][i], init['I'][i], init['R'][i], init['D'][i]] for i in range(n)], []))\n\n    # Time vector\n    t = np.linspace(0, T, T+1)\n\n    # Computing the numerical solution\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {}\n    indexVar['S'] = 0\n    indexVar['I'] = 1\n    indexVar['R'] = 2\n    indexVar['D'] = 3\n\n    def varSol(patch, var):\n        return solution[:, 4*patch + indexVar[var]]\n\n    return {'S': {patch: varSol(patch, 'S') for patch in range(n)},\n            'I': {patch: varSol(patch, 'I') for patch in range(n)},\n            'R': {patch: varSol(patch, 'R') for patch in range(n)},\n            'D': {patch: varSol(patch, 'D') for patch in range(n)},\n            'N': N,\n            'init': init,\n            'beta': beta,\n            'gamma': gamma,\n            'mu': mu,\n            't': t,\n            'n': n,\n            'p': p,\n            'model': model,\n            'raw_solution': solution}\n\n\nModel_test = {'model': SIRD_model,\n              'init': {\n                  'S': S0,\n                  'I': I0,\n                  'R': R0,\n                  'D': D0,\n              },  # defining the initial values for the model\n              'beta': 0.35,\n              'gamma': 0.32,\n              'mu': 0.6,\n              'N': N,\n              'n': 19,\n              'p': 0.0000002,\n              'T': 600}\n\nmodel_dict = generate_sol(Model_test)\n</pre> def SIRD_model(y, t,                model_parameters: tuple[dict]):      parameters = model_parameters[0]     beta: float = parameters['beta']      p: float = parameters['p']     gamma: float = parameters['gamma']     mu: float = parameters['mu']     N = parameters['N']     n: int = parameters['n']      S = defaultdict(float)     I = defaultdict(float)     R = defaultdict(float)     D = defaultdict(float)      vars = tuple(sum([[S[i], I[i], R[i], D[i]] for i in range(n)], []))     vars = y     # Choosing the corresponding output for each subpopulation     def entryS(i):         return vars[4 * i]      def entryI(i):         return vars[4 * i + 1]      def entryR(i):         return vars[4 * i + 2]      def entryD(i):         return vars[4 * i + 3]      # Initializando the directory for each subpopulation     dS = {}     dI = {}     dR = {}     dD = {}      # Defining the seasonal transmission rate     def seasonal_transmission_rate(t, beta_0 = 1.5, beta_1 = 0.1, a = 120, frequency = 1/(360*1.0)):         return beta_0 * (1 + beta_1 * np.sin((2 * np.pi) * frequency * t - a))      # Defining the differential equations for each subpopulation     for i in range(n):         dS[i] = (- (entryS(i) / (N[i]*1.0)) *                  sum(TransmissionRateMatrix(beta, p)[i][j] *                     seasonal_transmission_rate(t) * entryI(j)                      for j in range(n))                  )         dI[i] = ((entryS(i) / (N[i]*1.0)) *                    sum(TransmissionRateMatrix(beta, p)[i][j] *                       seasonal_transmission_rate(t) * entryI(j)                        for j in range(n))                    - gamma * entryI(i)                    )         dR[i] = gamma * mu * entryI(i)         dD[i] = gamma * (1 - mu) * entryI(i)      derivates = sum([[dS[i], dI[i], dR[i], dD[i]] for i in range(n)], [])     return derivates   # For odeint   def generate_sol(genInput: dict) -&gt; dict:     model = genInput['model']     init = genInput['init']     beta = genInput['beta']     gamma = genInput['gamma']     mu = genInput['mu']     n = genInput['n']     p = genInput['p']     N = genInput['N']     T = genInput['T']      # Initial conditions vector for the metapopulation model. len(initConditions) = 4*n     initConditions = tuple(sum(         [[init['S'][i], init['I'][i], init['R'][i], init['D'][i]] for i in range(n)], []))      # Time vector     t = np.linspace(0, T, T+1)      # Computing the numerical solution     solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {}     indexVar['S'] = 0     indexVar['I'] = 1     indexVar['R'] = 2     indexVar['D'] = 3      def varSol(patch, var):         return solution[:, 4*patch + indexVar[var]]      return {'S': {patch: varSol(patch, 'S') for patch in range(n)},             'I': {patch: varSol(patch, 'I') for patch in range(n)},             'R': {patch: varSol(patch, 'R') for patch in range(n)},             'D': {patch: varSol(patch, 'D') for patch in range(n)},             'N': N,             'init': init,             'beta': beta,             'gamma': gamma,             'mu': mu,             't': t,             'n': n,             'p': p,             'model': model,             'raw_solution': solution}   Model_test = {'model': SIRD_model,               'init': {                   'S': S0,                   'I': I0,                   'R': R0,                   'D': D0,               },  # defining the initial values for the model               'beta': 0.35,               'gamma': 0.32,               'mu': 0.6,               'N': N,               'n': 19,               'p': 0.0000002,               'T': 600}  model_dict = generate_sol(Model_test)  <p>Plotting the solutions</p> In\u00a0[6]: Copied! <pre>#Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch] # list of floats         \n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n                for t in range(T_inf, T_sup)]\n\n# Plotting the solution\ndef plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):\n\n    for key in state:\n        if key not in model:\n            raise ValueError(f\"Invalid state: {key}\")\n        for i in range(model['n']):\n            plt.plot(model['t'], model[key][i], label=f'{key} {i}')\n    plt.xlabel('Time')\n    plt.ylabel('Infectious')\n    plt.title('SIRD model')\n    #plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))\n    plt.show()\n\n#Plotting the infected and daily deaths for the first five patches   \n\ntick_positions = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]\ntick_labels = [\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\"\n               , \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\"\n               , \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"\n              ]\n\nm = 19 # number of patches in the model\nplt.subplot(211)\nfor i in range(5):\n    plt.plot(model_dict['I'][i], label=('patch %s' %(i+1)))\nplt.xlabel('Time')\nplt.ylabel('Infectious')\npl.legend(loc=1)\n#plt.xticks(tick_positions, tick_labels, rotation=45)\nplt.subplot(212)\nfor i in range(5):\n    plt.plot(daily_deaths(model_dict, i, 0, 600), label=('patch %s' %(i+1)))\nplt.xlabel('Time')\nplt.ylabel('Daily Deaths')\npl.legend(loc=1)\n#plt.xticks(tick_positions, tick_labels, rotation=45)\nplt.show()\n</pre> #Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch] # list of floats              return [cumulative_deaths[t+1] - cumulative_deaths[t]                 for t in range(T_inf, T_sup)]  # Plotting the solution def plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):      for key in state:         if key not in model:             raise ValueError(f\"Invalid state: {key}\")         for i in range(model['n']):             plt.plot(model['t'], model[key][i], label=f'{key} {i}')     plt.xlabel('Time')     plt.ylabel('Infectious')     plt.title('SIRD model')     #plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))     plt.show()  #Plotting the infected and daily deaths for the first five patches     tick_positions = [30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570] tick_labels = [\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\"                , \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\"                , \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"               ]  m = 19 # number of patches in the model plt.subplot(211) for i in range(5):     plt.plot(model_dict['I'][i], label=('patch %s' %(i+1))) plt.xlabel('Time') plt.ylabel('Infectious') pl.legend(loc=1) #plt.xticks(tick_positions, tick_labels, rotation=45) plt.subplot(212) for i in range(5):     plt.plot(daily_deaths(model_dict, i, 0, 600), label=('patch %s' %(i+1))) plt.xlabel('Time') plt.ylabel('Daily Deaths') pl.legend(loc=1) #plt.xticks(tick_positions, tick_labels, rotation=45) plt.show()   In\u00a0[7]: Copied! <pre>plt.subplot(211)\nfor i in range(13,19):\n    plt.plot(daily_deaths(model_dict, i, 0, 570), label=('patch %s' %(i+1)))\nplt.xlabel('Time')\nplt.ylabel('Daily Deaths')\npl.legend(loc=1)\nplt.subplot(212)\nfor i in range(13,19):\n    plt.plot(model_dict['D'][i], label=('patch %s' %(i+1)))\nplt.xlabel('Time')\nplt.ylabel('Cumulative Deaths')\npl.legend(loc=1)\nplt.show()\n</pre> plt.subplot(211) for i in range(13,19):     plt.plot(daily_deaths(model_dict, i, 0, 570), label=('patch %s' %(i+1))) plt.xlabel('Time') plt.ylabel('Daily Deaths') pl.legend(loc=1) plt.subplot(212) for i in range(13,19):     plt.plot(model_dict['D'][i], label=('patch %s' %(i+1))) plt.xlabel('Time') plt.ylabel('Cumulative Deaths') pl.legend(loc=1) plt.show()"},{"location":"PlagueProject/SlowSeasonMetapopModelPlague/","title":"SlowSeasonMetapopModelPlague","text":"In\u00a0[32]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline  In\u00a0[33]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3  <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[34]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py  <p>Processing the data</p> In\u00a0[35]: Copied! <pre># Set the working directory for private files\ndata_private_folder = \"data/private\"\nsoutheast_path = os.path.join(data_private_folder, 'southeastScania.csv')\nsoutheastScania = pd.read_csv(southeast_path, sep=',')\n\n# Convert WKT (Well-Known Text) geometry to Shapely geometry\nsoutheastScania['geometry'] = southeastScania['geometry'].apply(wkt.loads)\n\n# Create a GeoDataFrame from the DataFrame\nsoutheastScania = gpd.GeoDataFrame(southeastScania, geometry='geometry')\ntype(southeastScania)\n</pre> # Set the working directory for private files data_private_folder = \"data/private\" southeast_path = os.path.join(data_private_folder, 'southeastScania.csv') southeastScania = pd.read_csv(southeast_path, sep=',')  # Convert WKT (Well-Known Text) geometry to Shapely geometry southeastScania['geometry'] = southeastScania['geometry'].apply(wkt.loads)  # Create a GeoDataFrame from the DataFrame southeastScania = gpd.GeoDataFrame(southeastScania, geometry='geometry') type(southeastScania)  Out[35]: <pre>geopandas.geodataframe.GeoDataFrame</pre> In\u00a0[36]: Copied! <pre># replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe\nsoutheastScania = southeastScania.replace(['UNDEFINED', '?'], np.nan)\n</pre> # replaced all the occurences of 'UNDEFINED' and '?' with np.nan in our dataframe southeastScania = southeastScania.replace(['UNDEFINED', '?'], np.nan)  In\u00a0[37]: Copied! <pre>southeastScania['BeginPlaguePeriod'] = pd.to_datetime(\n    southeastScania['BeginPlaguePeriod'], format='%b %Y')\nsoutheastScania['EndPlaguePeriod'] = pd.to_datetime(\n    southeastScania['EndPlaguePeriod'], format='%b %Y')\nsoutheastScania.sort_values(by=['BeginPlaguePeriod'],   # Row or columns names to sort by\n                            axis=0,       # Sort Rows axis = 0\n                            ascending=True,  # Sort ascending or descending?\n                            # Modify the DataFrame in place (do not create a new object)\n                            inplace=True\n                            )\n\nsoutheastScania.reset_index(drop=True, inplace=True)\n</pre> southeastScania['BeginPlaguePeriod'] = pd.to_datetime(     southeastScania['BeginPlaguePeriod'], format='%b %Y') southeastScania['EndPlaguePeriod'] = pd.to_datetime(     southeastScania['EndPlaguePeriod'], format='%b %Y') southeastScania.sort_values(by=['BeginPlaguePeriod'],   # Row or columns names to sort by                             axis=0,       # Sort Rows axis = 0                             ascending=True,  # Sort ascending or descending?                             # Modify the DataFrame in place (do not create a new object)                             inplace=True                             )  southeastScania.reset_index(drop=True, inplace=True)  In\u00a0[38]: Copied! <pre># Create a new column called \"BeginDaysPlague\"\nsoutheastScania[\"BeginDaysPlague\"] = southeastScania.apply(\n    # axis = 1 means apply function to each row\n    lambda row: begin_days_between(southeastScania[\"BeginPlaguePeriod\"].iloc[0], row[\"BeginPlaguePeriod\"]), axis=1\n)\n\n# Create a new column called \"EndDaysPlague\"\nsoutheastScania['EndDaysPlague'] = southeastScania.apply(lambda row: end_days_between(\n    southeastScania['BeginPlaguePeriod'].iloc[0], row['EndPlaguePeriod']) if pd.notna(row['EndPlaguePeriod']) else None, axis=1)\n</pre> # Create a new column called \"BeginDaysPlague\" southeastScania[\"BeginDaysPlague\"] = southeastScania.apply(     # axis = 1 means apply function to each row     lambda row: begin_days_between(southeastScania[\"BeginPlaguePeriod\"].iloc[0], row[\"BeginPlaguePeriod\"]), axis=1 )  # Create a new column called \"EndDaysPlague\" southeastScania['EndDaysPlague'] = southeastScania.apply(lambda row: end_days_between(     southeastScania['BeginPlaguePeriod'].iloc[0], row['EndPlaguePeriod']) if pd.notna(row['EndPlaguePeriod']) else None, axis=1)  In\u00a0[39]: Copied! <pre># Replace NaN values with a value in some columns (e.g., 0)\nsoutheastScania['BeginDaysPlague'].fillna(0, inplace=True)\nsoutheastScania['EndDaysPlague'].fillna(0, inplace=True)\nsoutheastScania['VictimsNumber'].fillna(0, inplace=True)\n\n# Changing the type of some columns from float to integer\nsoutheastScania['BeginDaysPlague'] = southeastScania['BeginDaysPlague'].astype(\n    int)\nsoutheastScania['EndDaysPlague'] = southeastScania['EndDaysPlague'].astype(int)\nsoutheastScania['VictimsNumber'] = southeastScania['VictimsNumber'].astype(int)\n</pre> # Replace NaN values with a value in some columns (e.g., 0) southeastScania['BeginDaysPlague'].fillna(0, inplace=True) southeastScania['EndDaysPlague'].fillna(0, inplace=True) southeastScania['VictimsNumber'].fillna(0, inplace=True)  # Changing the type of some columns from float to integer southeastScania['BeginDaysPlague'] = southeastScania['BeginDaysPlague'].astype(     int) southeastScania['EndDaysPlague'] = southeastScania['EndDaysPlague'].astype(int) southeastScania['VictimsNumber'] = southeastScania['VictimsNumber'].astype(int)  <p>Initializing the number of patches (n), the population size (patchPop) and the initial conditions for each variable (S0, E0, I0, R0, D0):</p> In\u00a0[40]: Copied! <pre>class Input_Model:\n    def __init__(self, gdf):\n        self.gdf = gdf\n        self.n = self.numPatches()\n        self.E0 = np.zeros(self.n)\n        self.I0 = np.zeros(self.n)\n        self.I0[0] = 1.0\n        self.R0 = np.zeros(self.n)\n        self.D0 = np.zeros(self.n)\n        self.S0 = np.zeros(self.n)\n        for i in range(self.n):\n            self.S0[i] = self.patchPop()[i] - self.E0[i] - \\\n                self.I0[i] - self.R0[i]\n\n    def patchNames(self, column_name: str = 'ParishName'):\n        return self.gdf[column_name].values\n\n    def numPatches(self):\n        return len(self.patchNames())\n\n    def patchPop(self, column_pop: str = 'BEF1699'):\n        return self.gdf[column_pop].values\n\n    def maxDays(self, column_EndDays: str = 'EndDaysPlague'):\n        return self.gdf[column_EndDays].max()\n</pre> class Input_Model:     def __init__(self, gdf):         self.gdf = gdf         self.n = self.numPatches()         self.E0 = np.zeros(self.n)         self.I0 = np.zeros(self.n)         self.I0[0] = 1.0         self.R0 = np.zeros(self.n)         self.D0 = np.zeros(self.n)         self.S0 = np.zeros(self.n)         for i in range(self.n):             self.S0[i] = self.patchPop()[i] - self.E0[i] - \\                 self.I0[i] - self.R0[i]      def patchNames(self, column_name: str = 'ParishName'):         return self.gdf[column_name].values      def numPatches(self):         return len(self.patchNames())      def patchPop(self, column_pop: str = 'BEF1699'):         return self.gdf[column_pop].values      def maxDays(self, column_EndDays: str = 'EndDaysPlague'):         return self.gdf[column_EndDays].max()  <p>Getting the centroid of each polygon for defining the transmission matrix.</p> In\u00a0[41]: Copied! <pre>southeastScania = get_centroid(southeastScania)\n</pre> southeastScania = get_centroid(southeastScania)  In\u00a0[42]: Copied! <pre># Defining the seasonal transmission rate\ndef gaussian(x, mu, sigma):\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n\n\ndef seasonal_transmission_rate(t, bump_center, bump_width, bump_height):\n    return bump_height * gaussian(t % 365, bump_center, bump_width) + bump_height * gaussian(t % 365 - 365, bump_center, bump_width) + bump_height * gaussian(t % 365 + 365, bump_center, bump_width)\n</pre> # Defining the seasonal transmission rate def gaussian(x, mu, sigma):     return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))   def seasonal_transmission_rate(t, bump_center, bump_width, bump_height):     return bump_height * gaussian(t % 365, bump_center, bump_width) + bump_height * gaussian(t % 365 - 365, bump_center, bump_width) + bump_height * gaussian(t % 365 + 365, bump_center, bump_width)  In\u00a0[43]: Copied! <pre>def transmission_matrix(gdf: gpd.GeoDataFrame, t, beta: float, p: float, bump_center: float, bump_width: float, bump_height: float, column_geometry: str = 'geometry', column_centroid: str = 'centroid', column_pop: str = 'BEF1699', column_name: str = 'ParishName'):\n    m = len(gdf)\n    matrix = np.zeros((m, m))\n    for i in range(m):\n        polygon_i = gdf.iloc[i][column_geometry]\n        centroid_i = gdf.loc[i][column_centroid]\n        pop_i = gdf.iloc[i][column_pop]\n        name_i = gdf.iloc[i][column_name]\n        for j in range(m):\n            polygon_j = gdf.iloc[j][column_geometry]\n            centroid_j = gdf.loc[j][column_centroid]\n            pop_j = gdf.iloc[j][column_pop]\n            name_j = gdf.iloc[j][column_name]\n            if i == j or name_i == name_j:\n                matrix[i][j] = beta + \\\n                    seasonal_transmission_rate(\n                        t, bump_center, bump_width, bump_height)\n            else:\n                if i != j and polygon_i != polygon_j and polygon_i.intersects(polygon_j):\n                    distance_i_j = centroid_i.distance(centroid_j)\n                    matrix[i][j] = p * (pop_i*pop_j) / ((distance_i_j)**2)\n                else:\n                    matrix[i][j] = 0\n    return matrix\n</pre> def transmission_matrix(gdf: gpd.GeoDataFrame, t, beta: float, p: float, bump_center: float, bump_width: float, bump_height: float, column_geometry: str = 'geometry', column_centroid: str = 'centroid', column_pop: str = 'BEF1699', column_name: str = 'ParishName'):     m = len(gdf)     matrix = np.zeros((m, m))     for i in range(m):         polygon_i = gdf.iloc[i][column_geometry]         centroid_i = gdf.loc[i][column_centroid]         pop_i = gdf.iloc[i][column_pop]         name_i = gdf.iloc[i][column_name]         for j in range(m):             polygon_j = gdf.iloc[j][column_geometry]             centroid_j = gdf.loc[j][column_centroid]             pop_j = gdf.iloc[j][column_pop]             name_j = gdf.iloc[j][column_name]             if i == j or name_i == name_j:                 matrix[i][j] = beta + \\                     seasonal_transmission_rate(                         t, bump_center, bump_width, bump_height)             else:                 if i != j and polygon_i != polygon_j and polygon_i.intersects(polygon_j):                     distance_i_j = centroid_i.distance(centroid_j)                     matrix[i][j] = p * (pop_i*pop_j) / ((distance_i_j)**2)                 else:                     matrix[i][j] = 0     return matrix  <p>Generating the differential equations</p> In\u00a0[44]: Copied! <pre>def SEIRD_model(y, t, model_parameters: tuple[dict]):\n\n    parameters = model_parameters[0]\n    gdf: gpd.GeoDataFrame = parameters['gdf']\n    beta: float = parameters['beta']\n    p: float = parameters['p']\n    gamma: float = parameters['gamma']\n    sigma: float = parameters['sigma']\n    bump_center: float = parameters['bump_center']\n    bump_width: float = parameters['bump_width']\n    bump_height: float = parameters['bump_height']\n    mu: float = parameters['mu']\n    N = parameters['N']\n    n: int = parameters['n']\n\n    S = defaultdict(float)\n    E = defaultdict(float)\n    I = defaultdict(float)\n    R = defaultdict(float)\n    D = defaultdict(float)\n\n    vars = tuple(sum([[S[i], E[i], I[i], R[i], D[i]] for i in range(n)], []))\n    vars = y\n\n   # Choosing the corresponding output for each subpopulation\n    def entryS(i):\n        return vars[5 * i]\n\n    def entryE(i):\n        return vars[5 * i + 1]\n\n    def entryI(i):\n        return vars[5 * i + 2]\n\n    def entryR(i):\n        return vars[5 * i + 3]\n\n    def entryD(i):\n        return vars[5 * i + 4]\n\n    # Initializando the directory for each subpopulation\n    dS = {}\n    dE = {}\n    dI = {}\n    dR = {}\n    dD = {}\n\n    # Defining the differential equations for each subpopulation\n    for i in range(n):\n        dS[i] = - entryS(i) / (N[i]*1.0) * sum(transmission_matrix(gdf, t, beta, p,\n                                                                   bump_center, bump_width, bump_height)[i][j] * entryI(j) for j in range(n))\n        dE[i] = entryS(i) / (N[i]*1.0) * sum(transmission_matrix(gdf, t, beta, p, bump_center,\n                                                                 bump_width, bump_height)[i][j] * entryI(j) for j in range(n)) - sigma * entryE(i)\n        dI[i] = sigma * entryE(i) - gamma * entryI(i)\n        dR[i] = gamma * (1 - mu) * entryI(i)\n        dD[i] = gamma * mu * entryI(i)\n\n    derivates = sum([[dS[i], dE[i], dI[i], dR[i], dD[i]]\n                    for i in range(n)], [])\n    return derivates   # For odeint\n\n\ndef generate_sol(genInput: dict) -&gt; dict:\n    init = genInput['init']\n    n = genInput['n']\n\n    # Initial conditions vector for the metapopulation model. len(initConditions) = 5*n\n    initConditions = tuple(sum(\n        [[init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i]] for i in range(n)], []))\n\n    # Time vector\n    T = genInput['T']\n    t = np.linspace(0, T, T+1)\n\n    # Computing the numerical solution\n    model = genInput['model']\n    solution = scipy.odeint(model, initConditions, t, args=((genInput,),))\n\n    indexVar = {}\n    indexVar['S'] = 0\n    indexVar['E'] = 1\n    indexVar['I'] = 2\n    indexVar['R'] = 3\n    indexVar['D'] = 4\n\n    def varSol(patch, var):\n        return solution[:, 5*patch + indexVar[var]]\n\n    return {'S': {patch: varSol(patch, 'S') for patch in range(n)},\n            'E': {patch: varSol(patch, 'E') for patch in range(n)},\n            'I': {patch: varSol(patch, 'I') for patch in range(n)},\n            'R': {patch: varSol(patch, 'R') for patch in range(n)},\n            'D': {patch: varSol(patch, 'D') for patch in range(n)},\n            'N': genInput['N'],\n            'init': init,\n            'beta': genInput['beta'],\n            'p': genInput['p'],\n            'gamma': genInput['gamma'],\n            'sigma': genInput['sigma'],\n            'mu':  genInput['mu'],\n            't': t,\n            'n': n,\n            'bump_center': genInput['bump_center'],\n            'bump_width': genInput['bump_width'],\n            'bump_height': genInput['bump_height'],\n            'model': model,\n            'raw_solution': solution}\n</pre> def SEIRD_model(y, t, model_parameters: tuple[dict]):      parameters = model_parameters[0]     gdf: gpd.GeoDataFrame = parameters['gdf']     beta: float = parameters['beta']     p: float = parameters['p']     gamma: float = parameters['gamma']     sigma: float = parameters['sigma']     bump_center: float = parameters['bump_center']     bump_width: float = parameters['bump_width']     bump_height: float = parameters['bump_height']     mu: float = parameters['mu']     N = parameters['N']     n: int = parameters['n']      S = defaultdict(float)     E = defaultdict(float)     I = defaultdict(float)     R = defaultdict(float)     D = defaultdict(float)      vars = tuple(sum([[S[i], E[i], I[i], R[i], D[i]] for i in range(n)], []))     vars = y     # Choosing the corresponding output for each subpopulation     def entryS(i):         return vars[5 * i]      def entryE(i):         return vars[5 * i + 1]      def entryI(i):         return vars[5 * i + 2]      def entryR(i):         return vars[5 * i + 3]      def entryD(i):         return vars[5 * i + 4]      # Initializando the directory for each subpopulation     dS = {}     dE = {}     dI = {}     dR = {}     dD = {}      # Defining the differential equations for each subpopulation     for i in range(n):         dS[i] = - entryS(i) / (N[i]*1.0) * sum(transmission_matrix(gdf, t, beta, p,                                                                    bump_center, bump_width, bump_height)[i][j] * entryI(j) for j in range(n))         dE[i] = entryS(i) / (N[i]*1.0) * sum(transmission_matrix(gdf, t, beta, p, bump_center,                                                                  bump_width, bump_height)[i][j] * entryI(j) for j in range(n)) - sigma * entryE(i)         dI[i] = sigma * entryE(i) - gamma * entryI(i)         dR[i] = gamma * (1 - mu) * entryI(i)         dD[i] = gamma * mu * entryI(i)      derivates = sum([[dS[i], dE[i], dI[i], dR[i], dD[i]]                     for i in range(n)], [])     return derivates   # For odeint   def generate_sol(genInput: dict) -&gt; dict:     init = genInput['init']     n = genInput['n']      # Initial conditions vector for the metapopulation model. len(initConditions) = 5*n     initConditions = tuple(sum(         [[init['S'][i], init['E'][i], init['I'][i], init['R'][i], init['D'][i]] for i in range(n)], []))      # Time vector     T = genInput['T']     t = np.linspace(0, T, T+1)      # Computing the numerical solution     model = genInput['model']     solution = scipy.odeint(model, initConditions, t, args=((genInput,),))      indexVar = {}     indexVar['S'] = 0     indexVar['E'] = 1     indexVar['I'] = 2     indexVar['R'] = 3     indexVar['D'] = 4      def varSol(patch, var):         return solution[:, 5*patch + indexVar[var]]      return {'S': {patch: varSol(patch, 'S') for patch in range(n)},             'E': {patch: varSol(patch, 'E') for patch in range(n)},             'I': {patch: varSol(patch, 'I') for patch in range(n)},             'R': {patch: varSol(patch, 'R') for patch in range(n)},             'D': {patch: varSol(patch, 'D') for patch in range(n)},             'N': genInput['N'],             'init': init,             'beta': genInput['beta'],             'p': genInput['p'],             'gamma': genInput['gamma'],             'sigma': genInput['sigma'],             'mu':  genInput['mu'],             't': t,             'n': n,             'bump_center': genInput['bump_center'],             'bump_width': genInput['bump_width'],             'bump_height': genInput['bump_height'],             'model': model,             'raw_solution': solution}  <p>Trying a small dataframe</p> In\u00a0[57]: Copied! <pre>k = 4\nexample = southeastScania.head(k)\nexample.shape\nm = Input_Model(example)\nm.maxDays()\n</pre> k = 4 example = southeastScania.head(k) example.shape m = Input_Model(example) m.maxDays()  Out[57]: <pre>579</pre> In\u00a0[58]: Copied! <pre>transmission_matrix(example, 0, 0.5, 0.5, 0.1, 180.0, 30.0)\n</pre> transmission_matrix(example, 0, 0.5, 0.5, 0.1, 180.0, 30.0)  Out[58]: <pre>array([[38.17833772,  0.        ,  0.        ,  0.        ],\n       [ 0.        , 38.17833772,  0.        ,  0.        ],\n       [ 0.        ,  0.        , 38.17833772,  0.        ],\n       [ 0.        ,  0.        ,  0.        , 38.17833772]])</pre> In\u00a0[\u00a0]: Copied! <pre>% % time\nModel_test = {'model': SEIRD_model,\n              'init': {\n                  'S': m.S0,\n                  'E': m.E0,\n                  'I': m.I0,\n                  'R': m.R0,\n                  'D': m.D0,\n              },  # defining the initial values for the model\n              'gdf': example,  # defining the graph\n              'beta': 0.3,\n              'p': 0.1,\n              'bump_center': 0.1,\n              'bump_width': 180.0,\n              'bump_height': 30.0,\n              'gamma': 0.06,\n              'sigma': 0.02,\n              'mu': 0.2,\n              'N': m.patchPop(),\n              'n': m.n,\n              'T': m.maxDays()}\n\nmodel_dict = generate_sol(Model_test)\nprint(model_dict['E'])\n</pre> % % time Model_test = {'model': SEIRD_model,               'init': {                   'S': m.S0,                   'E': m.E0,                   'I': m.I0,                   'R': m.R0,                   'D': m.D0,               },  # defining the initial values for the model               'gdf': example,  # defining the graph               'beta': 0.3,               'p': 0.1,               'bump_center': 0.1,               'bump_width': 180.0,               'bump_height': 30.0,               'gamma': 0.06,               'sigma': 0.02,               'mu': 0.2,               'N': m.patchPop(),               'n': m.n,               'T': m.maxDays()}  model_dict = generate_sol(Model_test) print(model_dict['E'])  In\u00a0[983]: Copied! <pre>southeastScania['BeginDaysPlague'] = southeastScania['BeginDaysPlague'].astype(\n    int)\nsoutheastScania['EndDaysPlague'] = southeastScania['EndDaysPlague'].astype(int)\nsoutheastScania['VictimsNumber'] = southeastScania['VictimsNumber'].astype(int)\n</pre> southeastScania['BeginDaysPlague'] = southeastScania['BeginDaysPlague'].astype(     int) southeastScania['EndDaysPlague'] = southeastScania['EndDaysPlague'].astype(int) southeastScania['VictimsNumber'] = southeastScania['VictimsNumber'].astype(int)  In\u00a0[985]: Copied! <pre># Define the objective function to minimize (sum of squared errors)\ndef objectiveFunction(parameters, beginTime, endTime, deathData):\n    beta, p, bump_center, bump_width, bump_height = parameters\n    model_info = {'model': SEIRD_model,\n                  'init': {\n                      'S': m.S0,\n                      'E': m.E0,\n                      'I': m.I0,\n                      'R': m.R0,\n                      'D': m.D0,\n                  },  # defining the initial values for the model\n                  'beta': beta,\n                  'p': p,\n                  'bump_center': bump_center,\n                  'bump_width': bump_width,\n                  'bump_height': bump_height,\n                  'gamma': 0.06,\n                  'sigma': 0.02,\n                  'mu': 0.2,\n                  'N': m.patchPop(),\n                  'n': 6,\n                  'T': 100}\n    model_sol = generate_sol(model_info)\n    totalError = 0\n    n = model_info['n']\n    for i in range(n):\n        initial_position = beginTime[i]\n        final_position = endTime[i]\n        if (deathData[i] != 0 and final_position != 0):\n            try:\n                val1 = model_sol['D'][i]\n                val2 = val1[final_position]\n                val3 = deathData[i]\n                totalError += 0.5*(model_sol['D'][i][initial_position] - 1.0)**2 + 0.5*(\n                    model_sol['D'][i][final_position] - deathData[i])**2\n            except:\n                print(n)\n                print(i)\n                print(final_position)\n                print(len(model_sol['D']))\n                print(model_sol['D'][i])\n                print(deathData[i])\n        else:\n            totalError += (model_sol['D'][i][initial_position] - 1.0)**2\n    return totalError\n</pre> # Define the objective function to minimize (sum of squared errors) def objectiveFunction(parameters, beginTime, endTime, deathData):     beta, p, bump_center, bump_width, bump_height = parameters     model_info = {'model': SEIRD_model,                   'init': {                       'S': m.S0,                       'E': m.E0,                       'I': m.I0,                       'R': m.R0,                       'D': m.D0,                   },  # defining the initial values for the model                   'beta': beta,                   'p': p,                   'bump_center': bump_center,                   'bump_width': bump_width,                   'bump_height': bump_height,                   'gamma': 0.06,                   'sigma': 0.02,                   'mu': 0.2,                   'N': m.patchPop(),                   'n': 6,                   'T': 100}     model_sol = generate_sol(model_info)     totalError = 0     n = model_info['n']     for i in range(n):         initial_position = beginTime[i]         final_position = endTime[i]         if (deathData[i] != 0 and final_position != 0):             try:                 val1 = model_sol['D'][i]                 val2 = val1[final_position]                 val3 = deathData[i]                 totalError += 0.5*(model_sol['D'][i][initial_position] - 1.0)**2 + 0.5*(                     model_sol['D'][i][final_position] - deathData[i])**2             except:                 print(n)                 print(i)                 print(final_position)                 print(len(model_sol['D']))                 print(model_sol['D'][i])                 print(deathData[i])         else:             totalError += (model_sol['D'][i][initial_position] - 1.0)**2     return totalError  In\u00a0[\u00a0]: Copied! <pre># Set up the data to fit\nbeginTime = southeastScania['BeginDaysPlague'].values\nendTime = southeastScania['EndDaysPlague'].values\ndeathData = southeastScania['VictimsNumber'].values\n\n# Choose initial guesses for the parameters to fit\nbeta_guess = 0.3\np_guess = 0.1\nbump_center_guess = 0.1\nbump_width_guess = 180.0\nbump_height_guess = 30.0\n\n# Minimize the objective function to obtain estimates for beta and gamma\nresult = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess), args=(beginTime, endTime, deathData),\n                           method='L-BFGS-B'\n                           # ,bounds=[(0, 1), (0, 1), (0, 10), (-2, 2), (-10, 10)]\n                           )\nbeta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x\n\nprint(\"beta = \", beta_estimated)\nprint(\"p = \", p_estimated)\nprint(\"bump_center = \", bump_center_estimated)\nprint(\"bump_width = \", bump_width_estimated)\nprint(\"bump_height = \", bump_height_estimated)\n</pre> # Set up the data to fit beginTime = southeastScania['BeginDaysPlague'].values endTime = southeastScania['EndDaysPlague'].values deathData = southeastScania['VictimsNumber'].values  # Choose initial guesses for the parameters to fit beta_guess = 0.3 p_guess = 0.1 bump_center_guess = 0.1 bump_width_guess = 180.0 bump_height_guess = 30.0  # Minimize the objective function to obtain estimates for beta and gamma result = optimize.minimize(objectiveFunction, x0=(beta_guess, p_guess, bump_center_guess, bump_width_guess, bump_height_guess), args=(beginTime, endTime, deathData),                            method='L-BFGS-B'                            # ,bounds=[(0, 1), (0, 1), (0, 10), (-2, 2), (-10, 10)]                            ) beta_estimated, p_estimated, bump_center_estimated, bump_width_estimated, bump_height_estimated = result.x  print(\"beta = \", beta_estimated) print(\"p = \", p_estimated) print(\"bump_center = \", bump_center_estimated) print(\"bump_width = \", bump_width_estimated) print(\"bump_height = \", bump_height_estimated)  <p>Plotting the solutions</p> In\u00a0[69]: Copied! <pre># Computing the daily deaths from the model output for a given patch\ndef daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:\n    assert T_inf &gt;= 0, \"t must be greater than 0\"\n    assert T_sup &gt;= 0, \"t must be greater than 0\"\n    cumulative_deaths = solution_model['D'][patch]  # list of floats\n    return [cumulative_deaths[t+1] - cumulative_deaths[t]\n            for t in range(T_inf, T_sup)]\n\n# Plotting the solution\n\n\ndef plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):\n\n    for key in state:\n        if key not in model:\n            raise ValueError(f\"Invalid state: {key}\")\n        for i in range(model['n']):\n            plt.plot(model['t'], model[key][i],\n                     label=f'{key} - {patchNames[i]}')\n    plt.xlabel('Time')\n    plt.ylabel('Infectious')\n    plt.title('SIRD model')\n    # plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))\n    plt.show()\n</pre> # Computing the daily deaths from the model output for a given patch def daily_deaths(solution_model: dict, patch: int, T_inf: int, T_sup: int) -&gt; list[float]:     assert T_inf &gt;= 0, \"t must be greater than 0\"     assert T_sup &gt;= 0, \"t must be greater than 0\"     cumulative_deaths = solution_model['D'][patch]  # list of floats     return [cumulative_deaths[t+1] - cumulative_deaths[t]             for t in range(T_inf, T_sup)]  # Plotting the solution   def plot_SIRD_solution(model: dict, state: list[str] = ['S', 'I', 'R', 'D', 'DailyDeaths']):      for key in state:         if key not in model:             raise ValueError(f\"Invalid state: {key}\")         for i in range(model['n']):             plt.plot(model['t'], model[key][i],                      label=f'{key} - {patchNames[i]}')     plt.xlabel('Time')     plt.ylabel('Infectious')     plt.title('SIRD model')     # plt.legend(loc=\"upper center\", mode =\"expand\",  ncol = 19, bbox_to_anchor=(0.5, -0.25))     plt.show()  In\u00a0[\u00a0]: Copied! <pre># Number of patches\nn = model_estimation['n']\n\n# Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height), sharex=False)\n\ntick_positions = [30, 60, 90, 120, 150, 180, 210, 240,\n                  270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]\ntick_labels = [\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"\n               ]\n\n# Plot daily deaths for each patch i\nfor i in range(n):\n    if TotalDeathsByParish[i] != 0 and EndPlagueByParish[i] != 0:\n        initial_position = beginTime[i]\n        final_position = EndPlagueByParish[i]\n        axes[i].plot(final_position, TotalDeathsByParish[i],\n                     'bo', label='Observed data')\n        axes[i].plot(model_dict['D'][i], color='orange', label=(patchNames[i]))\n        axes[i].set_ylabel('Cumulative Deaths', font='Helvetica')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, font='Helvetica', fontsize=9)\n    else:\n        axes[i].plot(model_dict['D'][i], color='orange', label=(patchNames[i]))\n        axes[i].set_ylabel('Cumulative Deaths', font='Helvetica')\n        axes[i].legend(loc='lower right')\n        axes[i].set_xticks(tick_positions, tick_labels,\n                           rotation=70, font='Helvetica', fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> # Number of patches n = model_estimation['n']  # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height), sharex=False)  tick_positions = [30, 60, 90, 120, 150, 180, 210, 240,                   270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570] tick_labels = [\"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"                ]  # Plot daily deaths for each patch i for i in range(n):     if TotalDeathsByParish[i] != 0 and EndPlagueByParish[i] != 0:         initial_position = beginTime[i]         final_position = EndPlagueByParish[i]         axes[i].plot(final_position, TotalDeathsByParish[i],                      'bo', label='Observed data')         axes[i].plot(model_dict['D'][i], color='orange', label=(patchNames[i]))         axes[i].set_ylabel('Cumulative Deaths', font='Helvetica')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, font='Helvetica', fontsize=9)     else:         axes[i].plot(model_dict['D'][i], color='orange', label=(patchNames[i]))         axes[i].set_ylabel('Cumulative Deaths', font='Helvetica')         axes[i].legend(loc='lower right')         axes[i].set_xticks(tick_positions, tick_labels,                            rotation=70, font='Helvetica', fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()  In\u00a0[71]: Copied! <pre># Set the figsize for each subplot\nfigsize_single_subplot = (8, 2)\n\n# Calculate the total figure height based on the number of subplots and their height\nfig_height = figsize_single_subplot[1] * n\n\n# Create a figure and an array of axes with nrows=n and ncols=1\nfig, axes = plt.subplots(nrows=n, ncols=1, figsize=(\n    figsize_single_subplot[0], fig_height))\n\ntick_labels = [\"Mar 1712\", \"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"\n               ]\ntick_positions = [0, 30, 60, 90, 120, 150, 180, 210, 240,\n                  270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]\n\n\nfor i in range(n):\n    # Plot daily deaths for each patch i\n    axes[i].plot(daily_deaths(model_dict, i, 0, 570),\n                 color='blue', label=(patchNames[i]))\n    axes[i].set_ylabel('Daily Deaths', font='Helvetica')\n    axes[i].legend(loc='upper right')\n    axes[i].xaxis.set_ticks(tick_positions, tick_labels,\n                            rotation=70, font='Helvetica', fontsize=9)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\nplt.show()\n</pre> # Set the figsize for each subplot figsize_single_subplot = (8, 2)  # Calculate the total figure height based on the number of subplots and their height fig_height = figsize_single_subplot[1] * n  # Create a figure and an array of axes with nrows=n and ncols=1 fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(     figsize_single_subplot[0], fig_height))  tick_labels = [\"Mar 1712\", \"Apr 1712\", \"May 1712\", \"Jun 1712\", \"Jul 1712\", \"Aug 1712\", \"Sep 1712\", \"Oct 1712\", \"Nov 1712\", \"Dec 1712\", \"Jan 1713\", \"Feb 1713\", \"Mar 1713\", \"Apr 1713\", \"May 1713\", \"Jun 1713\", \"Jul 1713\", \"Aug 1713\", \"Sep 1713\", \"Oct 1713\"                ] tick_positions = [0, 30, 60, 90, 120, 150, 180, 210, 240,                   270, 300, 330, 360, 390, 420, 450, 480, 510, 540, 570]   for i in range(n):     # Plot daily deaths for each patch i     axes[i].plot(daily_deaths(model_dict, i, 0, 570),                  color='blue', label=(patchNames[i]))     axes[i].set_ylabel('Daily Deaths', font='Helvetica')     axes[i].legend(loc='upper right')     axes[i].xaxis.set_ticks(tick_positions, tick_labels,                             rotation=70, font='Helvetica', fontsize=9)  # Adjust the layout to avoid overlapping plt.tight_layout() plt.show()"},{"location":"PlagueProject/databaseScania/","title":"DatabaseScania","text":"In\u00a0[756]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3  <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[757]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n%matplotlib inline\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py %matplotlib inline  <p>We have three different data sources.</p> <ol> <li>The data collected by Bodil corresponds to the plague period.</li> <li>The data provided by Lennart Palm which contains the population size for each parish in 1699 and 1718 based on a combination of tax records and estimations of population totals for Scania.</li> <li>The information from the TABVERK database includes the population size for parishes in the posterior years of the plague.</li> <li>The geographical information (polygons) for some parishes. This information doesn't correspond to the plague period.</li> </ol> <p>Our goal is to create a unique database for our project: Plague spread across Scania, Sweden, from 1710 to 1715.</p> <ol> <li>We start working with Bodil's information which we store in two databases: One database corresponds to the parishes affected by the plague, the region where parishes are located in Scania, the beginning and end of the outbreaks, and the number of victims. The second database corresponds to all the parishes in Scania during the plague period, the district, and the region they belonged to.</li> </ol> <p>The goal is to merge these two databases. First we set the working directory for private and public files.</p> In\u00a0[758]: Copied! <pre># For public files paths\ndata_folder = \"data\"\nappendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")\n\n# For private files paths\ndata_private_folder = \"data/private\"\nallParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\")\n</pre> # For public files paths data_folder = \"data\" appendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")  # For private files paths data_private_folder = \"data/private\" allParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\") <p>Reading the different data sources (.xlsx, and .csv files)</p> In\u00a0[759]: Copied! <pre># Bodil's data Appendix 6 plague parishes\nplagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")\n# All parishes in Scania during the plague period\nallParishesScania = pd.read_excel(allParishes_path)\n# All parishes from the Southeast, Middle and Southwest region of Scania with population data from Lennart Palm file\nsoutheastScania = pd.read_excel(allParishes_path, sheet_name=\"southeast\")\nsouthwestScania = pd.read_excel(allParishes_path, sheet_name=\"southwest\")\nmiddleScania = pd.read_excel(allParishes_path, sheet_name=\"middle\")\n</pre> # Bodil's data Appendix 6 plague parishes plagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\") # All parishes in Scania during the plague period allParishesScania = pd.read_excel(allParishes_path) # All parishes from the Southeast, Middle and Southwest region of Scania with population data from Lennart Palm file southeastScania = pd.read_excel(allParishes_path, sheet_name=\"southeast\") southwestScania = pd.read_excel(allParishes_path, sheet_name=\"southwest\") middleScania = pd.read_excel(allParishes_path, sheet_name=\"middle\") <p>Transforming the lowercase to uppercase and checking the type</p> In\u00a0[760]: Copied! <pre>allParishesScania = allParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nplagueParishesScania = plagueParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nsoutheastScania = southeastScania.apply(\n    lambda x: x.astype(str).str.upper())\nmiddleScania = middleScania.apply(\n    lambda x: x.astype(str).str.upper())\nsouthwestScania = southwestScania.apply(\n    lambda x: x.astype(str).str.upper())\ntype(plagueParishesScania)\ntype(allParishesScania)\n</pre> allParishesScania = allParishesScania.apply(     lambda x: x.astype(str).str.upper()) plagueParishesScania = plagueParishesScania.apply(     lambda x: x.astype(str).str.upper()) southeastScania = southeastScania.apply(     lambda x: x.astype(str).str.upper()) middleScania = middleScania.apply(     lambda x: x.astype(str).str.upper()) southwestScania = southwestScania.apply(     lambda x: x.astype(str).str.upper()) type(plagueParishesScania) type(allParishesScania)  Out[760]: <pre>pandas.core.frame.DataFrame</pre> <p>Visualizing the DataFrames and calculating the length of each one.</p> In\u00a0[761]: Copied! <pre>print(len(allParishesScania))\nplagueParishesScania.head(3)\n</pre> print(len(allParishesScania)) plagueParishesScania.head(3) <pre>397\n</pre> Out[761]: BeginPlaguePeriod EndPlaguePeriod ParishName VictimsNumber Region 0 NOV 1710 APR 1711 N\u00c4SUM 671 NORTHEAST 1 FEB 1712 UNDEFINED N\u00c4SUM ? NORTHEAST 2 NOV 1710 AUG 1711 IV\u00d6 123 NORTHEAST In\u00a0[762]: Copied! <pre>print(len(plagueParishesScania))\nallParishesScania.head(3)\n</pre> print(len(plagueParishesScania)) allParishesScania.head(3) <pre>177\n</pre> Out[762]: Region District(H\u00e4rad) ParishName 0 SOUTHEAST ALBO ANDRARUM 1 SOUTHEAST ALBO BR\u00d6SARP 2 SOUTHEAST ALBO ELJAR\u00d6D <p>Merging the two datasets (allParishesScania and plagueParishesScania)</p> In\u00a0[763]: Copied! <pre>parishesScania = pd.merge(\n    allParishesScania, plagueParishesScania, how='left', on=['ParishName', 'Region'])\n</pre> parishesScania = pd.merge(     allParishesScania, plagueParishesScania, how='left', on=['ParishName', 'Region']) <p>Checking that the new data frame keep all the outbreaks for parish</p> In\u00a0[764]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'N\u00c4SUM']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'N\u00c4SUM']  Out[764]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 394 NORTHEAST VILLANDS N\u00c4SUM NOV 1710 APR 1711 671 395 NORTHEAST VILLANDS N\u00c4SUM FEB 1712 UNDEFINED ? <p>Extracting the parishes' names from the data frame</p> In\u00a0[765]: Copied! <pre>parishesScania_names = get_Names(\n    parishesScania, 'ParishName').unique().tolist()\nlen(parishesScania_names)\n</pre> parishesScania_names = get_Names(     parishesScania, 'ParishName').unique().tolist() len(parishesScania_names)  Out[765]: <pre>396</pre> <p>The length of 'parishesScania_names' is less than the number of rows in the data frame 'allparishesScania'. This means, there is a repeated name: 'L\u00d6DDEK\u00d6PINGE'. We have to check the information for this parish:</p> In\u00a0[766]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[766]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 86 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? 160 SOUTHWEST TORNA L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? <p>Only the parish L\u00d6DDEK\u00d6PINGE at HARJAGER was affected by the plague according to the file 'Bilaga 6 d - sydva\u0308st.doc' provided by Bodil. So we need to fix the information in the other row (160).</p> In\u00a0[767]: Copied! <pre>parishesScania.at[160, 'BeginPlaguePeriod'] = np.NaN\nparishesScania.at[160, 'EndPlaguePeriod'] = np.NaN\nparishesScania.at[160, 'VictimsNumber'] = np.NaN\n</pre> parishesScania.at[160, 'BeginPlaguePeriod'] = np.NaN parishesScania.at[160, 'EndPlaguePeriod'] = np.NaN parishesScania.at[160, 'VictimsNumber'] = np.NaN  <p>Checking the data:</p> In\u00a0[768]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[768]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 86 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? 160 SOUTHWEST TORNA L\u00d6DDEK\u00d6PINGE NaN NaN NaN <p>Filtering the data frame by region and then get the names of the parishes:</p> In\u00a0[792]: Copied! <pre>southeastParishes = parishesByregion(parishesScania, 'SOUTHEAST')\nsouthwestParishes = parishesByregion(parishesScania, 'SOUTHWEST')\nmiddleParishes = parishesByregion(parishesScania, 'MIDDLE')\n</pre> southeastParishes = parishesByregion(parishesScania, 'SOUTHEAST') southwestParishes = parishesByregion(parishesScania, 'SOUTHWEST') middleParishes = parishesByregion(parishesScania, 'MIDDLE') In\u00a0[772]: Copied! <pre>southeastParishes_names = get_Names(southeastParishes, 'ParishName')\nsouthwestParishes_names = get_Names(southwestParishes, 'ParishName')\nmiddleParishes_names = get_Names(middleParishes, 'ParishName')\n\n# Check parishes per region by name\nsoutheastParishes.loc[southeastParishes['ParishName'] == 'R\u00d6RUM']\nlen(southeastParishes_names)\n</pre> southeastParishes_names = get_Names(southeastParishes, 'ParishName') southwestParishes_names = get_Names(southwestParishes, 'ParishName') middleParishes_names = get_Names(middleParishes, 'ParishName')  # Check parishes per region by name southeastParishes.loc[southeastParishes['ParishName'] == 'R\u00d6RUM'] len(southeastParishes_names) Out[772]: <pre>62</pre> <ol> <li>Here, we used the population size estimations provided by Lennart Palm. Merging the two datasets (southeastScania and southeastParishes)</li> </ol> In\u00a0[773]: Copied! <pre>southeastParishesPop = pd.merge(\n    southeastScania, southeastParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)'])\nsouthwestParishesPop = pd.merge(\n    southwestScania, southwestParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)'])\nmiddleParishesPop = pd.merge(\n    middleScania, middleParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)'])\n</pre> southeastParishesPop = pd.merge(     southeastScania, southeastParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)']) southwestParishesPop = pd.merge(     southwestScania, southwestParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)']) middleParishesPop = pd.merge(     middleScania, middleParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)']) In\u00a0[774]: Copied! <pre>southeastParishesPop.head(3)\n</pre> southeastParishesPop.head(3)  Out[774]: Region District(H\u00e4rad) ParishName BEF1699 BEF1718 AV_BEF BeginPlaguePeriod EndPlaguePeriod VictimsNumber 0 SOUTHEAST ALBO ANDRARUM 1100 1241 1170.5 NaN NaN NaN 1 SOUTHEAST ALBO BR\u00d6SARP 480 541 510.5 NaN NaN NaN 2 SOUTHEAST ALBO ELJAR\u00d6D 320 361 340.5 JAN 1713 UNDEFINED 3 <ol> <li>The geographical information for Scania is already projected on the plane, i.e. the measures are in meters not in longitude and latitude. To process the shape file, we proceed as with the census file. First, we set the directory and chose the columns to work with.</li> </ol> In\u00a0[775]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\")\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\nparishScaniaMap = parishScaniaMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\") SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] parishScaniaMap = parishScaniaMap[selected_columns]  <p>Now, we remove white spaces and patterns. Then, we filter the shape file considering the column \"GET_END_YE\" of the polygon.</p> In\u00a0[776]: Copied! <pre>parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'\n                                                                        ])\nparishScaniaMap = process_dataframe(parishScaniaMap, 'G_NAME', 'GET_END_YE')\nlen(parishScaniaMap)\n</pre> parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'                                                                         ]) parishScaniaMap = process_dataframe(parishScaniaMap, 'G_NAME', 'GET_END_YE') len(parishScaniaMap)  Out[776]: <pre>412</pre> <p>Plotting the map only with the polygons obtained after clean the data</p> In\u00a0[779]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nparishScaniaMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) parishScaniaMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() <p>Working only with Southeast Scania: southeastParishesPop and parishScaniaMap</p> In\u00a0[780]: Copied! <pre>df1 = southeastParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=80,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = southeastParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=80,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[781]: Copied! <pre>southeastParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n# Get the index for Sk\u00f6rup for fixing the geographical data\nsoutheastParishMap.loc[southeastParishMap['ParishName'] == 'R\u00d6RUM']\nsoutheastParishMap.loc[southeastParishMap['ParishName'] == 'SK\u00d6RUP']\n</pre> southeastParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] # Get the index for Sk\u00f6rup for fixing the geographical data southeastParishMap.loc[southeastParishMap['ParishName'] == 'R\u00d6RUM'] southeastParishMap.loc[southeastParishMap['ParishName'] == 'SK\u00d6RUP']  Out[781]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id 58 SOUTHEAST LJUNITS SK\u00d6RUP SKURUPS 242 273 257.5 NaN NaN NaN POLYGON ((4214392.138 3182887.177, 4214576.093... <p>We need to modify manually the geographical information assigned to Sk\u00f6rup and R\u00f6rum for Sj\u00f6rups and R\u00f6rums information,respectively.</p> In\u00a0[782]: Copied! <pre># Get the geometry from the map\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == 'SJ\u00d6RUPS', 'geometry'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == 'R\u00d6RUMS', 'geometry'].values[0]\n\n# Get the name from the map\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'SJ\u00d6RUPS', 'G_NAME'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'R\u00d6RUMS', 'G_NAME'].values[0]\n</pre> # Get the geometry from the map parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == 'SJ\u00d6RUPS', 'geometry'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == 'R\u00d6RUMS', 'geometry'].values[0]  # Get the name from the map parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'SJ\u00d6RUPS', 'G_NAME'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'R\u00d6RUMS', 'G_NAME'].values[0]  Out[782]: <pre>'R\u00d6RUMS'</pre> In\u00a0[783]: Copied! <pre>southeastParishMap.at[5, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'R\u00d6RUMS', 'G_NAME'].values[0]\nsoutheastParishMap.at[5, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                           == 'R\u00d6RUMS', 'geometry'].values[0]\nsoutheastParishMap.at[58, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                          == 'SJ\u00d6RUPS', 'G_NAME'].values[0]\nsoutheastParishMap.at[58, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                            == 'SJ\u00d6RUPS', 'geometry'].values[0]\n</pre> southeastParishMap.at[5, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'R\u00d6RUMS', 'G_NAME'].values[0] southeastParishMap.at[5, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                            == 'R\u00d6RUMS', 'geometry'].values[0] southeastParishMap.at[58, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                           == 'SJ\u00d6RUPS', 'G_NAME'].values[0] southeastParishMap.at[58, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                             == 'SJ\u00d6RUPS', 'geometry'].values[0]  In\u00a0[784]: Copied! <pre>type(southeastParishMap)\n</pre> type(southeastParishMap) Out[784]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[785]: Copied! <pre>southeastScaniaMap = gpd.GeoDataFrame(southeastParishMap, geometry='geometry')\n</pre> southeastScaniaMap = gpd.GeoDataFrame(southeastParishMap, geometry='geometry') In\u00a0[786]: Copied! <pre># southeastScaniaMap.to_csv('southeastScania.csv', index=False)\n</pre> # southeastScaniaMap.to_csv('southeastScania.csv', index=False)  In\u00a0[787]: Copied! <pre>southeastScaniaMap = get_area(southeastScaniaMap)\nsoutheastScaniaMap = get_centroid(southeastScaniaMap)\nfrom shapely.geometry import Point, mapping\nsoutheastScaniaMap['centroid'] = southeastScaniaMap['centroid'].apply(mapping)\n</pre> southeastScaniaMap = get_area(southeastScaniaMap) southeastScaniaMap = get_centroid(southeastScaniaMap) from shapely.geometry import Point, mapping southeastScaniaMap['centroid'] = southeastScaniaMap['centroid'].apply(mapping) In\u00a0[788]: Copied! <pre># import json\n\n# class PointEncoder(json.JSONEncoder):\n#     def default(self, obj):\n#         if isinstance(obj, Point):\n#             point_dict = mapping(obj)\n#             point_json = json.dumps(point_dict)\n#             return point_json\n#         else:\n#             return json.JSONEncoder.default(self, obj)\n</pre> # import json  # class PointEncoder(json.JSONEncoder): #     def default(self, obj): #         if isinstance(obj, Point): #             point_dict = mapping(obj) #             point_json = json.dumps(point_dict) #             return point_json #         else: #             return json.JSONEncoder.default(self, obj)  <p>Plotting the southeast parishes</p> <p>Before to plot the map of parishes, for a given Geodataframe, we assign 'red' to the parishes affected by the plague and blue for the others. This information is added as a column with heading 'color'.</p> In\u00a0[794]: Copied! <pre>colorByColumn(southeastScaniaMap, 'EndPlaguePeriod')\nsoutheastMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nsoutheastScaniaMap.explore(\n    m = southeastMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"Southeast Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    southeastMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(southeastMap)  # use folium to add layer control\n\nsoutheastMap  # show map\n</pre> colorByColumn(southeastScaniaMap, 'EndPlaguePeriod') southeastMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  southeastScaniaMap.explore(     m = southeastMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"Southeast Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     southeastMap )  # use folium to add alternative tiles folium.LayerControl().add_to(southeastMap)  # use folium to add layer control  southeastMap  # show map Out[794]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[521]: Copied! <pre>type(southeastParishMap)\n</pre> type(southeastParishMap) Out[521]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[795]: Copied! <pre># Assuming you have a GeoDataFrame named 'gdf'\ndef calculate_quotient(gdf, col1, col2):\n    gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')\n    gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')\n    \n    # Calculate the death rate per 1000 inhabitants\n    gdf['quotient'] = (gdf[col1] / gdf[col2])*1000\n    pass\n\ncalculate_quotient(southeastScaniaMap, 'VictimsNumber', 'BEF1699')\n</pre> # Assuming you have a GeoDataFrame named 'gdf' def calculate_quotient(gdf, col1, col2):     gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')     gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')          # Calculate the death rate per 1000 inhabitants     gdf['quotient'] = (gdf[col1] / gdf[col2])*1000     pass  calculate_quotient(southeastScaniaMap, 'VictimsNumber', 'BEF1699') In\u00a0[523]: Copied! <pre>type(southeastScaniaMap['area_km2'][3])\n</pre> type(southeastScaniaMap['area_km2'][3]) Out[523]: <pre>numpy.float64</pre> In\u00a0[524]: Copied! <pre># fig, ax = plt.subplots()\n# ims = []\n# def update_fig(month):\n#     if len(ims) &gt; 0:\n#         ims[0].remove()\n#         del ims[0]\n#     geos = southeastScaniaMap['geometry'].values\n#     victims_rate = southeastScaniaMap['quotient'].sort_values(ascending=True).values\n#     artists = southeastScaniaMap.plotting.plot_polygon_collection(ax, geos, victims_rate, True, cmap='Reds')\n    \n#     ims.append(artists)\n#     ax.set_title(month)\n#     return ims\n# anim = FuncAnimation(fig, update_fig, frames=plagueMonth, interval=1000, repeat=True)\n# fig.show()\n</pre> # fig, ax = plt.subplots() # ims = [] # def update_fig(month): #     if len(ims) &gt; 0: #         ims[0].remove() #         del ims[0] #     geos = southeastScaniaMap['geometry'].values #     victims_rate = southeastScaniaMap['quotient'].sort_values(ascending=True).values #     artists = southeastScaniaMap.plotting.plot_polygon_collection(ax, geos, victims_rate, True, cmap='Reds')      #     ims.append(artists) #     ax.set_title(month) #     return ims # anim = FuncAnimation(fig, update_fig, frames=plagueMonth, interval=1000, repeat=True) # fig.show()  In\u00a0[793]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel1')\nsoutheastScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel1') southeastScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show()  <p>Working only with Middle Scania: middleParishesPop and parishScaniaMap</p> In\u00a0[801]: Copied! <pre>df1 = middleParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=90,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = middleParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=90,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[802]: Copied! <pre>middleParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\nmiddleParishMap\n</pre> middleParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] middleParishMap Out[802]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id 0 MIDDLE FROSTA BORLUNDA BORLUNDA 270.0 305.0 287.5 NaN NaN NaN POLYGON ((4204291.981 3215814.801, 4204132.173... 1 MIDDLE FROSTA BOSJ\u00d6KLOSTER BOSJ\u00d6KLOSTERS 502.0 567.0 534.5 OCT 1712 OCT 1712 3 POLYGON ((4212583.742 3219218.536, 4211813.942... 2 MIDDLE FROSTA FULLTOFTA FULLTOFTA 476.0 537.0 506.5 NaN NaN NaN POLYGON ((4215685.166 3221754.710, 4216273.458... 3 MIDDLE FROSTA GUDMUNDTORP GUDMUNTORPS 479.0 540.0 509.5 NaN NaN NaN POLYGON ((4212583.742 3219218.536, 4212844.606... 4 MIDDLE FROSTA G\u00c5RDST\u00c5NGA G\u00c5RDST\u00c5NGA 258.0 292.0 275.0 OCT 1712 OCT 1712 ? POLYGON ((4200187.129 3210912.050, 4200150.320... 5 MIDDLE FROSTA HAMMARLUNDA HAMMARLUNDA 229.0 259.0 244.0 NaN NaN NaN POLYGON ((4211783.346 3212162.046, 4212225.884... 6 MIDDLE FROSTA HARL\u00d6SA HARL\u00d6SA 636.0 717.0 676.5 NaN NaN NaN POLYGON ((4213074.502 3205693.512, 4212950.778... 7 MIDDLE FROSTA HOLMBY HOLMBY 306.0 345.0 325.5 NaN NaN NaN POLYGON ((4207597.596 3210656.569, 4207917.721... 8 MIDDLE FROSTA HURVA HURVA 156.0 176.0 166.0 MAR 1712 UNDEFINED ? POLYGON ((4211783.346 3212162.046, 4210973.708... 9 MIDDLE FROSTA H\u00d6GSER\u00d6D H\u00d6GSER\u00d6DS 375.0 423.0 399.0 NaN NaN NaN POLYGON ((4215051.117 3209868.930, 4214278.291... 10 MIDDLE FROSTA H\u00d6RBY H\u00d6RBY 756.0 853.0 804.5 DEC 1711 MAR 1712 76 POLYGON ((4218927.113 3218943.589, 4218987.130... 11 MIDDLE FROSTA H\u00d6\u00d6R H\u00d6RR\u00d6DS 694.0 783.0 738.5 OCT 1712 OCT 1712 1 POLYGON ((4245792.553 3211743.504, 4245370.977... 12 MIDDLE FROSTA LYBY LYBY 436.0 492.0 464.0 NaN NaN NaN POLYGON ((4218927.113 3218943.589, 4220013.140... 13 MIDDLE FROSTA MUNKARP MUNKARPS 244.0 275.0 259.5 NaN NaN NaN POLYGON ((4206942.870 3227991.392, 4206869.482... 14 MIDDLE FROSTA NORRA R\u00d6RUM NORRA R\u00d6RUMS 278.0 314.0 296.0 JAN 1711 JUL 1711 45 POLYGON ((4216094.670 3237379.308, 4216192.024... 15 MIDDLE FROSTA SKARHULT SKARHULTS 298.0 336.0 317.0 NaN NaN NaN POLYGON ((4207448.547 3214660.409, 4207474.124... 16 MIDDLE FROSTA SKEGLINGE SKEGLINGE 78.0 88.0 83.0 NaN NaN NaN POLYGON ((4203417.101 3213050.403, 4203488.906... 17 MIDDLE FROSTA SVENSK\u00d6P SVENSK\u00d6PS 136.0 153.0 144.5 NaN NaN NaN POLYGON ((4237203.042 3227245.029, 4237241.371... 18 MIDDLE FROSTA S\u00d6DRA R\u00d6RUM S\u00d6DRA R\u00d6RUMS 347.0 391.0 369.0 NaN NaN NaN POLYGON ((4219693.709 3227632.070, 4219625.094... 19 MIDDLE FROSTA \u00d6STRA SALLERUP \u00d6STRA SALLERUPS 528.0 595.0 561.5 NaN NaN NaN POLYGON ((4227466.490 3217123.874, 4227549.708... 20 MIDDLE FROSTA \u00d6STRA STR\u00d6 \u00d6STRA STR\u00d6 207.0 234.0 220.5 NaN NaN NaN MULTIPOLYGON (((4208413.366 3216916.730, 42082... 21 MIDDLE FROSTA \u00d6STRA \u00c4SPINGE \u00c4SP\u00d6 339.0 383.0 361.0 NaN NaN NaN POLYGON ((4207584.658 3169226.994, 4208038.714... 22 MIDDLE F\u00c4RS BJ\u00d6RKA BJ\u00d6RKA 106.0 120.0 113.0 NaN NaN NaN POLYGON ((4221896.508 3196012.945, 4220903.365... 23 MIDDLE F\u00c4RS BRANDSTAD BRANDSTADS 306.0 345.0 325.5 SEP 1712 NOV 1712 15 POLYGON ((4229153.582 3198970.194, 4229110.593... 24 MIDDLE F\u00c4RS FR\u00c4NNINGE FR\u00c4NNINGE 684.0 772.0 728.0 NaN NaN NaN POLYGON ((4231850.464 3211837.148, 4231887.690... 25 MIDDLE F\u00c4RS ILSTORP ILSTORPS 143.0 162.0 152.5 NaN NaN NaN POLYGON ((4220828.463 3195635.124, 4220731.877... 26 MIDDLE F\u00c4RS L\u00c5NGAR\u00d6D L\u00c5NGAR\u00d6DS 436.0 492.0 464.0 NaN NaN NaN POLYGON ((4231850.464 3211837.148, 4231682.053... 27 MIDDLE F\u00c4RS L\u00d6VESTAD L\u00d6VESTADS 633.0 714.0 673.5 MAY 1713 DEC 1713 95 POLYGON ((4242426.101 3200641.194, 4242603.864... 28 MIDDLE F\u00c4RS RAMS\u00c5SA RAMS\u00c5SA 176.0 199.0 187.5 AUG 1712 SEP 1712 5 POLYGON ((4237241.941 3188704.248, 4237143.806... 29 MIDDLE F\u00c4RS R\u00d6DDINGE R\u00d6DDINGE 320.0 361.0 340.5 SEP 1712 SEP 1712 122 POLYGON ((4231594.393 3190271.662, 4231316.185... 30 MIDDLE F\u00c4RS R\u00d6DDINGE R\u00d6DDINGE 320.0 361.0 340.5 JUL 1713 OCT 1713 3 POLYGON ((4231594.393 3190271.662, 4231316.185... 31 MIDDLE F\u00c4RS R\u00d6DDINGE R\u00d6DDINGE 320.0 361.0 340.5 FEB 1715 FEB 1715 5 POLYGON ((4231594.393 3190271.662, 4231316.185... 32 MIDDLE F\u00c4RS SKARTOFTA \u00d6RTOFTA NAN NAN NAN JUL 1712 OCT 1712 7 POLYGON ((4197042.732 3210958.553, 4196950.051... 33 MIDDLE F\u00c4RS S\u00d6DRA \u00c5SUM S\u00d6DRA \u00c5SUMS 370.0 418.0 394.0 JUL 1712 DEC 1712 45 POLYGON ((4229153.582 3198970.194, 4229211.219... 34 MIDDLE F\u00c4RS S\u00d6VDE S\u00d6VDE 800.0 903.0 851.5 AUG 1712 UNDEFINED ? POLYGON ((4231594.393 3190271.662, 4232487.658... 35 MIDDLE F\u00c4RS TOL\u00c5NGA TOL\u00c5NGA 449.0 507.0 478.0 NaN NaN NaN POLYGON ((4229153.582 3198970.194, 4229495.412... 36 MIDDLE F\u00c4RS VANSTAD VANSTADS 377.0 426.0 401.5 JUL 1713 OCT 1713 10 POLYGON ((4239226.842 3192791.174, 4238845.150... 37 MIDDLE F\u00c4RS VOLLSJ\u00d6 VOLLSJ\u00d6 261.0 294.0 277.5 NaN NaN NaN POLYGON ((4231030.032 3205003.509, 4231168.710... 38 MIDDLE F\u00c4RS VOMB V\u00c4 269.0 303.0 286.0 SEP 1712 NOV 1712 9 POLYGON ((4246916.116 3231887.997, 4247067.301... 39 MIDDLE F\u00c4RS V\u00c4STERSTAD V\u00c4STERSTADS 434.0 489.0 461.5 NaN NaN NaN POLYGON ((4219292.490 3209122.549, 4219052.540... 40 MIDDLE F\u00c4RS \u00d6STRA K\u00c4RRSTORP \u00d6STRA K\u00c4RRSTORPS 385.0 434.0 409.5 NaN NaN NaN POLYGON ((4229941.187 3206811.777, 4229670.694... 41 MIDDLE F\u00c4RS \u00d6STRABY \u00d6STRABY 574.0 648.0 611.0 OCT 1713 OCT 1713 ? POLYGON ((4219292.490 3209122.549, 4219425.695... 42 MIDDLE F\u00c4RS \u00d6VED V\u00c4 357.0 403.0 380.0 NaN NaN NaN POLYGON ((4246916.116 3231887.997, 4247067.301... <ol> <li>We will process the census file to get the population size by parish. This file corresponds to all of Sweden, so we process it to keep only the information for Scania for the closest year to the plague outbreaks. We start setting the directory and reading the census file:</li> </ol> In\u00a0[353]: Copied! <pre># Set the working directory for private files\ndata_private_folder = \"data/private\"\ncensus_path = os.path.join(data_private_folder, 'FILE01_FALD.csv')\ncensusSweden = pd.read_csv(census_path, sep=';')\ncensusSweden.shape\n</pre> # Set the working directory for private files data_private_folder = \"data/private\" census_path = os.path.join(data_private_folder, 'FILE01_FALD.csv') censusSweden = pd.read_csv(census_path, sep=';') censusSweden.shape  Out[353]: <pre>(102360, 50)</pre> <p>Checking the memory usage (this is not necessary)</p> In\u00a0[354]: Copied! <pre># censusSweden.info(memory_usage='deep')\n</pre> # censusSweden.info(memory_usage='deep')  <p>Checking the names of all columns in the data</p> In\u00a0[355]: Copied! <pre>columns = censusSweden.columns\n</pre> columns = censusSweden.columns  <p>Calling the data only with specific columns to reduce the memory usage.</p> In\u00a0[356]: Copied! <pre>censusSweden = pd.read_csv(census_path, sep=';', usecols=[\n                           'LANGENNMN'  # Standard name of the county for the geographical area in plain text\n                           , 'GEOIDNMN'  # Standard name of the geographical area in plain text, i.e. not a source name\n                           , 'GEOIDTYP'  # Type of breakdown of the geographical area  0 =Assembly, 1 = Pastorate, 2 = Other type, 3 = Several parishes, 9 = Part of a parish\n                           , 'AR'  # Year\n                           , 'KON'  # 1 = Man  2 = Female. I choose 1 but it could be 2 for the total population\n                           , 'BEF_TOT'  # Total population at source\n                           , 'BEF_GENTOT'  # Total population, generated\n                           ])\n</pre> censusSweden = pd.read_csv(census_path, sep=';', usecols=[                            'LANGENNMN'  # Standard name of the county for the geographical area in plain text                            , 'GEOIDNMN'  # Standard name of the geographical area in plain text, i.e. not a source name                            , 'GEOIDTYP'  # Type of breakdown of the geographical area  0 =Assembly, 1 = Pastorate, 2 = Other type, 3 = Several parishes, 9 = Part of a parish                            , 'AR'  # Year                            , 'KON'  # 1 = Man  2 = Female. I choose 1 but it could be 2 for the total population                            , 'BEF_TOT'  # Total population at source                            , 'BEF_GENTOT'  # Total population, generated                            ])  <p>Processing the census data such that corresponds only to Scania.</p> In\u00a0[357]: Copied! <pre>censusScania = censusSweden.loc[((censusSweden['LANGENNMN'] == 'KRISTIANSTADS L\u00c4N') | (\n    censusSweden['LANGENNMN'] == 'MALM\u00d6HUS L\u00c4N')) &amp; (censusSweden['KON'] == 1)]\ncensusScania.shape\n</pre> censusScania = censusSweden.loc[((censusSweden['LANGENNMN'] == 'KRISTIANSTADS L\u00c4N') | (     censusSweden['LANGENNMN'] == 'MALM\u00d6HUS L\u00c4N')) &amp; (censusSweden['KON'] == 1)] censusScania.shape  Out[357]: <pre>(8748, 7)</pre> <p>Cleaning the data: Now, we remove given strings and white spaces at the end of a word. To do so, we must provide the string list to delete. In this step, you can use regular expressions.</p> In\u00a0[358]: Copied! <pre># Regex to delete the following strings:\n# ', DEL (KRISTIANSTAD)', ', DEL (MALM\u00d6HUS)', ', DEL (MALM\u00d6HUS L\u00c4N)'\n# ,', DEL AV (FROSTA H\u00c4RAD, MALM\u00d6HUS L\u00c4N)', ', DEL (EVER\u00d6D, MALM\u00d6HUS)'\n# ,' DEL (HYLLINGE, MALM\u00d6HUS)', ' (MALM\u00d6 SF)', '(STAFFANSTORP)'\n# ,' GARNISONSF\u00d6RS.', ' OCH GARNISIONSF\u00d6RS.', ' STADS', ' STAD'\n\n# regex = r'((,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISIONSF\u00d6RS\\.?)|(\\s+STADS?))$'\nregex = r'(,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISONSF\u00d6RS\\.?)'\ncensusScania = replace_strings_and_regex(censusScania, 'GEOIDNMN', [\n    'PASTORAT', 'HOSPITAL', ' LANDS', ' SLOTTSF\u00d6RSAMLING', ' DOMKYRKOF\u00d6RSAMLING', ' STADS', regex\n])\ncensusScania.shape\n</pre> # Regex to delete the following strings: # ', DEL (KRISTIANSTAD)', ', DEL (MALM\u00d6HUS)', ', DEL (MALM\u00d6HUS L\u00c4N)' # ,', DEL AV (FROSTA H\u00c4RAD, MALM\u00d6HUS L\u00c4N)', ', DEL (EVER\u00d6D, MALM\u00d6HUS)' # ,' DEL (HYLLINGE, MALM\u00d6HUS)', ' (MALM\u00d6 SF)', '(STAFFANSTORP)' # ,' GARNISONSF\u00d6RS.', ' OCH GARNISIONSF\u00d6RS.', ' STADS', ' STAD'  # regex = r'((,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISIONSF\u00d6RS\\.?)|(\\s+STADS?))$' regex = r'(,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISONSF\u00d6RS\\.?)' censusScania = replace_strings_and_regex(censusScania, 'GEOIDNMN', [     'PASTORAT', 'HOSPITAL', ' LANDS', ' SLOTTSF\u00d6RSAMLING', ' DOMKYRKOF\u00d6RSAMLING', ' STADS', regex ]) censusScania.shape  Out[358]: <pre>(8748, 7)</pre> <p>Process the data from Scania only to keep the first population size registered for each parish. This was done following two approaches.</p> <ol> <li>First approach: We group the data by parish name and then select the minimum year. As the minimum year is not unique after deleted strings, this approach allows repetitions.</li> </ol> In\u00a0[359]: Copied! <pre>popSizeScania_rep = process_dataframe_rep(censusScania, 'GEOIDNMN', 'AR')\nprint(popSizeScania_rep.shape)\npopSizeScania_rep.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')\n</pre> popSizeScania_rep = process_dataframe_rep(censusScania, 'GEOIDNMN', 'AR') print(popSizeScania_rep.shape) popSizeScania_rep.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')  <pre>(475, 7)\n</pre> Out[359]: LANGENNMN GEOIDNMN GEOIDTYP AR KON BEF_TOT BEF_GENTOT 57393 MALM\u00d6HUS L\u00c4N HELSINGBORGS 0 1775 1 1290 1290 57426 MALM\u00d6HUS L\u00c4N HELSINGBORGS 0 1775 1 453 453 <ol> <li>Second approach: This method explores the given DataFrame exhaustively and keeps the required information in a dictionary. In our case, this information corresponds to the position associated with each parish name and the minimum year, according to the original DataFrame. This approach doesn't allow repetitions since the condition for replacing the information in the dictionary is strict (&lt;).</li> </ol> In\u00a0[360]: Copied! <pre>popSizeScania = process_dataframe(censusScania, 'GEOIDNMN', 'AR')\nprint(popSizeScania.shape)\npopSizeScania.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')\n</pre> popSizeScania = process_dataframe(censusScania, 'GEOIDNMN', 'AR') print(popSizeScania.shape) popSizeScania.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')  <pre>(473, 7)\n</pre> Out[360]: LANGENNMN GEOIDNMN GEOIDTYP AR KON BEF_TOT BEF_GENTOT 57393 MALM\u00d6HUS L\u00c4N HELSINGBORGS 0 1775 1 1290 1290 <p>Additional functions</p> In\u00a0[361]: Copied! <pre># popSizeScania_rep_names = get_Names(\n#     popSizeScania, 'GEOIDNMN').unique().tolist()\n# parishScaniaMap_names = get_Names(parishScaniaMap, 'G_NAME').unique().tolist()\n\n# set1 = set(popSizeScania_rep_names)  # 403 different names\n# set2 = set(parishScaniaMap_names)  # 411 different names\n</pre> # popSizeScania_rep_names = get_Names( #     popSizeScania, 'GEOIDNMN').unique().tolist() # parishScaniaMap_names = get_Names(parishScaniaMap, 'G_NAME').unique().tolist()  # set1 = set(popSizeScania_rep_names)  # 403 different names # set2 = set(parishScaniaMap_names)  # 411 different names  In\u00a0[362]: Copied! <pre># new_df = pd.merge(\n#     popSizeScania_rep, parishScaniaMap, left_on='GEOIDNMN', right_on='G_NAME')\n</pre> # new_df = pd.merge( #     popSizeScania_rep, parishScaniaMap, left_on='GEOIDNMN', right_on='G_NAME')  In\u00a0[363]: Copied! <pre># def merge_df_condition(df1, df2, column_df1: str, column_df2: str, threshold):\n#     df1_name = get_Names(df1, column_df1).unique().tolist()\n#     df2_name = get_Names(df2, column_df2).unique().tolist()\n#     for i in range(len(df1)):\n#         name_df1_i = df1[column_df1].iloc[i]\n#         for j in range(len(df2)):\n#             name_df2_j = df2[column_df2].iloc[j]\n#             if is_similar(name_df1_i, name_df2_j, threshold) is True:\n\n#                 df = pd.merge(df1, df2, left_on=column_df1,\n#                               right_on=column_df2)\n#     return df\n</pre> # def merge_df_condition(df1, df2, column_df1: str, column_df2: str, threshold): #     df1_name = get_Names(df1, column_df1).unique().tolist() #     df2_name = get_Names(df2, column_df2).unique().tolist() #     for i in range(len(df1)): #         name_df1_i = df1[column_df1].iloc[i] #         for j in range(len(df2)): #             name_df2_j = df2[column_df2].iloc[j] #             if is_similar(name_df1_i, name_df2_j, threshold) is True:  #                 df = pd.merge(df1, df2, left_on=column_df1, #                               right_on=column_df2) #     return df  In\u00a0[364]: Copied! <pre># def find_sim_names(data: pd.DataFrame, heading: str, name_list: list[str], threshold=0.8):\n#     sim_names_set = []\n#     for i in range(len(data)):\n#         name_i = data[heading].iloc[i]\n#         for name in name_list:\n#             if is_similar(name_i, name, threshold) is True:\n#                 sim_names_set = sim_names_set + \\\n#                     [{'name': name, 'name_i': name_i}]\n#     return sim_names_set\n</pre> # def find_sim_names(data: pd.DataFrame, heading: str, name_list: list[str], threshold=0.8): #     sim_names_set = [] #     for i in range(len(data)): #         name_i = data[heading].iloc[i] #         for name in name_list: #             if is_similar(name_i, name, threshold) is True: #                 sim_names_set = sim_names_set + \\ #                     [{'name': name, 'name_i': name_i}] #     return sim_names_set  In\u00a0[365]: Copied! <pre># def find_diff_names(data: pd.DataFrame, heading: str, name_list: list[str], threshold=0.8):\n#     diff_names_list = []\n#     for i in range(len(data)):\n#         name_i = data[heading].iloc[i]\n#         for name in name_list:\n#             if is_similar(name_i, name, threshold) is True:\n#                 continue\n#             else:\n#                 distancia_i = levenshtein_distance(name, name_i)\n#                 diff_names_list = diff_names_list + \\\n#                     [{'name': name, 'name_i': name_i, 'distance': distancia_i}]\n#     return diff_names_list\n</pre> # def find_diff_names(data: pd.DataFrame, heading: str, name_list: list[str], threshold=0.8): #     diff_names_list = [] #     for i in range(len(data)): #         name_i = data[heading].iloc[i] #         for name in name_list: #             if is_similar(name_i, name, threshold) is True: #                 continue #             else: #                 distancia_i = levenshtein_distance(name, name_i) #                 diff_names_list = diff_names_list + \\ #                     [{'name': name, 'name_i': name_i, 'distance': distancia_i}] #     return diff_names_list  In\u00a0[366]: Copied! <pre># from Levenshtein import distance as levenshtein_distance\n\n\n# def find_similar_names(data: pd.DataFrame, heading: str, name: str, threshold=0.8):\n#     aux_dict = {}\n#     for i in range(len(data)):\n#         name_i = data[heading].iloc[i]\n#         max_len_i = max(len(name), len(name_i))\n#         if max_len_i == 0:\n#             continue\n#         if (max_len_i - levenshtein_distance(name, name_i)) / max_len_i &gt;= threshold:\n#             aux_dict[name_i] = {'distance': levenshtein_distance(\n#                 name, name_i), 'position': i}\n#     final_positions = [value['position'] for key, value in aux_dict.items()]\n#     return data.iloc[final_positions]\n</pre> # from Levenshtein import distance as levenshtein_distance   # def find_similar_names(data: pd.DataFrame, heading: str, name: str, threshold=0.8): #     aux_dict = {} #     for i in range(len(data)): #         name_i = data[heading].iloc[i] #         max_len_i = max(len(name), len(name_i)) #         if max_len_i == 0: #             continue #         if (max_len_i - levenshtein_distance(name, name_i)) / max_len_i &gt;= threshold: #             aux_dict[name_i] = {'distance': levenshtein_distance( #                 name, name_i), 'position': i} #     final_positions = [value['position'] for key, value in aux_dict.items()] #     return data.iloc[final_positions]  In\u00a0[367]: Copied! <pre># def filter_data_by_name(data: pd.DataFrame, heading: str, input_names: list[str]):\n#     output_names = []\n#     for name in input_names:\n#         filter_data = check_name(data, heading, name)\n#         if len(filter_data) == 0:\n#             output_names = output_names + [name]\n#         else:\n#             continue\n#     return output_names\n</pre> # def filter_data_by_name(data: pd.DataFrame, heading: str, input_names: list[str]): #     output_names = [] #     for name in input_names: #         filter_data = check_name(data, heading, name) #         if len(filter_data) == 0: #             output_names = output_names + [name] #         else: #             continue #     return output_names  In\u00a0[368]: Copied! <pre># %%timeit\n# aux_dict = {}\n# parishMap = gpd.GeoDataFrame()\n\n# for i in range(len(parishScaniaMap)):\n#     name_i = parishScaniaMap['G_NAME'].iloc[i]\n#     ar_i = parishScaniaMap['GET_END_YE'].iloc[i]\n#     if name_i in aux_dict:\n#         if ar_i &lt; aux_dict[name_i]['min']:\n#             aux_dict[name_i] = {'min': ar_i, 'position': i}\n#     else:\n#         aux_dict[name_i] = {'min': ar_i, 'position': i}\n# final_positions = [value['position'] for key, value in aux_dict.items()]\n# parishMap = parishScaniaMap.iloc[final_positions]\n# print(parishMap.shape)\n</pre> # %%timeit # aux_dict = {} # parishMap = gpd.GeoDataFrame()  # for i in range(len(parishScaniaMap)): #     name_i = parishScaniaMap['G_NAME'].iloc[i] #     ar_i = parishScaniaMap['GET_END_YE'].iloc[i] #     if name_i in aux_dict: #         if ar_i &lt; aux_dict[name_i]['min']: #             aux_dict[name_i] = {'min': ar_i, 'position': i} #     else: #         aux_dict[name_i] = {'min': ar_i, 'position': i} # final_positions = [value['position'] for key, value in aux_dict.items()] # parishMap = parishScaniaMap.iloc[final_positions] # print(parishMap.shape)  In\u00a0[369]: Copied! <pre># df1 = southeastParishMap\n# df2 = popSizeScania_rep\n\n# df_matches = fuzzy_match(\n#     df1,\n#     df2,\n#     'ParishName',\n#     'GEOIDNMN',\n#     threshold=90,\n#     limit=1\n# )\n\n# df_output = df1.merge(\n#     df_matches,\n#     how='left',\n#     left_index=True,\n#     right_on='df_left_id'\n# ).merge(\n#     df2,\n#     how='left',\n#     left_on='df_right_id',\n#     right_index=True,\n#     suffixes=['_df1', '_df2']\n# )\n\n# # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\n# df_output.set_index('df_left_id', inplace=True)\n\n# # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\n# df_output.index.name = 'id'\n</pre> # df1 = southeastParishMap # df2 = popSizeScania_rep  # df_matches = fuzzy_match( #     df1, #     df2, #     'ParishName', #     'GEOIDNMN', #     threshold=90, #     limit=1 # )  # df_output = df1.merge( #     df_matches, #     how='left', #     left_index=True, #     right_on='df_left_id' # ).merge( #     df2, #     how='left', #     left_on='df_right_id', #     right_index=True, #     suffixes=['_df1', '_df2'] # )  # # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table # df_output.set_index('df_left_id', inplace=True)  # # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching # df_output.index.name = 'id'"},{"location":"PlagueProject/ejemplo_clase/","title":"Ejemplo clase","text":"In\u00a0[\u00a0]: Copied! <pre>class Modelo:\n    def __init__(self, nombre, edad):\n        self.nombre = nombre\n        self.edad = edad\n\n    def solve(self):\n        return self.nombre + \" \" + str(self.edad)\n</pre> class Modelo:     def __init__(self, nombre, edad):         self.nombre = nombre         self.edad = edad      def solve(self):         return self.nombre + \" \" + str(self.edad) In\u00a0[\u00a0]: Copied! <pre>m = Modelo(\"Juan\", 20)\nprint(m.solve())\nprint(m.save())\n</pre> m = Modelo(\"Juan\", 20) print(m.solve()) print(m.save())"},{"location":"PlagueProject/funct_process_data/","title":"funct_process_data","text":"<p>Python 3.11.2 Import packages</p> In\u00a0[\u00a0]: Copied! <pre>from shapely import wkt\nfrom shapely.geometry import shape\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import Point\nimport scipy.integrate as scipy\nimport scipy.optimize as optimize\nimport scipy.stats as stats\nfrom thefuzz import process\nfrom Levenshtein import distance as levenshtein_distance\nimport pandas as pd\nimport numpy as np\nimport pylab as pl\n# import random\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom collections import defaultdict\n# import json  # for pretty printing\nimport geopandas as gpd\nimport os\nimport re\n# import folium\n# from mapclassify import classify\nimport math\nfrom datetime import datetime, timedelta\nimport networkx as nx # for network analysis, graphs\nimport plotly.express as px\n# from skopt import gp_minimize # for Bayesian optimization\nfrom pandas.tseries.offsets import DateOffset, MonthEnd\n</pre> from shapely import wkt from shapely.geometry import shape from shapely.geometry import Polygon from shapely.geometry import Point import scipy.integrate as scipy import scipy.optimize as optimize import scipy.stats as stats from thefuzz import process from Levenshtein import distance as levenshtein_distance import pandas as pd import numpy as np import pylab as pl # import random import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from collections import defaultdict # import json  # for pretty printing import geopandas as gpd import os import re # import folium # from mapclassify import classify import math from datetime import datetime, timedelta import networkx as nx # for network analysis, graphs import plotly.express as px # from skopt import gp_minimize # for Bayesian optimization from pandas.tseries.offsets import DateOffset, MonthEnd In\u00a0[\u00a0]: Copied! <pre>import locale\nlocale.setlocale(locale.LC_TIME, 'en_US.UTF-8')\n</pre> import locale locale.setlocale(locale.LC_TIME, 'en_US.UTF-8') In\u00a0[\u00a0]: Copied! <pre>TEST = \"change\"\n</pre> TEST = \"change\" In\u00a0[\u00a0]: Copied! <pre># Function to get all parishes from a specific region\ndef parishesByregion(df: pd.DataFrame, region: str) -&gt; pd.DataFrame:\n    return df.loc[df['Region'] == region]\n</pre> # Function to get all parishes from a specific region def parishesByregion(df: pd.DataFrame, region: str) -&gt; pd.DataFrame:     return df.loc[df['Region'] == region] In\u00a0[\u00a0]: Copied! <pre>def get_Names(data: pd.DataFrame, heading: str) -&gt; list:\n    return data[heading]\n</pre> def get_Names(data: pd.DataFrame, heading: str) -&gt; list:     return data[heading] In\u00a0[\u00a0]: Copied! <pre>def replace_strings_and_regex(dataframe: pd.DataFrame, heading: str, patterns):\n    for pattern in patterns:\n        dataframe.loc[:, heading] = dataframe.loc[:, heading].apply(\n            lambda x: re.sub(pattern, '', x))\n    # print(dataframe.loc[:, heading].__dict__)\n    return dataframe\n</pre> def replace_strings_and_regex(dataframe: pd.DataFrame, heading: str, patterns):     for pattern in patterns:         dataframe.loc[:, heading] = dataframe.loc[:, heading].apply(             lambda x: re.sub(pattern, '', x))     # print(dataframe.loc[:, heading].__dict__)     return dataframe In\u00a0[\u00a0]: Copied! <pre>def process_dataframe_rep(df, groupby_column, year_column):\n    # Group a Pandas DataFrame by a column\n    parish_grp = df.groupby([groupby_column])\n    # Get the unique values of a column as a list\n    parish_grp_name = parish_grp[groupby_column].unique().tolist()\n    result_df = pd.DataFrame()\n    for name in parish_grp_name:\n        grp_name = parish_grp.get_group(name[0])\n        result_df = pd.concat(\n            [result_df, (grp_name[grp_name[year_column] == grp_name[year_column].min()])], axis=0)\n    return result_df\n</pre> def process_dataframe_rep(df, groupby_column, year_column):     # Group a Pandas DataFrame by a column     parish_grp = df.groupby([groupby_column])     # Get the unique values of a column as a list     parish_grp_name = parish_grp[groupby_column].unique().tolist()     result_df = pd.DataFrame()     for name in parish_grp_name:         grp_name = parish_grp.get_group(name[0])         result_df = pd.concat(             [result_df, (grp_name[grp_name[year_column] == grp_name[year_column].min()])], axis=0)     return result_df In\u00a0[\u00a0]: Copied! <pre>def process_dataframe(df, column1: str, column2: str):\n    aux_dict = {}\n    for i in range(len(df)):\n        name_i = df[column1].iloc[i]\n        ar_i = df[column2].iloc[i]\n        if name_i in aux_dict:\n            if ar_i &lt; aux_dict[name_i]['min']:\n                aux_dict[name_i] = {'min': ar_i, 'position': i}\n        else:\n            aux_dict[name_i] = {'min': ar_i, 'position': i}\n    final_positions = [value['position'] for key, value in aux_dict.items()]\n    return df.iloc[final_positions]\n</pre> def process_dataframe(df, column1: str, column2: str):     aux_dict = {}     for i in range(len(df)):         name_i = df[column1].iloc[i]         ar_i = df[column2].iloc[i]         if name_i in aux_dict:             if ar_i &lt; aux_dict[name_i]['min']:                 aux_dict[name_i] = {'min': ar_i, 'position': i}         else:             aux_dict[name_i] = {'min': ar_i, 'position': i}     final_positions = [value['position'] for key, value in aux_dict.items()]     return df.iloc[final_positions] In\u00a0[\u00a0]: Copied! <pre>def check_name(data: pd.DataFrame, heading: str, name: str):\n    filt_name = data[heading].str.contains(name, na=False)\n    return data.loc[filt_name]\n</pre> def check_name(data: pd.DataFrame, heading: str, name: str):     filt_name = data[heading].str.contains(name, na=False)     return data.loc[filt_name] In\u00a0[\u00a0]: Copied! <pre>def is_similar(name1, name2, threshold=0.8):\n    max_len = max(len(name1), len(name2))\n    return levenshtein_distance(name1, name2) / max_len &lt; threshold\n</pre> def is_similar(name1, name2, threshold=0.8):     max_len = max(len(name1), len(name2))     return levenshtein_distance(name1, name2) / max_len &lt; threshold In\u00a0[\u00a0]: Copied! <pre>def fuzzy_match(\n    df_left, df_right, column_left, column_right, threshold=90, limit=1\n):\n    # Create a series\n    series_matches = df_left[column_left].apply(\n        # Creates a series with id from df_left and column name _column_left_, with _limit_ matches per item\n        lambda x: process.extract(x, df_right[column_right], limit=limit)\n    )\n\n    # Convert matches to a tidy dataframe\n    df_matches = series_matches.to_frame()\n    # Convert list of matches to rows\n    df_matches = df_matches.explode(column_left)\n    df_matches[\n        ['match_string', 'match_score', 'df_right_id']\n    ] = pd.DataFrame(df_matches[column_left].tolist(), index=df_matches.index)       # Convert match tuple to columns\n    # Drop column of match tuples\n    df_matches.drop(column_left, axis=1, inplace=True)\n\n    # Reset index, as in creating a tidy dataframe we've introduced multiple rows per id, so that no longer functions well as the index\n    if df_matches.index.name:\n        index_name = df_matches.index.name     # Stash index name\n    else:\n        index_name = 'index'        # Default used by pandas\n    df_matches.reset_index(inplace=True)\n    # The previous index has now become a column: rename for ease of reference\n    df_matches.rename(columns={index_name: 'df_left_id'}, inplace=True)\n\n    # Drop matches below threshold\n    df_matches.drop(\n        df_matches.loc[df_matches['match_score'] &lt; threshold].index,\n        inplace=True\n    )\n\n    return df_matches\n</pre> def fuzzy_match(     df_left, df_right, column_left, column_right, threshold=90, limit=1 ):     # Create a series     series_matches = df_left[column_left].apply(         # Creates a series with id from df_left and column name _column_left_, with _limit_ matches per item         lambda x: process.extract(x, df_right[column_right], limit=limit)     )      # Convert matches to a tidy dataframe     df_matches = series_matches.to_frame()     # Convert list of matches to rows     df_matches = df_matches.explode(column_left)     df_matches[         ['match_string', 'match_score', 'df_right_id']     ] = pd.DataFrame(df_matches[column_left].tolist(), index=df_matches.index)       # Convert match tuple to columns     # Drop column of match tuples     df_matches.drop(column_left, axis=1, inplace=True)      # Reset index, as in creating a tidy dataframe we've introduced multiple rows per id, so that no longer functions well as the index     if df_matches.index.name:         index_name = df_matches.index.name     # Stash index name     else:         index_name = 'index'        # Default used by pandas     df_matches.reset_index(inplace=True)     # The previous index has now become a column: rename for ease of reference     df_matches.rename(columns={index_name: 'df_left_id'}, inplace=True)      # Drop matches below threshold     df_matches.drop(         df_matches.loc[df_matches['match_score'] &lt; threshold].index,         inplace=True     )      return df_matches <p>Adding geographical characteristics to the data</p> In\u00a0[\u00a0]: Copied! <pre>def get_area(gpd: gpd.GeoDataFrame, heading: str = 'geometry'):\n    for i in range(len(gpd)):\n        gpd['area_m2'] = shape(gpd.loc[i][heading]).area\n        gpd['area_km2'] = gpd['area_m2']/1000000\n    return gpd\n</pre> def get_area(gpd: gpd.GeoDataFrame, heading: str = 'geometry'):     for i in range(len(gpd)):         gpd['area_m2'] = shape(gpd.loc[i][heading]).area         gpd['area_km2'] = gpd['area_m2']/1000000     return gpd In\u00a0[\u00a0]: Copied! <pre>def get_centroid(gpd: gpd.GeoDataFrame):\n    for i in range(len(gpd)):\n        gpd.loc[i, 'centroid'] = gpd.geometry.centroid[i]\n    return gpd\n</pre> def get_centroid(gpd: gpd.GeoDataFrame):     for i in range(len(gpd)):         gpd.loc[i, 'centroid'] = gpd.geometry.centroid[i]     return gpd In\u00a0[\u00a0]: Copied! <pre>def distance_btw_centroids(gpd: gpd.GeoDataFrame):\n    for i in range(len(gpd)):\n        gpd.loc[i, 'distance'] = gpd.geometry.distance(gpd.centroid[i])\n    return gpd\n</pre> def distance_btw_centroids(gpd: gpd.GeoDataFrame):     for i in range(len(gpd)):         gpd.loc[i, 'distance'] = gpd.geometry.distance(gpd.centroid[i])     return gpd In\u00a0[\u00a0]: Copied! <pre>def maxDays(gdf, column_EndDays: str = 'EndDaysPlague'):\n        return gdf[column_EndDays].max()  # Get the maximum value of a column\n</pre> def maxDays(gdf, column_EndDays: str = 'EndDaysPlague'):         return gdf[column_EndDays].max()  # Get the maximum value of a column <p>Computing the distance between the centroids and shared borders</p> In\u00a0[\u00a0]: Copied! <pre>def compute_info(gdp: gpd.GeoDataFrame,\n                 column_name: str = 'ParishName',\n                 column_geometry: str = 'geometry',\n                 column_centroid: str = 'centroid',\n                 units: int = 1) -&gt; dict:\n\n    nPolygons = len(gdp)\n    info = defaultdict(dict)\n\n    for i in range(nPolygons):\n        polygon_i = gdp.iloc[i][column_geometry]\n        centroid_i = gdp.loc[i, column_centroid]\n        name_i = gdp.iloc[i][column_name]\n\n        for j in range(i+1, nPolygons):\n            polygon_j = gdp.iloc[j][column_geometry]\n            centroid_j = gdp.loc[j, column_centroid]\n            name_j = gdp.iloc[j][column_name]\n\n            distance = centroid_i.distance(centroid_j) / units * 1.0\n            info[\"distance\"][(i, j)] = distance  # in meters\n            info[\"distance\"][(j, i)] = distance  # in meters\n            info[\"distance\"][(name_i, name_j)] = distance  # in meters\n            info[\"distance\"][(name_j, name_i)] = distance  # in meters\n\n            shared_border = polygon_i.intersection(polygon_j)\n            info[\"shared_border\"][(\n                i, j)] = shared_border.length if shared_border != None else 0  # in meters\n            info[\"shared_border\"][(\n                j, i)] = shared_border.length if shared_border != None else 0  # in meters\n            info[\"shared_border\"][(name_i, name_j)\n                                  ] = info[\"shared_border\"][(i, j)]  # in meters\n            info[\"shared_border\"][(name_j, name_i)\n                                  ] = info[\"shared_border\"][(j, i)]  # in meters\n\n    return info\n</pre> def compute_info(gdp: gpd.GeoDataFrame,                  column_name: str = 'ParishName',                  column_geometry: str = 'geometry',                  column_centroid: str = 'centroid',                  units: int = 1) -&gt; dict:      nPolygons = len(gdp)     info = defaultdict(dict)      for i in range(nPolygons):         polygon_i = gdp.iloc[i][column_geometry]         centroid_i = gdp.loc[i, column_centroid]         name_i = gdp.iloc[i][column_name]          for j in range(i+1, nPolygons):             polygon_j = gdp.iloc[j][column_geometry]             centroid_j = gdp.loc[j, column_centroid]             name_j = gdp.iloc[j][column_name]              distance = centroid_i.distance(centroid_j) / units * 1.0             info[\"distance\"][(i, j)] = distance  # in meters             info[\"distance\"][(j, i)] = distance  # in meters             info[\"distance\"][(name_i, name_j)] = distance  # in meters             info[\"distance\"][(name_j, name_i)] = distance  # in meters              shared_border = polygon_i.intersection(polygon_j)             info[\"shared_border\"][(                 i, j)] = shared_border.length if shared_border != None else 0  # in meters             info[\"shared_border\"][(                 j, i)] = shared_border.length if shared_border != None else 0  # in meters             info[\"shared_border\"][(name_i, name_j)                                   ] = info[\"shared_border\"][(i, j)]  # in meters             info[\"shared_border\"][(name_j, name_i)                                   ] = info[\"shared_border\"][(j, i)]  # in meters      return info In\u00a0[\u00a0]: Copied! <pre># Assigning colors (red -&gt; Plague and blue -&gt; NoPlague)\ndef colorByColumn(gpd: gpd.GeoDataFrame, heading: str = 'BeginPlaguePeriod'):\n    gpd['color'] = gpd[heading].map(lambda x: 'blue' if pd.isna(x) else 'red')\n    pass\n</pre> # Assigning colors (red -&gt; Plague and blue -&gt; NoPlague) def colorByColumn(gpd: gpd.GeoDataFrame, heading: str = 'BeginPlaguePeriod'):     gpd['color'] = gpd[heading].map(lambda x: 'blue' if pd.isna(x) else 'red')     pass In\u00a0[\u00a0]: Copied! <pre># Assigning colors (red -&gt; Plague and blue -&gt; NoPlague)\ndef classByPlague(gpd: gpd.GeoDataFrame, heading: str = 'BeginPlaguePeriod'):\n    gpd['plague'] = gpd[heading].map(lambda x: 0 if pd.isna(x) else 1)\n    pass\n</pre> # Assigning colors (red -&gt; Plague and blue -&gt; NoPlague) def classByPlague(gpd: gpd.GeoDataFrame, heading: str = 'BeginPlaguePeriod'):     gpd['plague'] = gpd[heading].map(lambda x: 0 if pd.isna(x) else 1)     pass In\u00a0[\u00a0]: Copied! <pre>def begin_days_between(d1, d2):\n    if (type(d1) == float and math.isnan(d1)) or \\\n       (type(d2) == float and math.isnan(d2)):\n        return None\n    return abs((d2 - d1).days)\n</pre> def begin_days_between(d1, d2):     if (type(d1) == float and math.isnan(d1)) or \\        (type(d2) == float and math.isnan(d2)):         return None     return abs((d2 - d1).days) In\u00a0[\u00a0]: Copied! <pre>def end_days_between(d1, d2):\n    if (type(d1) == float and math.isnan(d1)) or (type(d2) == float and math.isnan(d2)):\n        return None\n    # Create first day of the first month\n    first_day_d1 = datetime(d1.year, d1.month, 1)\n    # Create last day of the second month\n    if d2.month == 12:\n        last_day_d2 = datetime(d2.year + 1, 1, 1) - timedelta(days=1)\n    else:\n        last_day_d2 = datetime(d2.year, d2.month + 1, 1) - timedelta(days=1)\n\n    return abs((last_day_d2 - first_day_d1).days)\n</pre> def end_days_between(d1, d2):     if (type(d1) == float and math.isnan(d1)) or (type(d2) == float and math.isnan(d2)):         return None     # Create first day of the first month     first_day_d1 = datetime(d1.year, d1.month, 1)     # Create last day of the second month     if d2.month == 12:         last_day_d2 = datetime(d2.year + 1, 1, 1) - timedelta(days=1)     else:         last_day_d2 = datetime(d2.year, d2.month + 1, 1) - timedelta(days=1)      return abs((last_day_d2 - first_day_d1).days) In\u00a0[\u00a0]: Copied! <pre>def convert_to_int(row, death_col : str ='VictimsNumber'):\n    if pd.notna(row[death_col]):\n        return int(row[death_col])\n    else:\n        return None\n</pre> def convert_to_int(row, death_col : str ='VictimsNumber'):     if pd.notna(row[death_col]):         return int(row[death_col])     else:         return None In\u00a0[\u00a0]: Copied! <pre>def sort_by_date(gdf, column_date: str = 'new_format_BeginPlaguePeriod'):\n    gdf_copy = gdf.copy()\n    gdf_copy.sort_values(by=[column_date],   # Row or columns names to sort by\n                    axis=0,       # Sort Rows axis = 0\n                    ascending=True,  # Sort ascending or descending?\n                    inplace=True     # Modify the DataFrame in place (do not create a new object)\n                    )\n    gdf_copy.reset_index(drop=True, inplace=True\n                         )\n    return gdf_copy\n</pre> def sort_by_date(gdf, column_date: str = 'new_format_BeginPlaguePeriod'):     gdf_copy = gdf.copy()     gdf_copy.sort_values(by=[column_date],   # Row or columns names to sort by                     axis=0,       # Sort Rows axis = 0                     ascending=True,  # Sort ascending or descending?                     inplace=True     # Modify the DataFrame in place (do not create a new object)                     )     gdf_copy.reset_index(drop=True, inplace=True                          )     return gdf_copy In\u00a0[\u00a0]: Copied! <pre>def add_Begin_End_days(gdf, begin_column:str = 'new_format_BeginPlaguePeriod', end_column:str = 'new_format_EndPlaguePeriod'):\n    gdf_copy = gdf.copy()\n    # Create a new column called \"BeginDaysPlague\"\n    gdf_copy[\"BeginDaysPlague\"] = gdf.apply(lambda row: begin_days_between(gdf[begin_column].iloc[0]\n                                                                         ,row[begin_column])\n                                                                         , axis=1  # axis = 1 means apply function to each row\n    )\n    \n    # Create a new column called \"EndDaysPlague\"\n    gdf_copy['EndDaysPlague'] = gdf.apply(lambda row: end_days_between(gdf[begin_column].iloc[0]\n                                            , row[end_column]) if pd.notna(row[end_column]) else None\n                                            , axis=1)\n\n    # Replace NaN values with a value in some columns (e.g., 0)\n    gdf_copy['BeginDaysPlague'].fillna(0, inplace=True\n                                       )\n    gdf_copy['EndDaysPlague'].fillna(0, inplace=True\n                                     )\n    #gdf_copy[death_column].fillna(None, inplace=True)\n    \n    # Changing the type of some columns from float to integer for the optimization process\n    gdf_copy['BeginDaysPlague'] = gdf_copy['BeginDaysPlague'].astype(int)\n    gdf_copy['EndDaysPlague'] = gdf_copy['EndDaysPlague'].astype(int)\n    #gdf_copy[death_column] = gdf_copy['VictimsNumber'].astype(int)\n        \n    gdf_copy.reset_index(drop=True, inplace=True\n                         )\n    return gdf_copy\n</pre> def add_Begin_End_days(gdf, begin_column:str = 'new_format_BeginPlaguePeriod', end_column:str = 'new_format_EndPlaguePeriod'):     gdf_copy = gdf.copy()     # Create a new column called \"BeginDaysPlague\"     gdf_copy[\"BeginDaysPlague\"] = gdf.apply(lambda row: begin_days_between(gdf[begin_column].iloc[0]                                                                          ,row[begin_column])                                                                          , axis=1  # axis = 1 means apply function to each row     )          # Create a new column called \"EndDaysPlague\"     gdf_copy['EndDaysPlague'] = gdf.apply(lambda row: end_days_between(gdf[begin_column].iloc[0]                                             , row[end_column]) if pd.notna(row[end_column]) else None                                             , axis=1)      # Replace NaN values with a value in some columns (e.g., 0)     gdf_copy['BeginDaysPlague'].fillna(0, inplace=True                                        )     gdf_copy['EndDaysPlague'].fillna(0, inplace=True                                      )     #gdf_copy[death_column].fillna(None, inplace=True)          # Changing the type of some columns from float to integer for the optimization process     gdf_copy['BeginDaysPlague'] = gdf_copy['BeginDaysPlague'].astype(int)     gdf_copy['EndDaysPlague'] = gdf_copy['EndDaysPlague'].astype(int)     #gdf_copy[death_column] = gdf_copy['VictimsNumber'].astype(int)              gdf_copy.reset_index(drop=True, inplace=True                          )     return gdf_copy In\u00a0[\u00a0]: Copied! <pre># Function to call the data from the excel files\ndef get_parish_data(parish_name, parish_folder):\n    parish_path = os.path.join(parish_folder, parish_name + '.xlsx')\n    parish = pd.read_excel(parish_path, sheet_name='Plague')\n\n    # Convert 'EndDate' to datetime with appropriate format\n    parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%b %Y')\n    parish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')\n    parish['first_day'] = parish['NewEndDate'].dt.to_timestamp()\n    parish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')\n\n    # Add a column with the days since the first date and then cumsum\n    parish['Days'] = parish['last_day'].dt.daysinmonth\n    parish['Days'] = parish['Days'].cumsum()\n    return parish\n</pre> # Function to call the data from the excel files def get_parish_data(parish_name, parish_folder):     parish_path = os.path.join(parish_folder, parish_name + '.xlsx')     parish = pd.read_excel(parish_path, sheet_name='Plague')      # Convert 'EndDate' to datetime with appropriate format     parish['NewEndDate'] = pd.to_datetime(parish['EndDate'], format='%b %Y')     parish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')     parish['first_day'] = parish['NewEndDate'].dt.to_timestamp()     parish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')      # Add a column with the days since the first date and then cumsum     parish['Days'] = parish['last_day'].dt.daysinmonth     parish['Days'] = parish['Days'].cumsum()     return parish In\u00a0[\u00a0]: Copied! <pre># Function to get the population of a specific parish\ndef get_parish_info(parish_name, df: pd.DataFrame, column_name='ParishName', column_pop='BEF1699'):\n    pop_df = df[(df[column_name] == parish_name)][column_pop]\n    name_df = df[(df[column_name] == parish_name)][column_name]\n    \n    if not pop_df.empty and not name_df.empty:\n        pop_parish = pop_df.values[0]\n        name_parish = name_df.values[0]\n    else:\n        pop_parish = None\n        name_parish = None\n\n    return pop_parish, name_parish\n</pre> # Function to get the population of a specific parish def get_parish_info(parish_name, df: pd.DataFrame, column_name='ParishName', column_pop='BEF1699'):     pop_df = df[(df[column_name] == parish_name)][column_pop]     name_df = df[(df[column_name] == parish_name)][column_name]          if not pop_df.empty and not name_df.empty:         pop_parish = pop_df.values[0]         name_parish = name_df.values[0]     else:         pop_parish = None         name_parish = None      return pop_parish, name_parish <p>Defining the seasonal function</p> In\u00a0[\u00a0]: Copied! <pre>def gaussian(x, media, std):\n    return np.exp(-((x - media) ** 2) / (2 * std ** 2))\n</pre> def gaussian(x, media, std):     return np.exp(-((x - media) ** 2) / (2 * std ** 2)) In\u00a0[\u00a0]: Copied! <pre>def seasonal_transmission_rate(t, bump_center, bump_width, bump_height):\n    return bump_height * gaussian(t % 365, bump_center, bump_width) + bump_height * gaussian(t % 365 - 365, bump_center, bump_width) + bump_height * gaussian(t % 365 + 365, bump_center, bump_width)\n</pre> def seasonal_transmission_rate(t, bump_center, bump_width, bump_height):     return bump_height * gaussian(t % 365, bump_center, bump_width) + bump_height * gaussian(t % 365 - 365, bump_center, bump_width) + bump_height * gaussian(t % 365 + 365, bump_center, bump_width) <p>def transmission_matrix_p(gdf: gpd.GeoDataFrame, column_geometry: str = 'geometry', column_centroid: str = 'centroid', column_pop: str = 'BEF1699', column_name: str = 'ParishName'):</p> <pre><code># Calculate distances between all centroids in meters\ncentroid_distances = gdf[column_centroid].apply(\n    lambda x: gdf[column_centroid].apply(lambda y: x.distance(y))).values</code></pre> <pre><code># Calculate population products for all pairs of polygons\npop_products = np.outer(gdf[column_pop], gdf[column_pop])</code></pre> <pre><code># Create a boolean matrix to identify intersecting polygons\nintersecting_polygons = gdf[column_geometry].apply(\n    lambda x: gdf[column_geometry].intersects(x)).values</code></pre> <pre><code># Create a boolean matrix to identify same names\nsame_names = gdf[column_name].apply(lambda x: gdf[column_name] == x).values</code></pre> <pre><code># For non-intersecting polygons with the same name, set the distance to infinity\ncentroid_distances[np.logical_or(~intersecting_polygons, same_names)] = np.inf</code></pre> <pre><code># Replace diagonal elements in centroid_distances with 1 to avoid division by\nnp.fill_diagonal(centroid_distances, 1)</code></pre> <pre><code># Calculate the transmission matrix\np_matrix = (pop_products / (centroid_distances**2))\nnp.fill_diagonal(p_matrix, 0)\nreturn p_matrix </code></pre> <p>Definition without checking if the polygons intersect def transmission_matrix2_p(gdf: gpd.GeoDataFrame, column_geometry: str = 'geometry', column_centroid: str = 'centroid', column_pop: str = 'BEF1699', column_name: str = 'ParishName'):</p> <pre><code># Calculate distances between all centroids in meters\ncentroid_distances = gdf[column_centroid].apply(\n    lambda x: gdf[column_centroid].apply(lambda y: x.distance(y))).values</code></pre> <pre><code># Calculate population products for all pairs of polygons\npop_products = np.outer(gdf[column_pop], gdf[column_pop])</code></pre> <pre><code># Create a boolean matrix to identify same names\nsame_names = gdf[column_name].apply(lambda x: gdf[column_name] == x).values</code></pre> <pre><code># For polygons with the same name, set the distance to infinity\ncentroid_distances[same_names] = np.inf</code></pre> <pre><code># Replace diagonal elements in centroid_distances with 1 to avoid division by zero\nnp.fill_diagonal(centroid_distances, 1)</code></pre> <pre><code># Calculate the transmission matrix\np_weight = (pop_products / (centroid_distances**2))\nnp.fill_diagonal(p_weight, 0)\nreturn p_weight</code></pre> In\u00a0[\u00a0]: Copied! <pre># Definition considering unique names\ndef transmission_matrix_beta(gdf: gpd.GeoDataFrame, beta:np.array, column_name: str = 'ParishName'):\n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n    beta_matrix = np.zeros((len_unique_names,len_unique_names), dtype=float)\n    np.fill_diagonal(beta_matrix, beta) \n    return beta_matrix\n</pre> # Definition considering unique names def transmission_matrix_beta(gdf: gpd.GeoDataFrame, beta:np.array, column_name: str = 'ParishName'):     unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)     beta_matrix = np.zeros((len_unique_names,len_unique_names), dtype=float)     np.fill_diagonal(beta_matrix, beta)      return beta_matrix In\u00a0[\u00a0]: Copied! <pre># Beta matrix considering repeated names\ndef beta_matrix(gdf: gpd.GeoDataFrame, beta:np.array, column_name: str = 'ParishName'):\n    names = gdf[column_name]\n    len_names = len(names)\n    beta_matrix = np.zeros((len_names,len_names), dtype=float)\n    np.fill_diagonal(beta_matrix, beta) \n    return beta_matrix\n</pre> # Beta matrix considering repeated names def beta_matrix(gdf: gpd.GeoDataFrame, beta:np.array, column_name: str = 'ParishName'):     names = gdf[column_name]     len_names = len(names)     beta_matrix = np.zeros((len_names,len_names), dtype=float)     np.fill_diagonal(beta_matrix, beta)      return beta_matrix In\u00a0[\u00a0]: Copied! <pre># Beta matrix considering repeated names\ndef identity_matrix(gdf: gpd.GeoDataFrame\n                    , column_name: str = 'ParishName'):\n    names = gdf[column_name]\n    len_names = len(names)\n    matrix = np.zeros((len_names,len_names), dtype=float)\n    np.fill_diagonal(matrix, 1.0) \n    return matrix\n</pre> # Beta matrix considering repeated names def identity_matrix(gdf: gpd.GeoDataFrame                     , column_name: str = 'ParishName'):     names = gdf[column_name]     len_names = len(names)     matrix = np.zeros((len_names,len_names), dtype=float)     np.fill_diagonal(matrix, 1.0)      return matrix In\u00a0[\u00a0]: Copied! <pre>def getValueAt(array, n, i, j):\n    if i == j: return 0\n    if i &lt; j:\n        return array[int(j*(j-1)/2) + i]\n    return getValueAt(array, n, j, i)\n</pre> def getValueAt(array, n, i, j):     if i == j: return 0     if i &lt; j:         return array[int(j*(j-1)/2) + i]     return getValueAt(array, n, j, i) In\u00a0[\u00a0]: Copied! <pre># Transmission matrix defined for SCENARIO 4.\ndef trans_matrix4(gdf: gpd.GeoDataFrame, beta:np.array, p:np.array, n, column_name: str = 'ParishName', column_geometry: str = 'geometry'):\n    # Get unique parish names \n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n\n    # Initialize the beta matrix\n    beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)\n    np.fill_diagonal(beta_matrix, beta)\n\n    # Initialize the transmission matrix between patches\n    trans_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)\n\n    for i in range(len_unique_names):\n        for j in range(i+1,len_unique_names):\n            name_i = unique_names[i]\n            name_j = unique_names[j]\n            polygon_i = gdf[gdf[column_name] == name_i][column_geometry].values[0]\n            polygon_j = gdf[gdf[column_name] == name_j][column_geometry].values[0]\n            pVal = getValueAt(p, n, i, j)\n\n            if polygon_i.touches(polygon_j) and name_i != name_j:\n                trans_matrix[i,j] = pVal\n                trans_matrix[j,i] = trans_matrix[i,j]\n            else:\n                trans_matrix[i,j] = 0\n                trans_matrix[j,i] = 0\n\n    return beta_matrix + trans_matrix\n</pre> # Transmission matrix defined for SCENARIO 4. def trans_matrix4(gdf: gpd.GeoDataFrame, beta:np.array, p:np.array, n, column_name: str = 'ParishName', column_geometry: str = 'geometry'):     # Get unique parish names      unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)      # Initialize the beta matrix     beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)     np.fill_diagonal(beta_matrix, beta)      # Initialize the transmission matrix between patches     trans_matrix = np.full((len_unique_names, len_unique_names), 0.0, dtype=float)      for i in range(len_unique_names):         for j in range(i+1,len_unique_names):             name_i = unique_names[i]             name_j = unique_names[j]             polygon_i = gdf[gdf[column_name] == name_i][column_geometry].values[0]             polygon_j = gdf[gdf[column_name] == name_j][column_geometry].values[0]             pVal = getValueAt(p, n, i, j)              if polygon_i.touches(polygon_j) and name_i != name_j:                 trans_matrix[i,j] = pVal                 trans_matrix[j,i] = trans_matrix[i,j]             else:                 trans_matrix[i,j] = 0                 trans_matrix[j,i] = 0      return beta_matrix + trans_matrix   <p>def create_symmetric_matrix(array, n): # Create an empty matrix matrixA = np.zeros((n,n))</p> <pre><code># Fill the upper triangular part of the matrix using advanced indexing\nfor i in range(n-1):\n    matrixA[i+1, :i+1] = array[int(i*(i+1)/2) : int(i*(i+1)/2) + (i + 1)]\n# Make the matrix symmetric by adding it to its transpose and subtracting the diagonal\nnpSymMatrixA = matrixA + matrixA.T - np.diag(matrixA.diagonal())    \nreturn npSymMatrixA</code></pre> <p>print(\"=====\")</p> In\u00a0[\u00a0]: Copied! <pre>def total_transmission_matrix(gdf: gpd.GeoDataFrame, beta:np.array, p_coeff: np.array, n, column_geometry: str = 'geometry', \n                           column_centroid: str = 'centroid', column_pop: str = 'BEF1699', \n                           column_name: str = 'ParishName'):\n\n    # Get unique parish names \n    unique_names = gdf[column_name].unique()\n    len_unique_names = len(unique_names)\n\n    # Initialize the beta matrix\n    beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)\n    np.fill_diagonal(beta_matrix, beta)\n\n    # Initialize the gravitational matrix\n    gravitational = np.full((len_unique_names, len_unique_names), 0.0)\n\n    for i in range(len_unique_names):\n        for j in range(i+1,len_unique_names):\n            name_i = unique_names[i]\n            name_j = unique_names[j]\n            centroid_i = gdf[gdf[column_name] == name_i][column_centroid].values[0]\n            centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]\n            pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]\n            pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]\n            pVal = getValueAt(p_coeff, n, i, j)\n\n            if name_i != name_j:\n                gravitational[i,j] = pVal*((pop_i * pop_j) / (centroid_i.distance(centroid_j)**2))\n                gravitational[j,i] = gravitational[i,j]\n            else:\n                gravitational[i,j] = 0\n                gravitational[j,i] = 0\n\n    return  beta_matrix + gravitational\n</pre> def total_transmission_matrix(gdf: gpd.GeoDataFrame, beta:np.array, p_coeff: np.array, n, column_geometry: str = 'geometry',                             column_centroid: str = 'centroid', column_pop: str = 'BEF1699',                             column_name: str = 'ParishName'):      # Get unique parish names      unique_names = gdf[column_name].unique()     len_unique_names = len(unique_names)      # Initialize the beta matrix     beta_matrix = np.zeros((len_unique_names, len_unique_names), dtype=float)     np.fill_diagonal(beta_matrix, beta)      # Initialize the gravitational matrix     gravitational = np.full((len_unique_names, len_unique_names), 0.0)      for i in range(len_unique_names):         for j in range(i+1,len_unique_names):             name_i = unique_names[i]             name_j = unique_names[j]             centroid_i = gdf[gdf[column_name] == name_i][column_centroid].values[0]             centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]             pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]             pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]             pVal = getValueAt(p_coeff, n, i, j)              if name_i != name_j:                 gravitational[i,j] = pVal*((pop_i * pop_j) / (centroid_i.distance(centroid_j)**2))                 gravitational[j,i] = gravitational[i,j]             else:                 gravitational[i,j] = 0                 gravitational[j,i] = 0      return  beta_matrix + gravitational   In\u00a0[\u00a0]: Copied! <pre># Define the transmission matrix for the SEIRD model p equal for all patches\ndef transmission_matrix_p(gdf: gpd.GeoDataFrame, p_coeff: float, \n                           column_centroid: str = 'centroid',\n                           column_pop: str = 'BEF1699', \n                           column_name: str = 'ParishName'):\n    \n    # Reset the index\n    gdf.reset_index(drop=True#, inplace=True\n                    )\n\n    # Get parish names including duplicates\n    unique_names = gdf[column_name]\n    len_unique_names = len(unique_names)\n\n    # Initialize the gravitational matrix\n    gravitational = np.full((len_unique_names, len_unique_names), 0.0)\n\n    for i in range(len_unique_names):\n        for j in range(i+1,len_unique_names):\n            name_i = unique_names[i]\n            name_j = unique_names[j]\n            centroid_i = gdf[gdf[column_name] == name_i][column_centroid].values[0]\n            centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]\n            pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]\n            pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]\n            if name_i != name_j:\n                gravitational[i,j] = (pop_i * pop_j) / (centroid_i.distance(centroid_j)**2)\n                gravitational[j,i] = gravitational[i,j]\n            else:\n                gravitational[i,j] = 0\n                gravitational[j,i] = 0\n    p_matrix = p_coeff * gravitational   \n\n    return p_matrix\n</pre> # Define the transmission matrix for the SEIRD model p equal for all patches def transmission_matrix_p(gdf: gpd.GeoDataFrame, p_coeff: float,                             column_centroid: str = 'centroid',                            column_pop: str = 'BEF1699',                             column_name: str = 'ParishName'):          # Reset the index     gdf.reset_index(drop=True#, inplace=True                     )      # Get parish names including duplicates     unique_names = gdf[column_name]     len_unique_names = len(unique_names)      # Initialize the gravitational matrix     gravitational = np.full((len_unique_names, len_unique_names), 0.0)      for i in range(len_unique_names):         for j in range(i+1,len_unique_names):             name_i = unique_names[i]             name_j = unique_names[j]             centroid_i = gdf[gdf[column_name] == name_i][column_centroid].values[0]             centroid_j = gdf[gdf[column_name] == name_j][column_centroid].values[0]             pop_i = gdf[gdf[column_name] == name_i][column_pop].values[0]             pop_j = gdf[gdf[column_name] == name_j][column_pop].values[0]             if name_i != name_j:                 gravitational[i,j] = (pop_i * pop_j) / (centroid_i.distance(centroid_j)**2)                 gravitational[j,i] = gravitational[i,j]             else:                 gravitational[i,j] = 0                 gravitational[j,i] = 0     p_matrix = p_coeff * gravitational         return p_matrix  In\u00a0[\u00a0]: Copied! <pre>def gravitational_coeff(gdf: gpd.GeoDataFrame\n                        , column_centroid: str = 'centroid'\n                        , column_pop: str = 'BEF1699'\n                        , column_name: str = 'ParishName'):\n\n    # Get unique parish names and create a mapping to indices\n    unique_names = gdf[column_name].unique()\n    name_to_index = {name: index for index, name in enumerate(unique_names)}\n\n    # Calculate distances between all centroids in meters\n    centroid_distances = np.zeros((len(unique_names), len(unique_names)))\n    for name1, index1 in name_to_index.items():\n        for name2, index2 in name_to_index.items():\n            if name1 != name2:\n                centroid1 = gdf[gdf[column_name] == name1][column_centroid].values[0]\n                centroid2 = gdf[gdf[column_name] == name2][column_centroid].values[0]\n                centroid_distances[index1, index2] = centroid1.distance(centroid2)\n    # Set diagonal elements to infinity to avoid division by zero later\n    np.fill_diagonal(centroid_distances, np.inf)\n\n    # Calculate population products for all pairs of polygons\n    pop_products = np.zeros((len(unique_names), len(unique_names)))\n    for name1, index1 in name_to_index.items():\n        for name2, index2 in name_to_index.items():\n            pop1 = gdf[gdf[column_name] == name1][column_pop].values[0]\n            pop2 = gdf[gdf[column_name] == name2][column_pop].values[0]\n            pop_products[index1, index2] = pop1 * pop2\n\n    # Calculate the transmission matrix\n    p_weight = (pop_products / (centroid_distances**2))\n    return p_weight\n</pre> def gravitational_coeff(gdf: gpd.GeoDataFrame                         , column_centroid: str = 'centroid'                         , column_pop: str = 'BEF1699'                         , column_name: str = 'ParishName'):      # Get unique parish names and create a mapping to indices     unique_names = gdf[column_name].unique()     name_to_index = {name: index for index, name in enumerate(unique_names)}      # Calculate distances between all centroids in meters     centroid_distances = np.zeros((len(unique_names), len(unique_names)))     for name1, index1 in name_to_index.items():         for name2, index2 in name_to_index.items():             if name1 != name2:                 centroid1 = gdf[gdf[column_name] == name1][column_centroid].values[0]                 centroid2 = gdf[gdf[column_name] == name2][column_centroid].values[0]                 centroid_distances[index1, index2] = centroid1.distance(centroid2)     # Set diagonal elements to infinity to avoid division by zero later     np.fill_diagonal(centroid_distances, np.inf)      # Calculate population products for all pairs of polygons     pop_products = np.zeros((len(unique_names), len(unique_names)))     for name1, index1 in name_to_index.items():         for name2, index2 in name_to_index.items():             pop1 = gdf[gdf[column_name] == name1][column_pop].values[0]             pop2 = gdf[gdf[column_name] == name2][column_pop].values[0]             pop_products[index1, index2] = pop1 * pop2      # Calculate the transmission matrix     p_weight = (pop_products / (centroid_distances**2))     return p_weight In\u00a0[\u00a0]: Copied! <pre># Define gravitational matrix without excluding repeated names\ndef gravitational_matrix(gdf: gpd.GeoDataFrame\n                        , column_centroid: str = 'centroid'\n                        , column_pop: str = 'BEF1699'\n                        , column_name: str = 'ParishName'):\n\n    # Calculate distances between all centroids in meters\n    centroid_distances = np.zeros((len(gdf), len(gdf)))\n    for index1, row1 in gdf.iterrows():\n        for index2, row2 in gdf.iterrows():\n            if index1 != index2:\n                centroid1 = row1[column_centroid]\n                centroid2 = row2[column_centroid]\n                centroid_distances[index1, index2] = centroid1.distance(centroid2)\n\n    # Calculate population products for all pairs of polygons\n    pop_products = np.zeros((len(gdf), len(gdf)))\n    for index1, row1 in gdf.iterrows():\n        for index2, row2 in gdf.iterrows():\n            pop1 = row1[column_pop]\n            pop2 = row2[column_pop]\n            pop_products[index1, index2] = pop1 * pop2\n\n    # Calculate the transmission matrix\n    p_weight = np.zeros((len(gdf), len(gdf)))\n    for i in range(len(gdf)):\n        for j in range(len(gdf)):\n            if centroid_distances[i, j] == 0:\n                p_weight[i, j] = 0\n            else:\n                p_weight[i, j] = pop_products[i, j] / (centroid_distances[i, j]**2)\n    return p_weight\n</pre> # Define gravitational matrix without excluding repeated names def gravitational_matrix(gdf: gpd.GeoDataFrame                         , column_centroid: str = 'centroid'                         , column_pop: str = 'BEF1699'                         , column_name: str = 'ParishName'):      # Calculate distances between all centroids in meters     centroid_distances = np.zeros((len(gdf), len(gdf)))     for index1, row1 in gdf.iterrows():         for index2, row2 in gdf.iterrows():             if index1 != index2:                 centroid1 = row1[column_centroid]                 centroid2 = row2[column_centroid]                 centroid_distances[index1, index2] = centroid1.distance(centroid2)      # Calculate population products for all pairs of polygons     pop_products = np.zeros((len(gdf), len(gdf)))     for index1, row1 in gdf.iterrows():         for index2, row2 in gdf.iterrows():             pop1 = row1[column_pop]             pop2 = row2[column_pop]             pop_products[index1, index2] = pop1 * pop2      # Calculate the transmission matrix     p_weight = np.zeros((len(gdf), len(gdf)))     for i in range(len(gdf)):         for j in range(len(gdf)):             if centroid_distances[i, j] == 0:                 p_weight[i, j] = 0             else:                 p_weight[i, j] = pop_products[i, j] / (centroid_distances[i, j]**2)     return p_weight In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>def get_parish_data(parish_name, parish_folder): parish_path = os.path.join(parish_folder, parish_name + '.xlsx') parish = pd.read_excel(parish_path, sheet_name='Plague')</p> <pre><code># Rename two columns\nparish = parish.rename(columns={'CumDeaths': 'VictimsNumber', 'EndDate': 'EndPlaguePeriod'})</code></pre> <pre><code># Convert 'EndPlaguePeriod' to datetime with appropriate format\nparish['NewEndDate'] = pd.to_datetime(parish['EndPlaguePeriod'], format='%b %Y')\nparish['NewEndDate'] = parish['NewEndDate'].dt.to_period('M')\nparish['first_day'] = parish['NewEndDate'].dt.to_timestamp()\nparish['last_day'] = parish['NewEndDate'].dt.to_timestamp(how='end')</code></pre> <pre><code># Add a column with the days since the first date and then cumsum\nparish['EndDaysPlague'] = parish['last_day'].dt.daysinmonth\nparish['EndDaysPlague'] = parish['EndDaysPlague'].cumsum()\nreturn parish</code></pre> <p>dict_parish ={} grouped_by_parish = example.groupby('ParishName') group_dict = {} for name, data in grouped_by_parish: group_dict[name] = data</p> <p>errors = np.zeros(n) for i in range(n): current_parish = model_input.patchNames()[i] current_df = group_dict[current_parish] len_data_parish = len(current_df) #print(current_parish, current_df, len_data_parish) # If we only have one data point, we can't calculate the error if len_data_parish &lt; 2: print(current_parish) initial_position = current_df['BeginDaysPlague'].values[0] final_position = current_df['EndDaysPlague'].values[0] deaths = current_df['VictimsNumber'].values[0] if (deaths != 0 and final_position != 0): errors[i] = ((initial_position - 1.0)2 + (final_position - deaths)2) else: errors[i] = ((initial_position - 1.0)2) print(current_parish, initial_position, final_position, deaths,errors[i]) else: print(current_parish + ' has more than one data point') point_error = 0 for j in range(len(data_by_parish.get_group(current_parish))): position = current_df['BeginDaysPlague'].values[j] monthly_deaths = current_df['VictimsNumber'].values[j] point_error = (position - monthly_deaths)2 errors[i] = errors[i] + point_error print(current_parish, errors[i])</p> <pre><code># First, we reshape the  p_coeff vector into a lower triangular matrix\np_coeff_lower = np.tril(parameters[2*n:].reshape(n, n))</code></pre> <pre><code># Then, we add the transpose to itself, subtracting the diagonal (which was added twice)\np_coeff: np.array = p_coeff_lower + p_coeff_lower.T - np.diag(np.diag(p_coeff_lower))</code></pre> <pre><code>model_info = {'model': SEIRD_model,\n              'init': {\n                  'S': model_input.S0,\n                  'E': model_input.E0,\n                  'I': model_input.I0,\n                  'R': model_input.R0,\n                  'D': model_input.D0,\n              },\n              'gdf': example,\n              # defining the initial values for the model\n              'beta': beta,\n              'p_coeff': p_coeff,\n              'mu': mu,\n              'gamma': 0.4,\n              'sigma': 0.17,\n              'bump_center': 0.0,\n              'bump_width': 0.0,\n              'bump_height': 0.0,\n              'N': model_input.patchPop(),\n              'n': model_input.n,\n              'T': model_input.maxDays()}</code></pre> <pre><code>model_sol = generate_sol(model_info)\ntotalError = 0\nn = model_info['n']</code></pre> <pre><code># Create a dictionary where the key is the parish name and the value is the dataframe\ngrouped_by_parish = gdf.groupby(column_name)\ngroup_dict = {}\nfor name, data in grouped_by_parish:\n    group_dict[name] = data</code></pre> <pre><code># Calculate the error for each patch\nerrors = np.zeros(n)</code></pre> <pre><code>for i in range(n):\n    current_parish = model_input.patchNames()[i]\n    current_df = group_dict[current_parish]\n    len_data_parish = len(current_df)\n    if len_data_parish &lt; 2:         \n        initial_position = current_df[beginTime].values[0]\n        final_position = current_df[endTime].values[0]\n        deaths = current_df[deathData].values[0]\n        if (deaths != 0 and final_position != 0):\n            try:\n                errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2 + (\n                    model_sol['D'][i][final_position] - deaths)**2)\n            except:\n                print(\n                    f\"Error at: n={n}, i={i}, final_position={final_position}, len(model_sol['D'])= {len(model_sol['D'])}, model_sol['D'][i] = {model_sol['D'][i]}, deathData[i] = {deathData[i]}\")\n        else:\n            errors[i] = ((model_sol['D'][i][initial_position] - 1.0)**2)\n    else:\n        point_error = 0\n        for j in range(len_data_parish):\n            position = current_df[endTime].values[j]\n            monthly_deaths = current_df[deathData].values[j]\n            point_error = (model_sol['D'][i][position] - monthly_deaths)**2\n            errors[i] = errors[i] + point_error</code></pre> <pre><code># Calculate the total error\ntotalError = np.sum(errors)\nreturn totalError</code></pre> In\u00a0[\u00a0]: Copied! <pre>def count_infected_parishes_by_month(df, date, n, column_name: str = 'ParishName'\n                            , start_date: str = 'BeginPlaguePeriod'\n                            , end_date: str = 'EndPlaguePeriod'):\n    # Create a copy of the dataframe\n    df_copy = df.copy()\n\n    # Convert your date columns to datetime format\n    df_copy[start_date] = pd.to_datetime(df_copy[start_date], format='%b %Y')\n    df_copy[end_date] = pd.to_datetime(df_copy[end_date], format='%b %Y', errors='coerce')\n\n    # Replace NaT with corresponding date in start_date column plus n months\n    df_copy[end_date] = df_copy[end_date].fillna(df_copy[start_date] + DateOffset(months=n))\n\n    # Convert your date to datetime format\n    date = pd.to_datetime(date, format='%b %Y')\n\n    # Add the converted date to a new column in df\n    df_copy['ConvertedDate'] = date\n\n    # Define the range of dates\n    dates = pd.date_range(start=date, end=df_copy[end_date].max(), freq='MS')\n\n    # Create a unique identifier combining Parish and date ranges\n    df_copy['UniqueID'] = df_copy[column_name].astype(str) + '_' + df_copy[start_date].astype(str) + '_' + df_copy[end_date].astype(str)\n\n    # Create a dataframe to store the results\n    results = pd.DataFrame({'date': dates\n                            , 'DaysFromInitialDate': (dates - df_copy[start_date].min()).days\n                            , 'NumberInfectedParishes': 0\n                            , 'CumInfectParishes': 0\n                            , 'EndOfMonth': (dates + MonthEnd(1))\n                            })\n\n    # Initialize an empty list to store the sets of infected parishes\n    infected_parishes = []\n\n    # Iterate over the dates\n    for date in dates:\n        # Count nodes where infection start date is before or on the given date \n        # and either there is no end date or the end date is after the given date\n        infected_nodes = df_copy[(df_copy[start_date] &lt;= date) &amp; (df_copy[end_date] &gt;= date)]\n        \n        # Store the results\n        results.loc[results['date'] == date, 'NumberInfectedParishes'] = infected_nodes['UniqueID'].nunique()  # Count only unique instances\n\n        # Add the set of infected parishes to the list\n        infected_parishes.append(set(infected_nodes[column_name]))\n\n    # Add a new column to count the days from the initial date to the end of the month\n    results['DaysToEndOfMonth'] = (results['EndOfMonth'] - df_copy[start_date].min()).dt.days\n\n    # Add a new column with the sets of infected parishes\n    results['InfectedParishes'] = infected_parishes  \n\n    # Calculate the cumulative number of infected parishes by month using the sets\n    CumInfectParishes = np.zeros(len(dates), dtype=int)\n    \n    if len(infected_parishes[0]) &gt; 0:\n        CumInfectParishes[0] = len(infected_parishes[0])\n        # Defining a variable to store the union of the infected parishes\n        union_infected_parishes = set(infected_parishes[0])  \n    else:\n        union_infected_parishes = set()\n\n    for i in range(1, len(infected_parishes)): \n        if len(infected_parishes[i]) &gt; 0: \n            new_infections = infected_parishes[i].difference(union_infected_parishes)\n            CumInfectParishes[i] = CumInfectParishes[i-1] + len(new_infections)\n            # Update the union of infected parishes\n            union_infected_parishes.update(new_infections)\n        else:\n            CumInfectParishes[i] = CumInfectParishes[i-1]\n\n    # Add a new column with the cumulative number of infected parishes\n    results['CumInfectParishes'] = CumInfectParishes          \n    return results\n</pre> def count_infected_parishes_by_month(df, date, n, column_name: str = 'ParishName'                             , start_date: str = 'BeginPlaguePeriod'                             , end_date: str = 'EndPlaguePeriod'):     # Create a copy of the dataframe     df_copy = df.copy()      # Convert your date columns to datetime format     df_copy[start_date] = pd.to_datetime(df_copy[start_date], format='%b %Y')     df_copy[end_date] = pd.to_datetime(df_copy[end_date], format='%b %Y', errors='coerce')      # Replace NaT with corresponding date in start_date column plus n months     df_copy[end_date] = df_copy[end_date].fillna(df_copy[start_date] + DateOffset(months=n))      # Convert your date to datetime format     date = pd.to_datetime(date, format='%b %Y')      # Add the converted date to a new column in df     df_copy['ConvertedDate'] = date      # Define the range of dates     dates = pd.date_range(start=date, end=df_copy[end_date].max(), freq='MS')      # Create a unique identifier combining Parish and date ranges     df_copy['UniqueID'] = df_copy[column_name].astype(str) + '_' + df_copy[start_date].astype(str) + '_' + df_copy[end_date].astype(str)      # Create a dataframe to store the results     results = pd.DataFrame({'date': dates                             , 'DaysFromInitialDate': (dates - df_copy[start_date].min()).days                             , 'NumberInfectedParishes': 0                             , 'CumInfectParishes': 0                             , 'EndOfMonth': (dates + MonthEnd(1))                             })      # Initialize an empty list to store the sets of infected parishes     infected_parishes = []      # Iterate over the dates     for date in dates:         # Count nodes where infection start date is before or on the given date          # and either there is no end date or the end date is after the given date         infected_nodes = df_copy[(df_copy[start_date] &lt;= date) &amp; (df_copy[end_date] &gt;= date)]                  # Store the results         results.loc[results['date'] == date, 'NumberInfectedParishes'] = infected_nodes['UniqueID'].nunique()  # Count only unique instances          # Add the set of infected parishes to the list         infected_parishes.append(set(infected_nodes[column_name]))      # Add a new column to count the days from the initial date to the end of the month     results['DaysToEndOfMonth'] = (results['EndOfMonth'] - df_copy[start_date].min()).dt.days      # Add a new column with the sets of infected parishes     results['InfectedParishes'] = infected_parishes        # Calculate the cumulative number of infected parishes by month using the sets     CumInfectParishes = np.zeros(len(dates), dtype=int)          if len(infected_parishes[0]) &gt; 0:         CumInfectParishes[0] = len(infected_parishes[0])         # Defining a variable to store the union of the infected parishes         union_infected_parishes = set(infected_parishes[0])       else:         union_infected_parishes = set()      for i in range(1, len(infected_parishes)):          if len(infected_parishes[i]) &gt; 0:              new_infections = infected_parishes[i].difference(union_infected_parishes)             CumInfectParishes[i] = CumInfectParishes[i-1] + len(new_infections)             # Update the union of infected parishes             union_infected_parishes.update(new_infections)         else:             CumInfectParishes[i] = CumInfectParishes[i-1]      # Add a new column with the cumulative number of infected parishes     results['CumInfectParishes'] = CumInfectParishes               return results In\u00a0[\u00a0]: Copied! <pre># Defining a function to count the number of victims per month\ndef count_victims_by_month(gdf: pd.DataFrame\n                           , column_name: str = 'ParishName'\n                           , begin_date: str = 'BeginPlaguePeriod'\n                           , victims_column: str = 'VictimsNumber'\n                           , end_date: str = 'EndPlaguePeriod'\n                           , pop_size: str = 'BEF1699'):\n    # Create a copy of the dataframe\n    gdf_copy = gdf.copy()\n\n    # Filter the dataframe to get only the infected parishes\n    gdf_copy = gdf_copy[gdf_copy[begin_date].notnull()]\n\n    # Add a new column with the converted date to iterate over\n    gdf_copy['new_format_BeginPlaguePeriod'] = pd.to_datetime(gdf_copy[begin_date], format='%b %Y', errors='coerce')\n    gdf_copy['new_format_EndPlaguePeriod'] = pd.to_datetime(gdf_copy[end_date], format='%b %Y', errors='coerce') + pd.offsets.MonthEnd(1)\n\n    # sort df by date\n    gdf_copy = add_Begin_End_days(sort_by_date(gdf_copy)\n                                         , 'new_format_BeginPlaguePeriod'\n                                         , 'new_format_EndPlaguePeriod'\n                                         )\n  \n    # Fix the type of the victims number column to integer \n    gdf_copy[victims_column] = pd.to_numeric(gdf_copy[victims_column], errors='coerce')\n    # Now replace np.nan with a default value (like 0) if you want\n    gdf_copy[victims_column].fillna(0, inplace=True)\n    # Finally, convert the column to integer\n    gdf_copy[victims_column] = gdf_copy[victims_column].astype(int)\n\n    # Get the gdf sorted by the end of the plague period\n    gdf_copy = gdf_copy.sort_values('new_format_EndPlaguePeriod')\n    \n    # Get the unique dates \n    months = gdf_copy['new_format_EndPlaguePeriod'].unique()\n    \n    # Create a dataframe to store the results\n    results = pd.DataFrame({ 'EndMonth': months\n                            , 'CumDays' : gdf_copy['EndDaysPlague'].unique()\n                            , 'NumberDeaths': 0\n                            , 'CumDeaths': 0\n                            , 'CumPop': 0\n                            , 'Parishes': \"\"\n                            })\n    # Iterate over the dates\n    total_deaths = 0\n        \n    for i, date in enumerate(months):\n        if pd.notna(date):\n            # fill the dataframe \"results\" so in the correspondant row the\n            # number of deaths is added to the column number deaths\n            numberOfDeaths = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] == date, victims_column].sum()\n            parishes = ','.join(gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] &lt;= date, column_name])\n            results.loc[results['EndMonth'] == date, 'Parishes'] = parishes\n            results.loc[results['EndMonth'] == date, 'NumberDeaths'] = numberOfDeaths\n            total_deaths += numberOfDeaths\n            results.loc[results['EndMonth'] == date, 'CumDeaths'] = total_deaths\n\n            results.loc[results['EndMonth'] == date, 'CumPop'] = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod']\n                                                                               &lt;= date, pop_size].sum()\n            \n    return results\n</pre> # Defining a function to count the number of victims per month def count_victims_by_month(gdf: pd.DataFrame                            , column_name: str = 'ParishName'                            , begin_date: str = 'BeginPlaguePeriod'                            , victims_column: str = 'VictimsNumber'                            , end_date: str = 'EndPlaguePeriod'                            , pop_size: str = 'BEF1699'):     # Create a copy of the dataframe     gdf_copy = gdf.copy()      # Filter the dataframe to get only the infected parishes     gdf_copy = gdf_copy[gdf_copy[begin_date].notnull()]      # Add a new column with the converted date to iterate over     gdf_copy['new_format_BeginPlaguePeriod'] = pd.to_datetime(gdf_copy[begin_date], format='%b %Y', errors='coerce')     gdf_copy['new_format_EndPlaguePeriod'] = pd.to_datetime(gdf_copy[end_date], format='%b %Y', errors='coerce') + pd.offsets.MonthEnd(1)      # sort df by date     gdf_copy = add_Begin_End_days(sort_by_date(gdf_copy)                                          , 'new_format_BeginPlaguePeriod'                                          , 'new_format_EndPlaguePeriod'                                          )        # Fix the type of the victims number column to integer      gdf_copy[victims_column] = pd.to_numeric(gdf_copy[victims_column], errors='coerce')     # Now replace np.nan with a default value (like 0) if you want     gdf_copy[victims_column].fillna(0, inplace=True)     # Finally, convert the column to integer     gdf_copy[victims_column] = gdf_copy[victims_column].astype(int)      # Get the gdf sorted by the end of the plague period     gdf_copy = gdf_copy.sort_values('new_format_EndPlaguePeriod')          # Get the unique dates      months = gdf_copy['new_format_EndPlaguePeriod'].unique()          # Create a dataframe to store the results     results = pd.DataFrame({ 'EndMonth': months                             , 'CumDays' : gdf_copy['EndDaysPlague'].unique()                             , 'NumberDeaths': 0                             , 'CumDeaths': 0                             , 'CumPop': 0                             , 'Parishes': \"\"                             })     # Iterate over the dates     total_deaths = 0              for i, date in enumerate(months):         if pd.notna(date):             # fill the dataframe \"results\" so in the correspondant row the             # number of deaths is added to the column number deaths             numberOfDeaths = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] == date, victims_column].sum()             parishes = ','.join(gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod'] &lt;= date, column_name])             results.loc[results['EndMonth'] == date, 'Parishes'] = parishes             results.loc[results['EndMonth'] == date, 'NumberDeaths'] = numberOfDeaths             total_deaths += numberOfDeaths             results.loc[results['EndMonth'] == date, 'CumDeaths'] = total_deaths              results.loc[results['EndMonth'] == date, 'CumPop'] = gdf_copy.loc[gdf_copy['new_format_EndPlaguePeriod']                                                                                &lt;= date, pop_size].sum()                  return results In\u00a0[\u00a0]: Copied! <pre>def connection_matrix(cluster, column_name='ParishName'):\n    # consider only the parishes with different names\n    cluster = cluster.drop_duplicates(subset=column_name)\n    matrix = np.zeros((len(cluster), len(cluster)))\n    for i in range(len(cluster)):\n        for j in range(len(cluster)):\n            if i != j:\n                if cluster.iloc[i].geometry.touches(cluster.iloc[j].geometry):\n                    matrix[i, j] = 1\n            else:\n                matrix[i, j] = 0\n\n    # Divide each row by the total number of non-zero elements in the same row\n    for i in range(len(matrix)):\n        non_zero_count = np.count_nonzero(matrix[i])\n        if non_zero_count &gt; 0:  # Avoid division by zero\n            matrix[i] /= non_zero_count\n\n    # Convert the matrix to a DataFrame and add column labels\n    df = pd.DataFrame(matrix, columns=cluster[column_name].values)\n    return df\n</pre> def connection_matrix(cluster, column_name='ParishName'):     # consider only the parishes with different names     cluster = cluster.drop_duplicates(subset=column_name)     matrix = np.zeros((len(cluster), len(cluster)))     for i in range(len(cluster)):         for j in range(len(cluster)):             if i != j:                 if cluster.iloc[i].geometry.touches(cluster.iloc[j].geometry):                     matrix[i, j] = 1             else:                 matrix[i, j] = 0      # Divide each row by the total number of non-zero elements in the same row     for i in range(len(matrix)):         non_zero_count = np.count_nonzero(matrix[i])         if non_zero_count &gt; 0:  # Avoid division by zero             matrix[i] /= non_zero_count      # Convert the matrix to a DataFrame and add column labels     df = pd.DataFrame(matrix, columns=cluster[column_name].values)     return df In\u00a0[\u00a0]: Copied! <pre># Function to calculate the error in the cumulative number of infected parishes per month between the model and the data\ndef objectiveFunction_2 (model_sol: dict\n                         , gdf: gpd.GeoDataFrame\n                         , column_name: str = 'ParishName'\n                         , n: int = 0\n                         ):\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Make a list of the days to iterate over we took initial date because we are using infected humans\n    days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values\n    # Initializing the number of infected parishes per month for the model's output\n    model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month))\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_infected_parishes_by_month))\n\n    # Initializing a matrix where the rows represents the number of parishes and the columns the number of days\n    matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n    matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n\n    for k in range(len(grouped_by_parish)):\n        for i, day in enumerate(days):\n            if day &lt; len(model_sol['D'][k]):\n                if model_sol['D'][k][day] &gt;= 1.0 :\n                    matrix_death_parishes_month[k, i] = model_sol['D'][k][day]\n          \n    for i in range(matrix_death_parishes_month.shape[0]):\n        for j in range(matrix_death_parishes_month.shape[1]):\n            if j == 0: # For the first day, there's no previous day to compare\n                matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1\n            else:\n                diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]\n                if diff &gt;= 1.0:\n                    matrix_infected_parishes_month[i,j] = 1\n                else:\n                    matrix_infected_parishes_month[i,j] = 0\n    \n    # Computing the number of infected parishes per month from the matrix\n    for j in range(matrix_infected_parishes_month.shape[1]):\n        # Sum up all the values in the column and store it \n        model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])\n        error[j] = (model_infected_parishes[j] \n                           - cum_infected_parishes_by_month['NumberInfectedParishes'][j])**2\n    \n    max_error = np.max(error)\n    # Computing the error between the model's output and the data\n    total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))\n          \n    return (total_error, np.sum(error))\n</pre> # Function to calculate the error in the cumulative number of infected parishes per month between the model and the data def objectiveFunction_2 (model_sol: dict                          , gdf: gpd.GeoDataFrame                          , column_name: str = 'ParishName'                          , n: int = 0                          ):     #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Make a list of the days to iterate over we took initial date because we are using infected humans     days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values     # Initializing the number of infected parishes per month for the model's output     model_infected_parishes = np.zeros(len(cum_infected_parishes_by_month))     # Initializing the error between the model's output and the data     error = np.zeros(len(cum_infected_parishes_by_month))      # Initializing a matrix where the rows represents the number of parishes and the columns the number of days     matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))     matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))      for k in range(len(grouped_by_parish)):         for i, day in enumerate(days):             if day &lt; len(model_sol['D'][k]):                 if model_sol['D'][k][day] &gt;= 1.0 :                     matrix_death_parishes_month[k, i] = model_sol['D'][k][day]                for i in range(matrix_death_parishes_month.shape[0]):         for j in range(matrix_death_parishes_month.shape[1]):             if j == 0: # For the first day, there's no previous day to compare                 matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1             else:                 diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]                 if diff &gt;= 1.0:                     matrix_infected_parishes_month[i,j] = 1                 else:                     matrix_infected_parishes_month[i,j] = 0          # Computing the number of infected parishes per month from the matrix     for j in range(matrix_infected_parishes_month.shape[1]):         # Sum up all the values in the column and store it          model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])         error[j] = (model_infected_parishes[j]                             - cum_infected_parishes_by_month['NumberInfectedParishes'][j])**2          max_error = np.max(error)     # Computing the error between the model's output and the data     total_error = (np.sum(error))/(max_error * len(cum_infected_parishes_by_month))                return (total_error, np.sum(error))  In\u00a0[\u00a0]: Copied! <pre># Function to calculate the square error between the cumulative number of deaths per month and the cumulative number of deaths in the data\ndef objectiveFunction_3 (model_dict, gdf: gpd.GeoDataFrame \n                         , column_name: str = 'ParishName'):\n\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    \n    # Getting the number of deaths per month from the data\n    cum_deaths_by_month = count_victims_by_month(gdf)\n\n    # Remove rows where 'EndMonth' is null\n    cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])\n\n    # Initializing the cum. number of deaths per month for the model's output\n    model_deaths_month = np.zeros(len(cum_deaths_by_month))\n    model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))\n\n    # Initializing the error between the model's output and the data\n    error = np.zeros(len(cum_deaths_by_month))\n    \n    # Computing the number of cum. deaths per month from the model's output\n    for i in range(len(cum_deaths_by_month)):\n        # Add a condition to avoid when the day is NaT\n        if pd.notnull(cum_deaths_by_month['EndMonth'][i]):\n            day = cum_deaths_by_month['CumDays'][i]\n            data = cum_deaths_by_month['CumDeaths'][i]\n            \n        for k in range(len(grouped_by_parish)):\n            model_deaths_month[i] += model_dict['D'][k][day]\n        \n        model_cum_deaths_month[i] = model_deaths_month[i]   \n\n        if i &gt; 0:\n            model_cum_deaths_month[i] += model_cum_deaths_month[i-1] \n        \n        error[i] = (model_deaths_month[i] - data)**2\n        \n    max_error = np.max(error)    \n    # Computing the error between the model's output and the data\n    total_error = (np.sum(error))/(len(error)* max_error)\n          \n    return (total_error, np.sum(error))\n</pre> # Function to calculate the square error between the cumulative number of deaths per month and the cumulative number of deaths in the data def objectiveFunction_3 (model_dict, gdf: gpd.GeoDataFrame                           , column_name: str = 'ParishName'):      #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)          # Getting the number of deaths per month from the data     cum_deaths_by_month = count_victims_by_month(gdf)      # Remove rows where 'EndMonth' is null     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])      # Initializing the cum. number of deaths per month for the model's output     model_deaths_month = np.zeros(len(cum_deaths_by_month))     model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))      # Initializing the error between the model's output and the data     error = np.zeros(len(cum_deaths_by_month))          # Computing the number of cum. deaths per month from the model's output     for i in range(len(cum_deaths_by_month)):         # Add a condition to avoid when the day is NaT         if pd.notnull(cum_deaths_by_month['EndMonth'][i]):             day = cum_deaths_by_month['CumDays'][i]             data = cum_deaths_by_month['CumDeaths'][i]                      for k in range(len(grouped_by_parish)):             model_deaths_month[i] += model_dict['D'][k][day]                  model_cum_deaths_month[i] = model_deaths_month[i]             if i &gt; 0:             model_cum_deaths_month[i] += model_cum_deaths_month[i-1]                   error[i] = (model_deaths_month[i] - data)**2              max_error = np.max(error)         # Computing the error between the model's output and the data     total_error = (np.sum(error))/(len(error)* max_error)                return (total_error, np.sum(error)) In\u00a0[\u00a0]: Copied! <pre># Function to calculate the error in the number of infected parishes per month between the model and the data\ndef plot_infected_parishes (model_solution: dict\n                            , gdf: gpd.GeoDataFrame \n                            , column_name: str = 'ParishName'\n                            , n: int = 0\n                            ):\n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n    # Defining the initial date of the dataframe to start counting the number of infected parishes per month\n    date = gdf.loc[0, 'BeginPlaguePeriod']\n    # Getting the number of infected parishes per month from the data\n    cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)\n    # Make a list of the days to iterate over we took initial date because we are using infected humans\n    days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values\n    # Initializing the number of infected parishes per month for the model's output\n    model_infected_parishes = np.zeros(len(days))\n    # Initializing a matrix where the rows represents the number of parishes and the columns the number of days\n    matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n    matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))\n\n    for k in range(len(grouped_by_parish)):\n        for i, day in enumerate(days):\n            if day &lt; len(model_solution['D'][k]):\n                if model_solution['D'][k][day] &gt;= 1.0 :\n                    matrix_death_parishes_month[k, i] = model_solution['D'][k][day]\n     \n    for i in range(matrix_death_parishes_month.shape[0]):\n        for j in range(matrix_death_parishes_month.shape[1]):\n            if j == 0: # For the first day, there's no previous day to compare\n                matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1\n            else:\n                diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]\n                if diff &gt;= 1.0:\n                    matrix_infected_parishes_month[i,j] = 1\n                else:\n                    matrix_infected_parishes_month[i,j] = 0\n    \n    # Computing the number of infected parishes per month from the matrix\n    for j in range(matrix_infected_parishes_month.shape[1]):\n        # Sum up all the values in the column and store it \n        model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])\n    \n    plt.plot(days,model_infected_parishes, color = 'blue') \n    plt.plot(cum_infected_parishes_by_month['DaysToEndOfMonth'], cum_infected_parishes_by_month['NumberInfectedParishes'],\n              label='Number of infected parishes', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Number of infected parishes')\n    plt.title('South Scania')\n    plt.show()         \n    return (model_infected_parishes, cum_infected_parishes_by_month['NumberInfectedParishes'].values)\n</pre> # Function to calculate the error in the number of infected parishes per month between the model and the data def plot_infected_parishes (model_solution: dict                             , gdf: gpd.GeoDataFrame                              , column_name: str = 'ParishName'                             , n: int = 0                             ):     #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)     # Defining the initial date of the dataframe to start counting the number of infected parishes per month     date = gdf.loc[0, 'BeginPlaguePeriod']     # Getting the number of infected parishes per month from the data     cum_infected_parishes_by_month = count_infected_parishes_by_month(gdf,date,n)     # Make a list of the days to iterate over we took initial date because we are using infected humans     days = cum_infected_parishes_by_month['DaysToEndOfMonth'].values     # Initializing the number of infected parishes per month for the model's output     model_infected_parishes = np.zeros(len(days))     # Initializing a matrix where the rows represents the number of parishes and the columns the number of days     matrix_death_parishes_month = np.zeros((len(grouped_by_parish), len(days)))     matrix_infected_parishes_month = np.zeros((len(grouped_by_parish), len(days)))      for k in range(len(grouped_by_parish)):         for i, day in enumerate(days):             if day &lt; len(model_solution['D'][k]):                 if model_solution['D'][k][day] &gt;= 1.0 :                     matrix_death_parishes_month[k, i] = model_solution['D'][k][day]           for i in range(matrix_death_parishes_month.shape[0]):         for j in range(matrix_death_parishes_month.shape[1]):             if j == 0: # For the first day, there's no previous day to compare                 matrix_infected_parishes_month[i,j]= 0 if matrix_death_parishes_month[i,j] &lt; 1.0 else 1             else:                 diff = matrix_death_parishes_month[i,j] - matrix_death_parishes_month[i,j-1]                 if diff &gt;= 1.0:                     matrix_infected_parishes_month[i,j] = 1                 else:                     matrix_infected_parishes_month[i,j] = 0          # Computing the number of infected parishes per month from the matrix     for j in range(matrix_infected_parishes_month.shape[1]):         # Sum up all the values in the column and store it          model_infected_parishes[j] = np.sum(matrix_infected_parishes_month[:,j])          plt.plot(days,model_infected_parishes, color = 'blue')      plt.plot(cum_infected_parishes_by_month['DaysToEndOfMonth'], cum_infected_parishes_by_month['NumberInfectedParishes'],               label='Number of infected parishes', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Number of infected parishes')     plt.title('South Scania')     plt.show()              return (model_infected_parishes, cum_infected_parishes_by_month['NumberInfectedParishes'].values)  <p>Function to calculate the error in the cumulative number of infected parishes per month between the model and the data</p> In\u00a0[\u00a0]: Copied! <pre>def plot_cum_deaths_model(model_solution\n                            , gdf: gpd.GeoDataFrame\n                            , column_name: str = 'ParishName'\n                            ):\n      \n    #Group the dataframe by parish name without repetitions\n    grouped_by_parish = gdf.groupby(column_name)\n\n    # Getting the number of deaths per month from the data\n    cum_deaths_by_month = count_victims_by_month(gdf)\n\n    # Remove rows where 'EndMonth' is null\n    cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])\n\n    # Now, you can directly get the 'CumDays' and 'CumDeaths' without looping and checking for nulls\n    days = cum_deaths_by_month['CumDays'].values\n    cum_deaths = cum_deaths_by_month['CumDeaths'].values\n\n    # Initializing the cum. number of deaths per month for the model's output\n    model_deaths_month = np.zeros(len(cum_deaths_by_month))\n    # model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))\n        \n    # Computing the number of cum. deaths per month from the model's output\n    for i in range(len(cum_deaths_by_month)):\n        day = cum_deaths_by_month['CumDays'][i]\n                   \n        for k in range(len(grouped_by_parish)):\n            model_deaths_month[i] += model_solution['D'][k][day]\n                \n        # model_cum_deaths_month[i] = model_deaths_month[i]   \n\n        # if i &gt; 0:\n        #     model_cum_deaths_month[i] += model_cum_deaths_month[i-1] \n       \n           \n    plt.plot(days, model_deaths_month, color='blue') \n    plt.plot(days, cum_deaths, label='Number of infected parishes', color='orange')\n    plt.xlabel('Month')\n    plt.xticks( rotation=45)\n    plt.ylabel('Cumulative Deaths')\n    plt.title('South Scania')\n    plt.show()         \n    return (\"model_cumdeaths:\",model_deaths_month\n            , \"data_cumdeaths\",cum_deaths)\n</pre> def plot_cum_deaths_model(model_solution                             , gdf: gpd.GeoDataFrame                             , column_name: str = 'ParishName'                             ):            #Group the dataframe by parish name without repetitions     grouped_by_parish = gdf.groupby(column_name)      # Getting the number of deaths per month from the data     cum_deaths_by_month = count_victims_by_month(gdf)      # Remove rows where 'EndMonth' is null     cum_deaths_by_month = cum_deaths_by_month.dropna(subset=['EndMonth'])      # Now, you can directly get the 'CumDays' and 'CumDeaths' without looping and checking for nulls     days = cum_deaths_by_month['CumDays'].values     cum_deaths = cum_deaths_by_month['CumDeaths'].values      # Initializing the cum. number of deaths per month for the model's output     model_deaths_month = np.zeros(len(cum_deaths_by_month))     # model_cum_deaths_month = np.zeros(len(cum_deaths_by_month))              # Computing the number of cum. deaths per month from the model's output     for i in range(len(cum_deaths_by_month)):         day = cum_deaths_by_month['CumDays'][i]                             for k in range(len(grouped_by_parish)):             model_deaths_month[i] += model_solution['D'][k][day]                          # model_cum_deaths_month[i] = model_deaths_month[i]             # if i &gt; 0:         #     model_cum_deaths_month[i] += model_cum_deaths_month[i-1]                          plt.plot(days, model_deaths_month, color='blue')      plt.plot(days, cum_deaths, label='Number of infected parishes', color='orange')     plt.xlabel('Month')     plt.xticks( rotation=45)     plt.ylabel('Cumulative Deaths')     plt.title('South Scania')     plt.show()              return (\"model_cumdeaths:\",model_deaths_month             , \"data_cumdeaths\",cum_deaths)"},{"location":"PlagueProject/funct_process_data/#transmission-matrix-defined-for-scenario-1","title":"Transmission matrix defined for SCENARIO 1.\u00b6","text":"<p>def trans_matrix1(gdf: gpd.GeoDataFrame, beta:float, p:float, column_name: str = 'ParishName', column_geometry: str = 'geometry'): unique_names = gdf[column_name].unique() len_unique_names = len(unique_names) trans_matrix = np.full((len_unique_names,len_unique_names),p, dtype=float) for i in range(len_unique_names): for j in range(i+1, len_unique_names): polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0] polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0] # If polygons don't touch, set value in trans_matrix to 0 if not polygon_i.touches(polygon_j): trans_matrix[i,j] = 0 trans_matrix[j,i] = 0 np.fill_diagonal(trans_matrix, beta) return trans_matrix</p>"},{"location":"PlagueProject/funct_process_data/#transmission-matrix-defined-for-scenario-2-and-scenario-3","title":"Transmission matrix defined for SCENARIO 2 and SCENARIO 3.\u00b6","text":"<p>def trans_matrix2(gdf: gpd.GeoDataFrame, beta, p:float, column_name: str = 'ParishName', column_geometry: str = 'geometry'): unique_names = gdf[column_name].unique() len_unique_names = len(unique_names) trans_matrix = np.full((len_unique_names,len_unique_names),p, dtype=float) for i in range(len_unique_names): for j in range(i, len_unique_names): if i != j: polygon_i = gdf[gdf[column_name] == unique_names[i]][column_geometry].values[0] polygon_j = gdf[gdf[column_name] == unique_names[j]][column_geometry].values[0] # If polygons don't touch, set value in trans_matrix to 0 if not polygon_i.touches(polygon_j): trans_matrix[i,j] = 0 trans_matrix[j,i] = 0 else: trans_matrix[i,j] = beta[i] return trans_matrix</p>"},{"location":"PlagueProject/funct_process_data/#p-create_symmetric_matrixnparray123456-4","title":"p = create_symmetric_matrix(np.array([1,2,3,4,5,6]), 4)\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#printp","title":"print(p)\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#def-getvalueatarrayn-i-j","title":"def getValueAt(array,n, i, j):\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#if-i-j-return-0","title":"if i == j: return 0\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#if-i-j","title":"if i &lt; j:\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#return-arrayintjj-12-i","title":"return array[int(j*(j-1)/2) + i]\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#return-getvalueatarray-n-j-i","title":"return getValueAt(array, n, j, i)\u00b6","text":""},{"location":"PlagueProject/funct_process_data/#print-the-matrix-using-getvalueat-function","title":"print the matrix using getValueAt function\u00b6","text":"<p>for i in range(4): for j in range(4): print(getValueAt([1,2,3,4,5,6], 4, i, j), end=' ') print()</p>"},{"location":"PlagueProject/funct_process_data/#create-a-dictionary","title":"Create a dictionary\u00b6","text":"<p>parish_file_dict ={ \"YSTAD\": get_parish_data('Ystad', southeast_parishes_folder) , \"SK\u00c5RBY\": get_parish_data('Skarby', southeast_parishes_folder) , \"SN\u00c5RESTAD\": get_parish_data('Snarestad', southeast_parishes_folder) , \"\u00d6VED\": get_parish_data('Oved', middle_parishes_folder) , \"S\u00d6DRA \u00c5SUM\": get_parish_data('SodraAsum', middle_parishes_folder) , \"BARSEB\u00c4CK\": get_parish_data('Barseback', southwest_parishes_folder) , \"LILLA BEDDINGE\": get_parish_data('LillaBeddinge', southwest_parishes_folder) , \"R\u00c4NG\": get_parish_data('Rang', southwest_parishes_folder) , \"SVENSTORP\": get_parish_data('Svenstorp', southwest_parishes_folder) }</p>"},{"location":"PlagueProject/funct_process_data/#calculate-the-total-error","title":"Calculate the total error\u00b6","text":"<p>totalError = np.sum(errors) print(totalError)</p>"},{"location":"PlagueProject/funct_process_data/#define-the-objective-function-to-minimize-sum-of-squared-errors-slow-version","title":"Define the objective function to minimize (sum of squared errors) Slow version\u00b6","text":"<p>def objectiveFunction(parameters, gdf: gpd.GeoDataFrame, column_name: str = 'ParishName' , beginTime: str = 'BeginDaysPlague', endTime: str = 'EndDaysPlague' , deathData: str = 'VictimsNumber' ): n = model_input.n # Reshape parameters back to their original shapes beta: np.array = parameters[:n].reshape(n,) mu:  np.array = parameters[n:2*n].reshape(n,)</p>"},{"location":"PlagueProject/middleScaniadatabase/","title":"middleScaniadatabase","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 In\u00a0[2]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n%matplotlib inline\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py %matplotlib inline <p>We have three different data sources.</p> <ol> <li>The data collected by Bodil corresponds to the plague period.</li> <li>The data provided by Lennart Palm which contains the population size for each parish in 1699 and 1718 based on a combination of tax records and estimations of population totals for Scania.</li> <li>The information from the TABVERK database includes the population size for parishes in the posterior years of the plague.</li> <li>The geographical information (polygons) for some parishes. This information doesn't correspond to the plague period.</li> </ol> <p>Our goal is to create a unique database for our project: Plague spread across Scania, Sweden, from 1710 to 1715.</p> <ol> <li>We start working with Bodil's information which we store in two databases: One database corresponds to the parishes affected by the plague, the region where parishes are located in Scania, the beginning and end of the outbreaks, and the number of victims. The second database corresponds to all the parishes in Scania during the plague period, the district, and the region they belonged to.</li> </ol> <p>The goal is to merge these two databases. First we set the working directory for private and public files.</p> In\u00a0[3]: Copied! <pre># For public files paths\ndata_folder = \"data\"\nappendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")\n\n# For private files paths\ndata_private_folder = \"data/private\"\nallParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\")\n</pre> # For public files paths data_folder = \"data\" appendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")  # For private files paths data_private_folder = \"data/private\" allParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\") <p>Reading the different data sources (.xlsx, and .csv files)</p> In\u00a0[4]: Copied! <pre># Bodil's data Appendix 6 plague parishes\nplagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")\n# All parishes in Scania during the plague period\nallParishesScania = pd.read_excel(allParishes_path)\n# All parishes from the Southeast, Middle and Southwest region of Scania with population data from Lennart Palm file\nmiddleScania = pd.read_excel(allParishes_path, sheet_name=\"middle\")\n</pre> # Bodil's data Appendix 6 plague parishes plagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\") # All parishes in Scania during the plague period allParishesScania = pd.read_excel(allParishes_path) # All parishes from the Southeast, Middle and Southwest region of Scania with population data from Lennart Palm file middleScania = pd.read_excel(allParishes_path, sheet_name=\"middle\") <p>Transforming the lowercase to uppercase and checking the type</p> In\u00a0[5]: Copied! <pre>allParishesScania = allParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nplagueParishesScania = plagueParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nmiddleScania = middleScania.apply(\n    lambda x: x.astype(str).str.upper())\n</pre> allParishesScania = allParishesScania.apply(     lambda x: x.astype(str).str.upper()) plagueParishesScania = plagueParishesScania.apply(     lambda x: x.astype(str).str.upper()) middleScania = middleScania.apply(     lambda x: x.astype(str).str.upper()) <p>Merging the two datasets (allParishesScania and plagueParishesScania)</p> In\u00a0[6]: Copied! <pre>parishesScania = pd.merge(\n    allParishesScania, plagueParishesScania, how='left', on=['ParishName', 'Region'])\n</pre> parishesScania = pd.merge(     allParishesScania, plagueParishesScania, how='left', on=['ParishName', 'Region']) <p>Checking that the new data frame keep all the outbreaks for parish</p> <p>Extracting the parishes' names from the data frame</p> In\u00a0[7]: Copied! <pre>parishesScania_names = get_Names(\n    parishesScania, 'ParishName').unique().tolist()\nlen(parishesScania_names)\n</pre> parishesScania_names = get_Names(     parishesScania, 'ParishName').unique().tolist() len(parishesScania_names)  Out[7]: <pre>396</pre> <p>The length of 'parishesScania_names' is less than the number of rows in the data frame 'allparishesScania'. This means, there is a repeated name: 'L\u00d6DDEK\u00d6PINGE'. We have to check the information for this parish:</p> In\u00a0[8]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[8]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 86 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? 160 SOUTHWEST TORNA L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? <p>Only the parish L\u00d6DDEK\u00d6PINGE at HARJAGER was affected by the plague according to the file 'Bilaga 6 d - sydva\u0308st.doc' provided by Bodil. So we need to fix the information in the other row (160).</p> In\u00a0[9]: Copied! <pre>parishesScania.at[160, 'BeginPlaguePeriod'] = np.NaN\nparishesScania.at[160, 'EndPlaguePeriod'] = np.NaN\nparishesScania.at[160, 'VictimsNumber'] = np.NaN\n</pre> parishesScania.at[160, 'BeginPlaguePeriod'] = np.NaN parishesScania.at[160, 'EndPlaguePeriod'] = np.NaN parishesScania.at[160, 'VictimsNumber'] = np.NaN <p>Checking the data:</p> In\u00a0[10]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[10]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 86 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? 160 SOUTHWEST TORNA L\u00d6DDEK\u00d6PINGE NaN NaN NaN <p>Filtering the data frame by region and then get the names of the parishes:</p> In\u00a0[11]: Copied! <pre>middleParishes = parishesByregion(parishesScania, 'MIDDLE')\nmiddleParishes_names = get_Names(middleParishes, 'ParishName')\n</pre> middleParishes = parishesByregion(parishesScania, 'MIDDLE') middleParishes_names = get_Names(middleParishes, 'ParishName') <ol> <li>Here, we used the population size estimations provided by Lennart Palm. Merging the two datasets (southeastScania and southeastParishes)</li> </ol> In\u00a0[12]: Copied! <pre>middleParishesPop = pd.merge(\n    middleScania, middleParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)'])\n</pre> middleParishesPop = pd.merge(     middleScania, middleParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)']) <ol> <li>The geographical information for Scania is already projected on the plane, i.e. the measures are in meters not in longitude and latitude. To process the shape file, we proceed as with the census file. First, we set the directory and chose the columns to work with.</li> </ol> In\u00a0[16]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\")\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\nparishScaniaMap = parishScaniaMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\") SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] parishScaniaMap = parishScaniaMap[selected_columns] <p>Now, we remove white spaces and patterns. Then, we filter the shape file considering the column \"GET_END_YE\" of the polygon.</p> In\u00a0[17]: Copied! <pre>parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'\n                                                                        ])\nparishScaniaMap = process_dataframe(parishScaniaMap, 'G_NAME', 'GET_END_YE')\nlen(parishScaniaMap)\n</pre> parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'                                                                         ]) parishScaniaMap = process_dataframe(parishScaniaMap, 'G_NAME', 'GET_END_YE') len(parishScaniaMap) Out[17]: <pre>412</pre> <p>Plotting the map only with the polygons obtained after clean the data</p> In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nparishScaniaMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) parishScaniaMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() <p>Working only with Middle Scania: middleParishesPop and parishScaniaMap</p> In\u00a0[18]: Copied! <pre>df1 = middleParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=90,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = middleParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=90,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[27]: Copied! <pre>middleParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n# Get the index for H\u00f6\u00f6r, \u00d6stra \u00c4spinge, Vomb, and \u00d6ved for fixing the geographical data\n# middleParishMap.loc[middleParishMap['ParishName'] == 'H\u00d6\u00d6R']\n# middleParishMap.loc[middleParishMap['ParishName'] == '\u00d6STRA \u00c4SPINGE']\n# middleParishMap.loc[middleParishMap['ParishName'] == 'VOMB']\n# middleParishMap.loc[middleParishMap['ParishName'] == '\u00d6VED']\n</pre> middleParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] # Get the index for H\u00f6\u00f6r, \u00d6stra \u00c4spinge, Vomb, and \u00d6ved for fixing the geographical data # middleParishMap.loc[middleParishMap['ParishName'] == 'H\u00d6\u00d6R'] # middleParishMap.loc[middleParishMap['ParishName'] == '\u00d6STRA \u00c4SPINGE'] # middleParishMap.loc[middleParishMap['ParishName'] == 'VOMB'] # middleParishMap.loc[middleParishMap['ParishName'] == '\u00d6VED'] <p>We need to modify manually the geographical information assigned to H\u00f6\u00f6r, \u00d6stra \u00c4spinge, Vomb, and \u00d6ved.</p> In\u00a0[22]: Copied! <pre># Get the geometry from the map\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == 'H\u00d6\u00d6RS', 'geometry'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == '\u00c4SPINGE', 'geometry'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == 'VOMBS', 'geometry'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == '\u00d6VEDS', 'geometry'].values[0]\nmiddleParishMap.at[11, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00d6\u00d6RS', 'G_NAME'].values[0]\nmiddleParishMap.at[11, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                           == 'H\u00d6\u00d6RS', 'geometry'].values[0]\nmiddleParishMap.at[21, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == '\u00c4SPINGE', 'G_NAME'].values[0]\nmiddleParishMap.at[21, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                           == '\u00c4SPINGE', 'geometry'].values[0]\nmiddleParishMap.at[37, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                          == 'VOMBS', 'G_NAME'].values[0]\nmiddleParishMap.at[37, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                            == 'VOMBS', 'geometry'].values[0]\nmiddleParishMap.at[41, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                          == '\u00d6VEDS', 'G_NAME'].values[0]\nmiddleParishMap.at[41, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                            == '\u00d6VEDS', 'geometry'].values[0]\n</pre> # Get the geometry from the map parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == 'H\u00d6\u00d6RS', 'geometry'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == '\u00c4SPINGE', 'geometry'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == 'VOMBS', 'geometry'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == '\u00d6VEDS', 'geometry'].values[0] middleParishMap.at[11, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00d6\u00d6RS', 'G_NAME'].values[0] middleParishMap.at[11, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                            == 'H\u00d6\u00d6RS', 'geometry'].values[0] middleParishMap.at[21, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == '\u00c4SPINGE', 'G_NAME'].values[0] middleParishMap.at[21, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                            == '\u00c4SPINGE', 'geometry'].values[0] middleParishMap.at[37, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                           == 'VOMBS', 'G_NAME'].values[0] middleParishMap.at[37, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                             == 'VOMBS', 'geometry'].values[0] middleParishMap.at[41, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                           == '\u00d6VEDS', 'G_NAME'].values[0] middleParishMap.at[41, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                             == '\u00d6VEDS', 'geometry'].values[0]  In\u00a0[26]: Copied! <pre>middleScaniaMap = gpd.GeoDataFrame(middleParishMap, geometry='geometry')\n</pre> middleScaniaMap = gpd.GeoDataFrame(middleParishMap, geometry='geometry') In\u00a0[25]: Copied! <pre>middleScaniaMap.to_csv('middleScania.csv', index=False)\n</pre> middleScaniaMap.to_csv('middleScania.csv', index=False) In\u00a0[23]: Copied! <pre>middleScaniaMap = get_area(middleScaniaMap)\nmiddleScaniaMap = get_centroid(middleScaniaMap)\nfrom shapely.geometry import Point, mapping\nmiddleScaniaMap['centroid'] = middleScaniaMap['centroid'].apply(mapping)\n</pre> middleScaniaMap = get_area(middleScaniaMap) middleScaniaMap = get_centroid(middleScaniaMap) from shapely.geometry import Point, mapping middleScaniaMap['centroid'] = middleScaniaMap['centroid'].apply(mapping) <p>Plotting the parishes from Middle Scania</p> <p>Before to plot the map of parishes, for a given Geodataframe, we assign 'red' to the parishes affected by the plague and blue for the others. This information is added as a column with heading 'color'.</p> In\u00a0[24]: Copied! <pre>colorByColumn(middleScaniaMap, 'EndPlaguePeriod')\nmiddleMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nmiddleScaniaMap.explore(\n    m = middleMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"Middle Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    middleMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(middleMap)  # use folium to add layer control\n\nmiddleMap  # show map\n</pre> colorByColumn(middleScaniaMap, 'EndPlaguePeriod') middleMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  middleScaniaMap.explore(     m = middleMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"Middle Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     middleMap )  # use folium to add alternative tiles folium.LayerControl().add_to(middleMap)  # use folium to add layer control  middleMap  # show map Out[24]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[25]: Copied! <pre>type(middleParishMap)\n</pre> type(middleParishMap) Out[25]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[28]: Copied! <pre># Assuming you have a GeoDataFrame named 'gdf'\ndef calculate_quotient(gdf, col1, col2):\n    gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')\n    gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')\n    \n    # Calculate the death rate per 1000 inhabitants\n    gdf['quotient'] = (gdf[col1] / gdf[col2])*1000\n    pass\n\ncalculate_quotient(middleScaniaMap, 'VictimsNumber', 'BEF1699')\n</pre> # Assuming you have a GeoDataFrame named 'gdf' def calculate_quotient(gdf, col1, col2):     gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')     gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')          # Calculate the death rate per 1000 inhabitants     gdf['quotient'] = (gdf[col1] / gdf[col2])*1000     pass  calculate_quotient(middleScaniaMap, 'VictimsNumber', 'BEF1699') In\u00a0[29]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel1')\nmiddleScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel1') middleScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show()"},{"location":"PlagueProject/northeastScaniadatabase/","title":"northeastScaniadatabase","text":"In\u00a0[89]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[90]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n%matplotlib inline\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py %matplotlib inline <p>We have three different data sources.</p> <ol> <li>The data collected by Bodil corresponds to the plague period.</li> <li>The data provided by Lennart Palm which contains the population size for each parish in 1699 and 1718 based on a combination of tax records and estimations of population totals for Scania.</li> <li>The geographical information (polygons) for some parishes. This information doesn't correspond to the plague period.</li> </ol> <p>Our goal is to create a unique database for our project: Plague spread across Scania, Sweden, from 1710 to 1715.</p> <ol> <li>We start working with Bodil's information which we store in two databases: One database corresponds to the parishes affected by the plague, the region where parishes are located in Scania, the beginning and end of the outbreaks, and the number of victims. The second database corresponds to all the parishes in Scania during the plague period, the district, and the region they belonged to.</li> </ol> <p>The goal is to merge these two databases. First we set the working directory for private and public files.</p> In\u00a0[91]: Copied! <pre># For public files paths\ndata_folder = \"data\"\nappendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")\n\n# For private files paths\ndata_private_folder = \"data/private\"\nallParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\")\n</pre> # For public files paths data_folder = \"data\" appendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")  # For private files paths data_private_folder = \"data/private\" allParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\") <p>Reading the different data sources (.xlsx, and .csv files)</p> In\u00a0[92]: Copied! <pre># Bodil's data Appendix 6 plague parishes\nplagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")\n\n# All parishes from the Southwest Scania with population data from Lennart Palm file\nnortheastScania = pd.read_excel(allParishes_path, sheet_name=\"northeast\")\n</pre> # Bodil's data Appendix 6 plague parishes plagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")  # All parishes from the Southwest Scania with population data from Lennart Palm file northeastScania = pd.read_excel(allParishes_path, sheet_name=\"northeast\") <p>Transforming the lowercase to uppercase and checking the type</p> In\u00a0[93]: Copied! <pre>plagueParishesScania = plagueParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nnortheastScania = northeastScania.apply(\n    lambda x: x.astype(str).str.upper())\n</pre> plagueParishesScania = plagueParishesScania.apply(     lambda x: x.astype(str).str.upper()) northeastScania = northeastScania.apply(     lambda x: x.astype(str).str.upper()) <p>Merging the two datasets (northeastScania and plagueParishesScania)</p> In\u00a0[94]: Copied! <pre>northeastParishesPop = pd.merge(\n    northeastScania, plagueParishesScania, how='left', on=['ParishName', 'Region'])\n</pre> northeastParishesPop = pd.merge(     northeastScania, plagueParishesScania, how='left', on=['ParishName', 'Region']) In\u00a0[95]: Copied! <pre>northeastParishesPop.loc[0:20]\n</pre> northeastParishesPop.loc[0:20] Out[95]: Region District(H\u00e4rad) ParishName BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber 0 NORTHEAST G\u00c4RDS DEGEBERGA 463 523 493.0 2 5 NaN NaN NaN 1 NORTHEAST G\u00c4RDS DJURR\u00d6D 327 369 348.0 2 5 NaN NaN NaN 2 NORTHEAST G\u00c4RDS EVER\u00d6D 362 409 385.5 2 5 NaN NaN NaN 3 NORTHEAST G\u00c4RDS HUAR\u00d6D 342 386 364.0 2 5 NaN NaN NaN 4 NORTHEAST G\u00c4RDS H\u00d6RR\u00d6D 258 291 274.5 3 5 NaN NaN NaN 5 NORTHEAST G\u00c4RDS K\u00d6PINGE 644 727 685.5 3 5 NaN NaN NaN 6 NORTHEAST G\u00c4RDS LINDER\u00d6D 318 359 338.5 3 5 NaN NaN NaN 7 NORTHEAST G\u00c4RDS LYNGSJ\u00d6 224 253 238.5 3 5 NaN NaN NaN 8 NORTHEAST G\u00c4RDS MAGLEHEM 477 538 507.5 3 5 NaN NaN NaN 9 NORTHEAST G\u00c4RDS NORRA \u00c5SUM 522 589 555.5 3 4 OCT 1711 FEB 1712 ? 10 NORTHEAST G\u00c4RDS SKEPPARSL\u00d6V 409 462 435.5 3 4 OCT 1711 UNDEFINED 5 11 NORTHEAST G\u00c4RDS TR\u00c4NE 474 535 504.5 2 4 OCT 1711 MAR 1712 40 12 NORTHEAST G\u00c4RDS VITTSK\u00d6VLE 639 721 680.0 2 5 NaN NaN NaN 13 NORTHEAST G\u00c4RDS V\u00c4 556 628 592.0 1 5 NOV 1711 JAN 1712 55 14 NORTHEAST G\u00c4RDS V\u00c4STRA VRAM 673 759 716.0 2 4 DEC 1710 JAN 1711 18 15 NORTHEAST G\u00c4RDS \u00c4SPHULT 418 472 445.0 3 5 NaN NaN NaN 16 NORTHEAST G\u00c4RDS \u00d6STRA S\u00d6NNARSL\u00d6V 582 656 619.0 2 5 NaN NaN NaN 17 NORTHEAST G\u00c4RDS \u00d6STRA VRAM 128 144 136.0 2 5 NaN NaN NaN 18 NORTHEAST G\u00d6INGE V\u00c4STRA BR\u00d6NNESTAD 550 620 585.0 2 5 NaN NaN NaN 19 NORTHEAST G\u00d6INGE V\u00c4STRA FARSTORP 436 492 464.0 2 5 NaN NaN NaN 20 NORTHEAST G\u00d6INGE V\u00c4STRA FINJA 403 455 429.0 2 5 NaN NaN NaN <p>Extracting the parishes' names from the data frame</p> In\u00a0[96]: Copied! <pre>parishesNortheastScania_names = get_Names(\n    northeastParishesPop, 'ParishName').unique().tolist()\nlen(parishesNortheastScania_names)\n</pre> parishesNortheastScania_names = get_Names(     northeastParishesPop, 'ParishName').unique().tolist() len(parishesNortheastScania_names)  Out[96]: <pre>71</pre> <ol> <li>The geographical information for Scania is already projected on the plane, i.e. the measures are in meters not in longitude and latitude. To process the shape file, we set the directory and chose the columns to work with.</li> </ol> In\u00a0[97]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\")\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\nselected_columns = ['G_NAME', 'GET_START_', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\nparishScaniaMap = parishScaniaMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\") SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) selected_columns = ['G_NAME', 'GET_START_', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] parishScaniaMap = parishScaniaMap[selected_columns] <p>Now, we remove white spaces and patterns. Then, the shape file is filtered considering the column \"GET_END_YE\" of the polygon.</p> In\u00a0[98]: Copied! <pre>parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'\n                                                                    ])\n</pre> parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'                                                                     ]) <p>Working only with Southwest Scania: southwestParishesPop and parishScaniaMap</p> In\u00a0[99]: Copied! <pre>df1 = northeastParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=90,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = northeastParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=90,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[100]: Copied! <pre>northeastParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699'\n    , 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod'\n    , 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n</pre> northeastParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699'     , 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod'     , 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] In\u00a0[102]: Copied! <pre>northeastParishMap.loc[0:20]\n</pre> northeastParishMap.loc[0:20] Out[102]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id 0 NORTHEAST G\u00c4RDS DEGEBERGA DEGEBERGA 463 523 493.0 2 5 NaN NaN NaN POLYGON ((4242432.518 3216403.674, 4242587.452... 1 NORTHEAST G\u00c4RDS DJURR\u00d6D DJURR\u00d6DS 327 369 348.0 2 5 NaN NaN NaN POLYGON ((4235034.141 3239316.925, 4235277.475... 2 NORTHEAST G\u00c4RDS EVER\u00d6D EVER\u00d6DS 362 409 385.5 2 5 NaN NaN NaN POLYGON ((4244852.781 3223435.169, 4244763.383... 3 NORTHEAST G\u00c4RDS HUAR\u00d6D HUAR\u00d6DS 342 386 364.0 2 5 NaN NaN NaN POLYGON ((4242432.518 3216403.674, 4242367.213... 4 NORTHEAST G\u00c4RDS H\u00d6RR\u00d6D NaN 258 291 274.5 3 5 NaN NaN NaN None 5 NORTHEAST G\u00c4RDS K\u00d6PINGE K\u00d6PINGE 644 727 685.5 3 5 NaN NaN NaN POLYGON ((4255464.917 3230813.998, 4255408.599... 6 NORTHEAST G\u00c4RDS LINDER\u00d6D LINDER\u00d6DS 318 359 338.5 3 5 NaN NaN NaN POLYGON ((4227839.808 3230701.940, 4228280.140... 7 NORTHEAST G\u00c4RDS LYNGSJ\u00d6 LYNGSJ\u00d6 224 253 238.5 3 5 NaN NaN NaN POLYGON ((4245353.997 3229661.393, 4245321.119... 8 NORTHEAST G\u00c4RDS MAGLEHEM MAGLEHEMS 477 538 507.5 3 5 NaN NaN NaN POLYGON ((4251987.638 3216821.325, 4252685.190... 9 NORTHEAST G\u00c4RDS NORRA \u00c5SUM NORRA \u00c5SUMS 522 589 555.5 3 4 OCT 1711 FEB 1712 ? POLYGON ((4249757.034 3241445.338, 4249865.346... 10 NORTHEAST G\u00c4RDS SKEPPARSL\u00d6V SKEPPARSL\u00d6VS 409 462 435.5 3 4 OCT 1711 UNDEFINED 5 POLYGON ((4241776.825 3239062.212, 4241471.416... 11 NORTHEAST G\u00c4RDS TR\u00c4NE TR\u00c4NE 474 535 504.5 2 4 OCT 1711 MAR 1712 40 POLYGON ((4241776.825 3239062.212, 4241952.361... 12 NORTHEAST G\u00c4RDS VITTSK\u00d6VLE VITTSK\u00d6VLE 639 721 680.0 2 5 NaN NaN NaN POLYGON ((4251987.638 3216821.325, 4252069.239... 13 NORTHEAST G\u00c4RDS V\u00c4 V\u00c4 556 628 592.0 1 5 NOV 1711 JAN 1712 55 POLYGON ((4246916.116 3231887.997, 4247067.301... 14 NORTHEAST G\u00c4RDS V\u00c4STRA VRAM V\u00c4STRA VRAMS 673 759 716.0 2 4 DEC 1710 JAN 1711 18 POLYGON ((4237203.042 3227245.029, 4236492.276... 15 NORTHEAST G\u00c4RDS \u00c4SPHULT \u00c4SPHULTS 418 472 445.0 3 5 NaN NaN NaN POLYGON ((4227839.808 3230701.940, 4228155.888... 16 NORTHEAST G\u00c4RDS \u00d6STRA S\u00d6NNARSL\u00d6V \u00d6STRA S\u00d6NNARSL\u00d6VS 582 656 619.0 2 5 NaN NaN NaN POLYGON ((4244852.781 3223435.169, 4245130.252... 17 NORTHEAST G\u00c4RDS \u00d6STRA VRAM \u00d6STRA VRAMS 128 144 136.0 2 5 NaN NaN NaN POLYGON ((4245961.839 3231295.748, 4246056.931... 18 NORTHEAST G\u00d6INGE V\u00c4STRA BR\u00d6NNESTAD BR\u00d6NNESTADS 550 620 585.0 2 5 NaN NaN NaN POLYGON ((4225492.274 3250504.084, 4225598.126... 19 NORTHEAST G\u00d6INGE V\u00c4STRA FARSTORP FARSTORPS 436 492 464.0 2 5 NaN NaN NaN POLYGON ((4233766.998 3266498.553, 4233697.441... 20 NORTHEAST G\u00d6INGE V\u00c4STRA FINJA FINJA 403 455 429.0 2 5 NaN NaN NaN POLYGON ((4221889.882 3259320.758, 4222291.452... <p>After checking the previous data, we found some issues when we assign the polygons to H\u00d6RROD (4), FARL\u00d6V(42), GRYT (44), and N\u00c4SUM (64  and 65). We will modify the geographical data for these parishes using the indexes and the shape file.</p> In\u00a0[76]: Copied! <pre># Check the index of the parish that is not matched\nnortheastParishMap.loc[northeastParishMap['ParishName'] == 'H\u00d6RR\u00d6D']\nnortheastParishMap.loc[northeastParishMap['ParishName'] == 'F\u00c4RL\u00d6V']\nnortheastParishMap.loc[northeastParishMap['ParishName'] == 'GRYT']\nnortheastParishMap.loc[northeastParishMap['ParishName'] == 'N\u00c4SUM']\n\n# Check the correct geographical information in the shapefile\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'H\u00d6RR\u00d6DS']\nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'F\u00c4RL\u00d6VS')\n                    &amp; (parishScaniaMap['GET_START_'] == 1707)]\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'GRYTS']\nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'N\u00c4SUMS')]\n\n# Correct the classification for the source of the plague data\nnortheastParishMap.loc[northeastParishMap['ParishName'] == '\u00d6STERSL\u00d6V']\nnortheastParishMap.at[72, 'ChurchBook'] = 2\nnortheastParishMap.at[73, 'OtherSources'] = 5\n\nnortheastParishMap.loc[northeastParishMap['ParishName'] == 'SVAL\u00d6V']\n</pre> # Check the index of the parish that is not matched northeastParishMap.loc[northeastParishMap['ParishName'] == 'H\u00d6RR\u00d6D'] northeastParishMap.loc[northeastParishMap['ParishName'] == 'F\u00c4RL\u00d6V'] northeastParishMap.loc[northeastParishMap['ParishName'] == 'GRYT'] northeastParishMap.loc[northeastParishMap['ParishName'] == 'N\u00c4SUM']  # Check the correct geographical information in the shapefile parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'H\u00d6RR\u00d6DS'] parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'F\u00c4RL\u00d6VS')                     &amp; (parishScaniaMap['GET_START_'] == 1707)] parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'GRYTS'] parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'N\u00c4SUMS')]  # Correct the classification for the source of the plague data northeastParishMap.loc[northeastParishMap['ParishName'] == '\u00d6STERSL\u00d6V'] northeastParishMap.at[72, 'ChurchBook'] = 2 northeastParishMap.at[73, 'OtherSources'] = 5  northeastParishMap.loc[northeastParishMap['ParishName'] == 'SVAL\u00d6V'] Out[76]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id <p>Modifying the geographical information</p> In\u00a0[55]: Copied! <pre># H\u00d6RR\u00d6DS\nnortheastParishMap.at[4, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00d6RR\u00d6DS', 'G_NAME'].values[0]\nnortheastParishMap.at[4, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00d6RR\u00d6DS', 'geometry'].values[0]\n# F\u00c4RL\u00d6VS\nnortheastParishMap.at[42, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'F\u00c4RL\u00d6VS')\n                     &amp; (parishScaniaMap['GET_START_'] == 1707), 'G_NAME'].values[0]\nnortheastParishMap.at[42, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'F\u00c4RL\u00d6VS')\n                     &amp; (parishScaniaMap['GET_START_'] == 1707), 'geometry'].values[0]\n# GRYTS\nnortheastParishMap.at[44, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'GRYTS', 'G_NAME'].values[0]\nnortheastParishMap.at[44, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'GRYTS', 'geometry'].values[0]\n# N\u00c4SUMS\nnortheastParishMap.at[64, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'N\u00c4SUMS', 'G_NAME'].values[0]\nnortheastParishMap.at[64, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                            == 'N\u00c4SUMS', 'geometry'].values[0]\nnortheastParishMap.at[65, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'N\u00c4SUMS', 'G_NAME'].values[0]\nnortheastParishMap.at[65, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                            == 'N\u00c4SUMS', 'geometry'].values[0]\n</pre> # H\u00d6RR\u00d6DS northeastParishMap.at[4, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00d6RR\u00d6DS', 'G_NAME'].values[0] northeastParishMap.at[4, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00d6RR\u00d6DS', 'geometry'].values[0] # F\u00c4RL\u00d6VS northeastParishMap.at[42, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'F\u00c4RL\u00d6VS')                      &amp; (parishScaniaMap['GET_START_'] == 1707), 'G_NAME'].values[0] northeastParishMap.at[42, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'F\u00c4RL\u00d6VS')                      &amp; (parishScaniaMap['GET_START_'] == 1707), 'geometry'].values[0] # GRYTS northeastParishMap.at[44, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'GRYTS', 'G_NAME'].values[0] northeastParishMap.at[44, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'GRYTS', 'geometry'].values[0] # N\u00c4SUMS northeastParishMap.at[64, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'N\u00c4SUMS', 'G_NAME'].values[0] northeastParishMap.at[64, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                             == 'N\u00c4SUMS', 'geometry'].values[0] northeastParishMap.at[65, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'N\u00c4SUMS', 'G_NAME'].values[0] northeastParishMap.at[65, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                             == 'N\u00c4SUMS', 'geometry'].values[0] In\u00a0[56]: Copied! <pre>northeastScaniaMap = gpd.GeoDataFrame(northeastParishMap, geometry='geometry')\n</pre> northeastScaniaMap = gpd.GeoDataFrame(northeastParishMap, geometry='geometry') In\u00a0[57]: Copied! <pre>#northeastScaniaMap.to_csv('northeastScania.csv', index=False)\n</pre> #northeastScaniaMap.to_csv('northeastScania.csv', index=False) In\u00a0[58]: Copied! <pre>northeastScaniaMap = get_area(northeastScaniaMap)\nnortheastScaniaMap = get_centroid(northeastScaniaMap)\nfrom shapely.geometry import Point, mapping\nnortheastScaniaMap['centroid'] = northeastScaniaMap['centroid'].apply(mapping)\n</pre> northeastScaniaMap = get_area(northeastScaniaMap) northeastScaniaMap = get_centroid(northeastScaniaMap) from shapely.geometry import Point, mapping northeastScaniaMap['centroid'] = northeastScaniaMap['centroid'].apply(mapping) <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652, in Index.get_loc(self, key)\n   3651 try:\n-&gt; 3652     return self._engine.get_loc(casted_key)\n   3653 except KeyError as err:\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:147, in pandas._libs.index.IndexEngine.get_loc()\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:176, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:2606, in pandas._libs.hashtable.Int64HashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:2630, in pandas._libs.hashtable.Int64HashTable.get_item()\n\nKeyError: 74\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/northeastScaniadatabase.ipynb Cell 29 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/northeastScaniadatabase.ipynb#X40sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; northeastScaniaMap = get_area(northeastScaniaMap)\n      &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/northeastScaniadatabase.ipynb#X40sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; northeastScaniaMap = get_centroid(northeastScaniaMap)\n      &lt;a href='vscode-notebook-cell:/Users/dianapli/Desktop/PythonMathematicalModeling/docs/PlagueProject/northeastScaniadatabase.ipynb#X40sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt; from shapely.geometry import Point, mapping\n\nFile ~/Desktop/PythonMathematicalModeling/docs/PlagueProject/funct_process_data.py:121, in get_area(gpd, heading)\n    119 def get_area(gpd: gpd.GeoDataFrame, heading: str = 'geometry'):\n    120     for i in range(len(gpd)):\n--&gt; 121         gpd['area_m2'] = shape(gpd.loc[i][heading]).area\n    122         gpd['area_km2'] = gpd['area_m2']/1000000\n    123     return gpd\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key)\n   1100 axis = self.axis or 0\n   1102 maybe_callable = com.apply_if_callable(key, self.obj)\n-&gt; 1103 return self._getitem_axis(maybe_callable, axis=axis)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/indexing.py:1343, in _LocIndexer._getitem_axis(self, key, axis)\n   1341 # fall thru to straight lookup\n   1342 self._validate_key(key, axis)\n-&gt; 1343 return self._get_label(key, axis=axis)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/indexing.py:1293, in _LocIndexer._get_label(self, label, axis)\n   1291 def _get_label(self, label, axis: AxisInt):\n   1292     # GH#5567 this will fail if the label is not present in the axis.\n-&gt; 1293     return self.obj.xs(label, axis=axis)\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:4095, in NDFrame.xs(self, key, axis, level, drop_level)\n   4093             new_index = index[loc]\n   4094 else:\n-&gt; 4095     loc = index.get_loc(key)\n   4097     if isinstance(loc, np.ndarray):\n   4098         if loc.dtype == np.bool_:\n\nFile /opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654, in Index.get_loc(self, key)\n   3652     return self._engine.get_loc(casted_key)\n   3653 except KeyError as err:\n-&gt; 3654     raise KeyError(key) from err\n   3655 except TypeError:\n   3656     # If we have a listlike key, _check_indexing_error will raise\n   3657     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3658     #  the TypeError.\n   3659     self._check_indexing_error(key)\n\nKeyError: 74</pre> <p>Plotting the southwest parishes</p> <p>Before to plot the map of parishes, for a given Geodataframe, we assign 'red' to the parishes affected by the plague and blue for the others. This information is added as a column with heading 'color'.</p> In\u00a0[\u00a0]: Copied! <pre>colorByColumn(northeastScaniaMap, 'EndPlaguePeriod')\nnortheastMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nnortheastScaniaMap.explore(\n    m = northeastMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"Northeast Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    northeastMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(northeastMap)  # use folium to add layer control\n\nnortheastMap  # show map\n</pre> colorByColumn(northeastScaniaMap, 'EndPlaguePeriod') northeastMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  northeastScaniaMap.explore(     m = northeastMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"Northeast Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     northeastMap )  # use folium to add alternative tiles folium.LayerControl().add_to(northeastMap)  # use folium to add layer control  northeastMap  # show map Out[\u00a0]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[41]: Copied! <pre>northeastScaniaMap.loc[northeastScaniaMap['ParishName'] == '\u00d6STERSL\u00d6V']\n</pre> northeastScaniaMap.loc[northeastScaniaMap['ParishName'] == '\u00d6STERSL\u00d6V'] Out[41]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry area_m2 area_km2 centroid color id 75 NORTHEAST VILLANDS \u00d6STERSL\u00d6V \u00d6STERSL\u00d6VS 655 739 697.0 1 4 DEC 1710 MAR 1711 ? POLYGON ((4255742.650 3255353.087, 4255922.824... 4.555912e+07 45.559121 {'type': 'Point', 'coordinates': (4257421.7993... red 76 NORTHEAST VILLANDS \u00d6STERSL\u00d6V \u00d6STERSL\u00d6VS 655 739 697.0 1 4 JUN 1711 SEP 1711 167 POLYGON ((4255742.650 3255353.087, 4255922.824... 4.555912e+07 45.559121 {'type': 'Point', 'coordinates': (4257421.7993... red In\u00a0[192]: Copied! <pre># Assuming you have a GeoDataFrame named 'gdf'\ndef calculate_quotient(gdf, col1, col2):\n    gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')\n    gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')\n    \n    # Calculate the death rate per 1000 inhabitants\n    gdf['quotient'] = (gdf[col1] / gdf[col2])*1000\n    pass\n\ncalculate_quotient(northeastScaniaMap, 'VictimsNumber', 'BEF1699')\n</pre> # Assuming you have a GeoDataFrame named 'gdf' def calculate_quotient(gdf, col1, col2):     gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')     gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')          # Calculate the death rate per 1000 inhabitants     gdf['quotient'] = (gdf[col1] / gdf[col2])*1000     pass  calculate_quotient(northeastScaniaMap, 'VictimsNumber', 'BEF1699') In\u00a0[193]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel2')\nnortheastScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel2') northeastScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show()"},{"location":"PlagueProject/northwestScaniadatabase/","title":"northwestScaniadatabase","text":"In\u00a0[27]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[28]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n%matplotlib inline\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py %matplotlib inline <p>We have three different data sources.</p> <ol> <li>The data collected by Bodil corresponds to the plague period.</li> <li>The data provided by Lennart Palm which contains the population size for each parish in 1699 and 1718 based on a combination of tax records and estimations of population totals for Scania.</li> <li>The geographical information (polygons) for some parishes. This information doesn't correspond to the plague period.</li> </ol> <p>Our goal is to create a unique database for our project: Plague spread across Scania, Sweden, from 1710 to 1715.</p> <ol> <li>We start working with Bodil's information which we store in two databases: One database corresponds to the parishes affected by the plague, the region where parishes are located in Scania, the beginning and end of the outbreaks, and the number of victims. The second database corresponds to all the parishes in Scania during the plague period, the district, and the region they belonged to.</li> </ol> <p>The goal is to merge these two databases. First we set the working directory for private and public files.</p> In\u00a0[29]: Copied! <pre># For public files paths\ndata_folder = \"data\"\nappendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")\n\n# For private files paths\ndata_private_folder = \"data/private\"\nallParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\")\n</pre> # For public files paths data_folder = \"data\" appendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")  # For private files paths data_private_folder = \"data/private\" allParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\") <p>Reading the different data sources (.xlsx, and .csv files)</p> In\u00a0[30]: Copied! <pre># Bodil's data Appendix 6 plague parishes\nplagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")\n\n# All parishes from the Northwest Scania with population data from Lennart Palm file\nnorthwestScania = pd.read_excel(allParishes_path, sheet_name=\"northwest\")\n</pre> # Bodil's data Appendix 6 plague parishes plagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")  # All parishes from the Northwest Scania with population data from Lennart Palm file northwestScania = pd.read_excel(allParishes_path, sheet_name=\"northwest\") In\u00a0[31]: Copied! <pre>plagueParishesScania[plagueParishesScania['Region']=='Northwest']\n</pre> plagueParishesScania[plagueParishesScania['Region']=='Northwest'] Out[31]: BeginPlaguePeriod EndPlaguePeriod ParishName VictimsNumber Region 51 Sep 1710 Oct 1710 St Ibb 3 Northwest 52 Jan 1711 Oct 1711 Allerum 275 Northwest 53 Apr 1711 Jul 1711 Brunnby ? Northwest 54 Apr 1711 Undefined Grevie 20 Northwest 55 Apr 1711 Jan 1712 Helsingborg 133 Northwest 56 Apr 1711 Sep 1711 Kattarp 21 Northwest 57 Apr 1711 May 1711 V\u00e4stra Broby ? Northwest 58 May 1711 Jun 1711 Bark\u00e5kra 20 Northwest 59 Oct 1711 Nov 1711 Bark\u00e5kra 20 Northwest 60 May 1711 Undefined F\u00f6rsl\u00f6v ? Northwest 61 May 1711 Oct 1711 Str\u00f6velstorp 90 Northwest 62 May 1711 Undefined Kropp 120 Northwest 63 Sep 1712 Sep 1712 Kropp 5 Northwest 64 May 1711 Sep 1711 Norra Vram 35 Northwest 65 Nov 1712 Nov 1712 Norra Vram ? Northwest 66 May 1711 Aug 1711 V\u00e4linge 9 Northwest 67 May 1711 Oct 1711 V\u00e4sby 190 Northwest 68 May 1711 Sep 1711 Fleninge 100 Northwest 69 Jun 1711 Oct 1711 Aus\u00e5s 30 Northwest 70 Jun 1711 Jul 1711 H\u00f6ja 4 Northwest 71 Jun 1711 Undefined Jonstorp 91 Northwest 72 Jun 1711 Jul 1711 Kvistofta 8 Northwest 73 Nov 1711 Dec 1711 Kvistofta 3 Northwest 74 Jun 1711 Undefined B\u00e5rsl\u00f6v ? Northwest 75 Jul 1711 Jul 1711 Ekeby 3 Northwest 76 Sep 1712 Oct 1712 Ekeby 5 Northwest 77 Jul 1711 Undefined Fj\u00e4restad ? Northwest 78 Jul 1711 Undefined H\u00e4rsl\u00f6v ? Northwest 79 Jul 1711 Undefined Kvidinge 13 Northwest 80 Jul 1711 Jan 1712 Landskrona 330 Northwest 81 Sep 1712 Dec 1712 Landskrona 15 Northwest 82 Jun 1711 Undefined M\u00f6rarp 35 Northwest 83 Sep 1712 Oct 1712 M\u00f6rarp 6 Northwest 84 Jul 1711 Undefined Raus ? Northwest 85 Jun 1711 Aug 1711 Sval\u00f6v 6 Northwest 86 Oct 1712 Undefined Sval\u00f6v 45 Northwest 87 Jun 1714 Undefined Sval\u00f6v 1 Northwest 88 Aug 1711 Feb 1712 K\u00e5ger\u00f6d 88 Northwest 89 Aug 1711 Sep 1711 \u00d6rja 3 Northwest 90 Aug 1711 Dec 1711 \u00d6stra Ljungby 10 Northwest 91 Sep 1711 Oct 1711 Frillestad 5 Northwest 92 Sep 1711 Sep 1711 Stenestad 4 Northwest 93 Oct 1711 Nov 1711 Farhult 3 Northwest 94 Jan 1712 Feb 1712 Torrl\u00f6sa 5 Northwest 95 Jul 1712 Aug 1712 Bosarp 6 Northwest 96 Nov 1712 Feb 1713 H\u00e4sslunda ? Northwest 97 Oct 1712 Undefined Risekatsl\u00f6sa ? Northwest 98 Nov 1712 Nov 1712 Bj\u00f6rnekulla 6 Northwest 99 Nov 1712 Dec 1712 Resl\u00f6v 3 Northwest 100 Jun 1712 Aug 1712 Billinge ? Northwest In\u00a0[32]: Copied! <pre>northwestScania.loc[0:20]\n</pre> northwestScania.loc[0:20] Out[32]: Region District(H\u00e4rad) ParishName BEF1699 BEF1718 AV_BEF ChurchBook OtherSources 0 Northwest Bj\u00e4re Bark\u00e5kra 541.0 610.0 575.5 1 5 1 Northwest Bj\u00e4re B\u00e5stad 361.0 407.0 384.0 3 5 2 Northwest Bj\u00e4re F\u00f6rsl\u00f6v 508.0 573.0 540.5 2 4 3 Northwest Bj\u00e4re Grevie 657.0 741.0 699.0 2 4 4 Northwest Bj\u00e4re Hj\u00e4rnarp 456.0 515.0 485.5 3 5 5 Northwest Bj\u00e4re Hov 461.0 520.0 490.5 3 5 6 Northwest Bj\u00e4re Rebbelberga 147.0 166.0 156.5 2 5 7 Northwest Bj\u00e4re Torekov 171.0 193.0 182.0 3 5 8 Northwest Bj\u00e4re V\u00e4stra Karup 1061.0 1197.0 1129.0 2 5 9 Northwest Luggude Allerum 799.0 902.0 850.5 2 4 10 Northwest Luggude Bjuv 229.0 259.0 244.0 3 5 11 Northwest Luggude Brunnby 1038.0 1172.0 1105.0 3 4 12 Northwest Luggude B\u00e5rsl\u00f6v 194.0 219.0 206.5 3 4 13 Northwest Luggude Ekeby 374.0 422.0 398.0 1 4 14 Northwest Luggude Farhult 299.0 338.0 318.5 1 5 15 Northwest Luggude Fj\u00e4restad 229.0 258.0 243.5 3 4 16 Northwest Luggude Fleninge 284.0 321.0 302.5 2 4 17 Northwest Luggude Frillestad 124.0 140.0 132.0 2 4 18 Northwest Luggude Halmstad 232.0 262.0 247.0 3 5 19 Northwest Luggude Helsingborg 952.0 1074.0 1013.0 1 5 20 Northwest Luggude H\u00e4sslunda 218.0 246.0 232.0 3 4 <p>Transforming the lowercase to uppercase and checking the type</p> In\u00a0[33]: Copied! <pre>plagueParishesScania = plagueParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nnorthwestScania = northwestScania.apply(\n    lambda x: x.astype(str).str.upper())\n</pre> plagueParishesScania = plagueParishesScania.apply(     lambda x: x.astype(str).str.upper()) northwestScania = northwestScania.apply(     lambda x: x.astype(str).str.upper()) <p>Merging the two datasets (northwestScania and plagueParishesScania)</p> In\u00a0[34]: Copied! <pre>northwestParishesPop = pd.merge(\n   northwestScania, plagueParishesScania, how='left', on=['ParishName'])\n# Drop duplicate columns\nnorthwestParishesPop = northwestParishesPop.drop(columns='Region_y', axis=1)\n</pre> northwestParishesPop = pd.merge(    northwestScania, plagueParishesScania, how='left', on=['ParishName']) # Drop duplicate columns northwestParishesPop = northwestParishesPop.drop(columns='Region_y', axis=1) In\u00a0[35]: Copied! <pre># Rename column\nnorthwestParishesPop = northwestParishesPop.rename(\n    columns={'Region_x': 'Region'})\n</pre> # Rename column northwestParishesPop = northwestParishesPop.rename(     columns={'Region_x': 'Region'}) <p>Extracting the parishes' names from the data frame</p> In\u00a0[36]: Copied! <pre>parishesNorthwestScania_names = get_Names(\n    northwestParishesPop, 'ParishName').unique().tolist()\n</pre> parishesNorthwestScania_names = get_Names(     northwestParishesPop, 'ParishName').unique().tolist() <ol> <li>The geographical information for Scania is already projected on the plane, i.e. the measures are in meters not in longitude and latitude. To process the shape file, we set the directory and chose the columns to work with.</li> </ol> In\u00a0[37]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\")\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\nparishScaniaMap = parishScaniaMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\") SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] parishScaniaMap = parishScaniaMap[selected_columns] <p>Now, we remove white spaces and patterns. Then, the shape file is filtered considering the column \"GET_END_YE\" of the polygon.</p> In\u00a0[38]: Copied! <pre>parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'\n                                                                    ])\n</pre> parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'                                                                     ]) <p>Working only with Southwest Scania: southwestParishesPop and parishScaniaMap</p> In\u00a0[39]: Copied! <pre>df1 = northwestParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=80,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = northwestParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=80,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[40]: Copied! <pre>northwestParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n</pre> northwestParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] In\u00a0[42]: Copied! <pre>northwestParishMap.loc[northwestParishMap['ParishName'] == 'K\u00c5GER\u00d6D']\n</pre> northwestParishMap.loc[northwestParishMap['ParishName'] == 'K\u00c5GER\u00d6D'] Out[42]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id 29 NORTHWEST LUGGUDE K\u00c5GER\u00d6D K\u00c5GER\u00d6DS 786.0 887.0 836.5 1 5 AUG 1711 FEB 1712 88 POLYGON ((4193102.225 3231352.519, 4193361.055... <p>After checking the previous data, we found some issues when we assign the polygons to HOV (5), BJUV (10), HELSINGBORG (19), V\u00c4SBY (33), ASK (40), and ST IBB (72). We will modify the geographical data for these parishes using the indexes and the shape file.</p> In\u00a0[16]: Copied! <pre># Check the index of the parish that is not matched\nnorthwestParishMap.loc[northwestParishMap['ParishName'] == 'HOV']\nnorthwestParishMap.loc[northwestParishMap['ParishName'] == 'BJUV']\nnorthwestParishMap.loc[northwestParishMap['ParishName'] == 'HELSINGBORG']\nnorthwestParishMap.loc[northwestParishMap['ParishName'] == 'V\u00c4SBY']\nnorthwestParishMap.loc[northwestParishMap['ParishName'] == 'ST IBB']\n\n# Check the correct geographical information in the shapefile\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'H0VS']\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'BJUVS']\nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'HELSINGBORGS MARIA')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1872)\n                     ] \nparishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'V\u00c4SBY') \n                     &amp; (parishScaniaMap['GET_END_YE'] == 1851)\n                     ) \n                    | ((parishScaniaMap['G_NAME'] == 'VIKENS'))\n                    ] \nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'SANKT IBBS']\n</pre> # Check the index of the parish that is not matched northwestParishMap.loc[northwestParishMap['ParishName'] == 'HOV'] northwestParishMap.loc[northwestParishMap['ParishName'] == 'BJUV'] northwestParishMap.loc[northwestParishMap['ParishName'] == 'HELSINGBORG'] northwestParishMap.loc[northwestParishMap['ParishName'] == 'V\u00c4SBY'] northwestParishMap.loc[northwestParishMap['ParishName'] == 'ST IBB']  # Check the correct geographical information in the shapefile parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'H0VS'] parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'BJUVS'] parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'HELSINGBORGS MARIA')                      &amp; (parishScaniaMap['GET_END_YE'] == 1872)                      ]  parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'V\u00c4SBY')                       &amp; (parishScaniaMap['GET_END_YE'] == 1851)                      )                      | ((parishScaniaMap['G_NAME'] == 'VIKENS'))                     ]  parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'SANKT IBBS']                    Out[16]: G_NAME GET_END_YE geometry 406 SANKT IBBS 9999 POLYGON ((4164582.624 3224721.541, 4165366.284... <p>Modifying the geographical information</p> In\u00a0[17]: Copied! <pre># HOV\nnorthwestParishMap.at[6, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'HOVS', 'G_NAME'].values[0]\nnorthwestParishMap.at[6, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'HOVS', 'geometry'].values[0]\n# BJUV\nnorthwestParishMap.at[11, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'BJUVS', 'G_NAME'].values[0]\nnorthwestParishMap.at[11, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'BJUVS', 'geometry'].values[0]\n# HELSINGBORG\nnorthwestParishMap.at[21, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'HELSINGBORGS MARIA')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1872), 'G_NAME'].values[0]\nnorthwestParishMap.at[21, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'HELSINGBORGS MARIA')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1872), 'geometry'].values[0]\n\n# V\u00c4SBY\nnorthwestParishMap.at[38, 'G_NAME'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'V\u00c4SBY') \n                                                      &amp; (parishScaniaMap['GET_END_YE'] == 1851)\n                                                      ) \n                                                      |((parishScaniaMap['G_NAME'] == 'VIKENS')                                                   \n                                                      ), 'G_NAME'].values[0]\n# Use unary_union to merge the polygons that satisfied the given conditions\nnorthwestParishMap.at[38, 'geometry'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'V\u00c4SBY') \n                     &amp; (parishScaniaMap['GET_END_YE'] == 1851)\n                     ) \n                    | ((parishScaniaMap['G_NAME'] == 'VIKENS'))\n                    , 'geometry'].unary_union\n\n# SANKT IBBS\nnorthwestParishMap.at[79, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'SANKT IBBS')\n                     , 'G_NAME'].values[0]\nnorthwestParishMap.at[79, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'SANKT IBBS')\n                     , 'geometry'].values[0]\n</pre> # HOV northwestParishMap.at[6, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'HOVS', 'G_NAME'].values[0] northwestParishMap.at[6, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'HOVS', 'geometry'].values[0] # BJUV northwestParishMap.at[11, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'BJUVS', 'G_NAME'].values[0] northwestParishMap.at[11, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'BJUVS', 'geometry'].values[0] # HELSINGBORG northwestParishMap.at[21, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'HELSINGBORGS MARIA')                      &amp; (parishScaniaMap['GET_END_YE'] == 1872), 'G_NAME'].values[0] northwestParishMap.at[21, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'HELSINGBORGS MARIA')                      &amp; (parishScaniaMap['GET_END_YE'] == 1872), 'geometry'].values[0]  # V\u00c4SBY northwestParishMap.at[38, 'G_NAME'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'V\u00c4SBY')                                                        &amp; (parishScaniaMap['GET_END_YE'] == 1851)                                                       )                                                        |((parishScaniaMap['G_NAME'] == 'VIKENS')                                                                                                          ), 'G_NAME'].values[0] # Use unary_union to merge the polygons that satisfied the given conditions northwestParishMap.at[38, 'geometry'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'V\u00c4SBY')                       &amp; (parishScaniaMap['GET_END_YE'] == 1851)                      )                      | ((parishScaniaMap['G_NAME'] == 'VIKENS'))                     , 'geometry'].unary_union  # SANKT IBBS northwestParishMap.at[79, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'SANKT IBBS')                      , 'G_NAME'].values[0] northwestParishMap.at[79, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'SANKT IBBS')                      , 'geometry'].values[0]                                          In\u00a0[18]: Copied! <pre>northwestScaniaMap = gpd.GeoDataFrame(northwestParishMap, geometry='geometry')\n</pre> northwestScaniaMap = gpd.GeoDataFrame(northwestParishMap, geometry='geometry') In\u00a0[19]: Copied! <pre>northwestScaniaMap = get_area(northwestScaniaMap)\nnorthwestScaniaMap = get_centroid(northwestScaniaMap)\nfrom shapely.geometry import Point, mapping\nnorthwestScaniaMap['centroid'] = northwestScaniaMap['centroid'].apply(mapping)\n</pre> northwestScaniaMap = get_area(northwestScaniaMap) northwestScaniaMap = get_centroid(northwestScaniaMap) from shapely.geometry import Point, mapping northwestScaniaMap['centroid'] = northwestScaniaMap['centroid'].apply(mapping) In\u00a0[20]: Copied! <pre>northwestScaniaMap.to_csv('northwestScania.csv', index=False)\n</pre> northwestScaniaMap.to_csv('northwestScania.csv', index=False) <p>Plotting the northwest parishes</p> <p>Before to plot the map of parishes, for a given Geodataframe, we assign 'red' to the parishes affected by the plague and blue for the others. This information is added as a column with heading 'color'.</p> In\u00a0[21]: Copied! <pre>colorByColumn(northwestScaniaMap, 'EndPlaguePeriod')\nnorthwestMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nnorthwestScaniaMap.explore(\n    m = northwestMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"Southwest Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    northwestMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(northwestMap)  # use folium to add layer control\n\nnorthwestMap  # show map\n</pre> colorByColumn(northwestScaniaMap, 'EndPlaguePeriod') northwestMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  northwestScaniaMap.explore(     m = northwestMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"Southwest Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     northwestMap )  # use folium to add alternative tiles folium.LayerControl().add_to(northwestMap)  # use folium to add layer control  northwestMap  # show map Out[21]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[22]: Copied! <pre># Assuming you have a GeoDataFrame named 'gdf'\ndef calculate_quotient(gdf, col1, col2):\n    gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')\n    gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')\n    \n    # Calculate the death rate per 1000 inhabitants\n    gdf['quotient'] = (gdf[col1] / gdf[col2])*1000\n    pass\n\ncalculate_quotient(northwestScaniaMap, 'VictimsNumber', 'BEF1699')\n</pre> # Assuming you have a GeoDataFrame named 'gdf' def calculate_quotient(gdf, col1, col2):     gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')     gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')          # Calculate the death rate per 1000 inhabitants     gdf['quotient'] = (gdf[col1] / gdf[col2])*1000     pass  calculate_quotient(northwestScaniaMap, 'VictimsNumber', 'BEF1699') In\u00a0[23]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel2')\nnorthwestScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel2') northwestScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show()"},{"location":"PlagueProject/southeastScaniadatabase/","title":"southeastScaniadatabase","text":"In\u00a0[16]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3  <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[17]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n%matplotlib inline\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py %matplotlib inline  <p>We have three different data sources.</p> <ol> <li>The data collected by Bodil corresponds to the plague period.</li> <li>The data provided by Lennart Palm which contains the population size for each parish in 1699 and 1718 based on a combination of tax records and estimations of population totals for Scania.</li> <li>The information from the TABVERK database includes the population size for parishes in the posterior years of the plague.</li> <li>The geographical information (polygons) for some parishes. This information doesn't correspond to the plague period.</li> </ol> <p>Our goal is to create a unique database for our project: Plague spread across Scania, Sweden, from 1710 to 1715.</p> <ol> <li>We start working with Bodil's information which we store in two databases: One database corresponds to the parishes affected by the plague, the region where parishes are located in Scania, the beginning and end of the outbreaks, and the number of victims. The second database corresponds to all the parishes in Scania during the plague period, the district, and the region they belonged to.</li> </ol> <p>The goal is to merge these two databases. First we set the working directory for private and public files.</p> In\u00a0[18]: Copied! <pre># For public files paths\ndata_folder = \"data\"\nappendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")\n\n# For private files paths\ndata_private_folder = \"data/private\"\nallParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\")\n</pre> # For public files paths data_folder = \"data\" appendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")  # For private files paths data_private_folder = \"data/private\" allParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\") <p>Reading the different data sources (.xlsx, and .csv files)</p> In\u00a0[19]: Copied! <pre># Bodil's data Appendix 6 plague parishes\nplagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")\n# All parishes in Scania during the plague period\nallParishesScania = pd.read_excel(allParishes_path)\n# All parishes from the Southeast, Middle and Southwest region of Scania with population data from Lennart Palm file\nsoutheastScania = pd.read_excel(allParishes_path, sheet_name=\"southeast\")\n</pre> # Bodil's data Appendix 6 plague parishes plagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\") # All parishes in Scania during the plague period allParishesScania = pd.read_excel(allParishes_path) # All parishes from the Southeast, Middle and Southwest region of Scania with population data from Lennart Palm file southeastScania = pd.read_excel(allParishes_path, sheet_name=\"southeast\") In\u00a0[20]: Copied! <pre>plagueParishesScania[plagueParishesScania['Region'] == 'Southeast']\n</pre> plagueParishesScania[plagueParishesScania['Region'] == 'Southeast'] Out[20]: BeginPlaguePeriod EndPlaguePeriod ParishName VictimsNumber Region 144 Jun 1711 Oct 1711 \u00d6vraby 60 Southeast 145 Sep 1712 Sep 1712 \u00d6vraby ? Southeast 146 Nov 1712 Nov 1712 \u00d6vraby ? Southeast 147 Oct 1711 Nov 1711 S\u00f6dra Mellby 3 Southeast 148 Apr 1712 May 1712 Bromma ? Southeast 149 Jun 1712 Dec 1712 Ystad 740 Southeast 150 Jun 1712 Mar 1713 \u00d6ja 40 Southeast 151 Jul 1712 Undefined Bj\u00e4resj\u00f6 ? Southeast 152 Jul 1712 Undefined H\u00f6rup 60 Southeast 153 Jul 1712 Jan 1713 Stora K\u00f6pinge 80 Southeast 154 Jul 1712 Undefined Valleberga ? Southeast 155 Aug 1712 Undefined Baldringe ? Southeast 156 Aug 1712 Nov 1712 Balk\u00e5kra 45 Southeast 157 Aug 1712 Aug 1712 Glemminge ? Southeast 158 Sep 1713 Oct 1713 Glemminge ? Southeast 159 Aug 1712 Undefined Hammenh\u00f6g ? Southeast 160 Aug 1712 Undefined Ingelstorp 1 Southeast 161 Aug 1712 Dec 1712 Sk\u00e5rby 68 Southeast 162 Aug 1712 Aug 1713 Sn\u00e5restad 38 Southeast 163 Aug 1712 Aug 1712 Villie ? Southeast 164 Aug 1712 Aug 1712 V\u00e4stra N\u00f6bbel\u00f6v 2 Southeast 165 Sep 1712 Oct 1712 Hedeskoga 5 Southeast 166 Sep 1712 Dec 1712 Tryde 40 Southeast 167 Sep 1712 Sep 1712 \u00d6stra Tommarp ? Southeast 168 Oct 1712 Dec 1712 S\u00f6vestad ? Southeast 169 Dec 1712 Sep 1713 Tran\u00e5s 127 Southeast 170 Jan 1713 Undefined Eljar\u00f6d 3 Southeast 171 Jan 1713 Oct 1713 L\u00f6derup 270 Southeast 172 Mar 1713 Oct 1713 Borrby 45 Southeast 173 Sep 1713 Oct 1713 Tosterup ? Southeast <p>Transforming the lowercase to uppercase and checking the type</p> In\u00a0[21]: Copied! <pre>allParishesScania = allParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nplagueParishesScania = plagueParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nsoutheastScania = southeastScania.apply(\n    lambda x: x.astype(str).str.upper())\ntype(plagueParishesScania)\ntype(allParishesScania)\n</pre> allParishesScania = allParishesScania.apply(     lambda x: x.astype(str).str.upper()) plagueParishesScania = plagueParishesScania.apply(     lambda x: x.astype(str).str.upper()) southeastScania = southeastScania.apply(     lambda x: x.astype(str).str.upper()) type(plagueParishesScania) type(allParishesScania) Out[21]: <pre>pandas.core.frame.DataFrame</pre> <p>Visualizing the DataFrames and calculating the length of each one.</p> In\u00a0[22]: Copied! <pre>print(len(allParishesScania))\nplagueParishesScania.head(3)\n</pre> print(len(allParishesScania)) plagueParishesScania.head(3) <pre>397\n</pre> Out[22]: BeginPlaguePeriod EndPlaguePeriod ParishName VictimsNumber Region 0 NOV 1710 APR 1711 N\u00c4SUM 671 NORTHEAST 1 FEB 1712 UNDEFINED N\u00c4SUM ? NORTHEAST 2 NOV 1710 AUG 1711 IV\u00d6 123 NORTHEAST In\u00a0[23]: Copied! <pre>print(len(plagueParishesScania))\nallParishesScania.head(3)\n</pre> print(len(plagueParishesScania)) allParishesScania.head(3) <pre>174\n</pre> Out[23]: Region District(H\u00e4rad) ParishName 0 SOUTHEAST ALBO ANDRARUM 1 SOUTHEAST ALBO BR\u00d6SARP 2 SOUTHEAST ALBO ELJAR\u00d6D <p>Merging the two datasets (allParishesScania and plagueParishesScania)</p> In\u00a0[24]: Copied! <pre>parishesScania = pd.merge(\n    allParishesScania, plagueParishesScania, how='left', on=['ParishName', 'Region'])\n</pre> parishesScania = pd.merge(     allParishesScania, plagueParishesScania, how='left', on=['ParishName', 'Region']) <p>Checking that the new data frame keep all the outbreaks for parish</p> In\u00a0[25]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'N\u00c4SUM']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'N\u00c4SUM']  Out[25]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 394 NORTHEAST VILLANDS N\u00c4SUM NOV 1710 APR 1711 671 395 NORTHEAST VILLANDS N\u00c4SUM FEB 1712 UNDEFINED ? <p>Extracting the parishes' names from the data frame</p> In\u00a0[26]: Copied! <pre>parishesScania_names = get_Names(\n    parishesScania, 'ParishName').unique().tolist()\nlen(parishesScania_names)\n</pre> parishesScania_names = get_Names(     parishesScania, 'ParishName').unique().tolist() len(parishesScania_names)  Out[26]: <pre>396</pre> <p>The length of 'parishesScania_names' is less than the number of rows in the data frame 'allparishesScania'. This means, there is a repeated name: 'L\u00d6DDEK\u00d6PINGE'. We have to check the information for this parish:</p> In\u00a0[27]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[27]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 86 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? 160 SOUTHWEST TORNA L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? <p>Only the parish L\u00d6DDEK\u00d6PINGE at HARJAGER was affected by the plague according to the file 'Bilaga 6 d - sydva\u0308st.doc' provided by Bodil. So we need to fix the information in the other row (160).</p> In\u00a0[28]: Copied! <pre>parishesScania.at[160, 'BeginPlaguePeriod'] = np.NaN\nparishesScania.at[160, 'EndPlaguePeriod'] = np.NaN\nparishesScania.at[160, 'VictimsNumber'] = np.NaN\n</pre> parishesScania.at[160, 'BeginPlaguePeriod'] = np.NaN parishesScania.at[160, 'EndPlaguePeriod'] = np.NaN parishesScania.at[160, 'VictimsNumber'] = np.NaN  <p>Checking the data:</p> In\u00a0[29]: Copied! <pre>parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> parishesScania.loc[parishesScania['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[29]: Region District(H\u00e4rad) ParishName BeginPlaguePeriod EndPlaguePeriod VictimsNumber 86 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE AUG 1712 DEC 1712 ? 160 SOUTHWEST TORNA L\u00d6DDEK\u00d6PINGE NaN NaN NaN <p>Filtering the data frame by region and then get the names of the parishes:</p> In\u00a0[30]: Copied! <pre>southeastParishes = parishesByregion(parishesScania, 'SOUTHEAST')\n</pre> southeastParishes = parishesByregion(parishesScania, 'SOUTHEAST') In\u00a0[31]: Copied! <pre>southeastParishes_names = get_Names(southeastParishes, 'ParishName')\n# Check parishes per region by name\nsoutheastParishes.loc[southeastParishes['ParishName'] == 'R\u00d6RUM']\nlen(southeastParishes_names)\n</pre> southeastParishes_names = get_Names(southeastParishes, 'ParishName') # Check parishes per region by name southeastParishes.loc[southeastParishes['ParishName'] == 'R\u00d6RUM'] len(southeastParishes_names) Out[31]: <pre>62</pre> <ol> <li>Here, we used the population size estimations provided by Lennart Palm. Merging the two datasets (southeastScania and southeastParishes)</li> </ol> In\u00a0[32]: Copied! <pre>southeastParishesPop = pd.merge(\n    southeastScania, southeastParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)'])\n</pre> southeastParishesPop = pd.merge(     southeastScania, southeastParishes, how='left', on=['ParishName', 'Region', 'District(H\u00e4rad)']) In\u00a0[34]: Copied! <pre>southeastParishesPop.head(10)\n</pre> southeastParishesPop.head(10) Out[34]: Region District(H\u00e4rad) ParishName BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber 0 SOUTHEAST ALBO ANDRARUM 1100 1241 1170.5 2 5 NaN NaN NaN 1 SOUTHEAST ALBO BR\u00d6SARP 480 541 510.5 2 5 NaN NaN NaN 2 SOUTHEAST ALBO ELJAR\u00d6D 320 361 340.5 3 4 JAN 1713 UNDEFINED 3 3 SOUTHEAST ALBO F\u00c5GELTOFTA 454 512 483.0 2 5 NaN NaN NaN 4 SOUTHEAST ALBO RAVLUNDA 434 489 461.5 2 5 NaN NaN NaN 5 SOUTHEAST ALBO R\u00d6RUM 393 444 418.5 3 5 NaN NaN NaN 6 SOUTHEAST ALBO SANKT OLOF 296 334 315.0 3 5 NaN NaN NaN 7 SOUTHEAST ALBO S\u00d6DRA MELLBY 628 708 668.0 1 4 OCT 1711 NOV 1711 3 8 SOUTHEAST ALBO VITABY 473 534 503.5 2 5 NaN NaN NaN 9 SOUTHEAST HERRESTAD BALDRINGE 235 265 250.0 3 4 AUG 1712 UNDEFINED ? <ol> <li>The geographical information for Scania is already projected on the plane, i.e. the measures are in meters not in longitude and latitude. To process the shape file, we proceed as with the census file. First, we set the directory and chose the columns to work with.</li> </ol> In\u00a0[35]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\")\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\nparishScaniaMap = parishScaniaMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\") SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] parishScaniaMap = parishScaniaMap[selected_columns]  <p>Now, we remove white spaces and patterns. Then, we filter the shape file considering the column \"GET_END_YE\" of the polygon.</p> In\u00a0[36]: Copied! <pre>parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'\n                                                                        ])\nparishScaniaMap = process_dataframe(parishScaniaMap, 'G_NAME', 'GET_END_YE')\nlen(parishScaniaMap)\n</pre> parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'                                                                         ]) parishScaniaMap = process_dataframe(parishScaniaMap, 'G_NAME', 'GET_END_YE') len(parishScaniaMap) Out[36]: <pre>412</pre> <p>Plotting the map only with the polygons obtained after clean the data</p> In\u00a0[37]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nparishScaniaMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) parishScaniaMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show() <p>Working only with Southeast Scania: southeastParishesPop and parishScaniaMap</p> In\u00a0[38]: Copied! <pre>df1 = southeastParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=80,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = southeastParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=80,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[42]: Copied! <pre>southeastParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n# Get the index for Sk\u00f6rup for fixing the geographical data\n# print(southeastParishMap.loc[southeastParishMap['ParishName'] == 'R\u00d6RUM'])\n# print(southeastParishMap.loc[southeastParishMap['ParishName'] == 'SK\u00d6RUP'])\nsoutheastParishMap.loc[40:65]\n</pre> southeastParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] # Get the index for Sk\u00f6rup for fixing the geographical data # print(southeastParishMap.loc[southeastParishMap['ParishName'] == 'R\u00d6RUM']) # print(southeastParishMap.loc[southeastParishMap['ParishName'] == 'SK\u00d6RUP']) southeastParishMap.loc[40:65]  Out[42]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id 40 SOUTHEAST INGELSTAD \u00d6STRA INGELSTAD \u00d6STRA INGELSTADS 113 127 120.0 3 5 NaN NaN NaN POLYGON ((4252740.191 3185633.164, 4252765.342... 41 SOUTHEAST INGELSTAD \u00d6VRABY \u00d6VRABY 208 235 221.5 1 4 JUN 1711 OCT 1711 60 POLYGON ((4242987.654 3183129.802, 4242831.234... 42 SOUTHEAST INGELSTAD \u00d6VRABY \u00d6VRABY 208 235 221.5 1 4 SEP 1712 SEP 1712 ? POLYGON ((4242987.654 3183129.802, 4242831.234... 43 SOUTHEAST INGELSTAD \u00d6VRABY \u00d6VRABY 208 235 221.5 1 4 NOV 1712 NOV 1712 ? POLYGON ((4242987.654 3183129.802, 4242831.234... 44 SOUTHEAST J\u00c4RRESTAD BOLSH\u00d6G BOLSH\u00d6GS 180 204 192.0 3 5 NaN NaN NaN POLYGON ((4260388.996 3185283.921, 4260628.137... 45 SOUTHEAST J\u00c4RRESTAD BORRBY BORRBY 552 623 587.5 2 4 MAR 1713 OCT 1713 45 POLYGON ((4257298.006 3181643.940, 4257440.553... 46 SOUTHEAST J\u00c4RRESTAD GLADSAX GLADSAXS 384 433 408.5 3 5 NaN NaN NaN POLYGON ((4260455.799 3193066.045, 4260401.310... 47 SOUTHEAST J\u00c4RRESTAD JERRESTAD J\u00c4RRESTADS 191 215 203.0 3 5 NaN NaN NaN POLYGON ((4260674.600 3188271.591, 4260674.576... 48 SOUTHEAST J\u00c4RRESTAD SIMRIS SIMRIS 261 294 277.5 2 5 NaN NaN NaN POLYGON ((4264225.685 3187020.058, 4263766.715... 49 SOUTHEAST J\u00c4RRESTAD SIMRISHAMN SIMRISHAMNS 491 532 511.5 3 5 NaN NaN NaN POLYGON ((4265674.431 3189809.256, 4264835.596... 50 SOUTHEAST J\u00c4RRESTAD STIBY STIBY 298 336 317.0 3 5 NaN NaN NaN POLYGON ((4253317.672 3191764.739, 4253479.915... 51 SOUTHEAST J\u00c4RRESTAD VALLBY VALLBY 334 377 355.5 3 5 NaN NaN NaN POLYGON ((4257298.006 3181643.940, 4256788.461... 52 SOUTHEAST J\u00c4RRESTAD \u00d6STRA N\u00d6BBEL\u00d6V \u00d6STRA N\u00d6BBEL\u00d6VS 262 296 279.0 2 5 NaN NaN NaN POLYGON ((4264225.685 3187020.058, 4264461.753... 53 SOUTHEAST J\u00c4RRESTAD \u00d6STRA TOMMARP \u00d6STRA TOMMARPS 466 526 496.0 3 4 SEP 1712 SEP 1712 ? POLYGON ((4260674.600 3188271.591, 4260465.599... 54 SOUTHEAST J\u00c4RRESTAD \u00d6STRA VEMMERL\u00d6V \u00d6STRA VEMMERL\u00d6VS 417 470 443.5 3 5 NaN NaN NaN POLYGON ((4258644.221 3191773.769, 4258406.913... 55 SOUTHEAST LJUNITS BALK\u00c5KRA BALK\u00c5KRA 169 191 180.0 1 5 AUG 1712 NOV 1712 45 POLYGON ((4228908.467 3177516.647, 4228752.084... 56 SOUTHEAST LJUNITS KATSL\u00d6SA KATSL\u00d6SA 195 220 207.5 3 5 NaN NaN NaN POLYGON ((4220929.933 3177748.416, 4221086.614... 57 SOUTHEAST LJUNITS SK\u00c5RBY SK\u00c5RBY 399 451 425.0 1 5 AUG 1712 DEC 1712 68 POLYGON ((4224367.564 3185742.892, 4224558.376... 58 SOUTHEAST LJUNITS SK\u00d6RUP SKURUPS 242 273 257.5 3 5 NaN NaN NaN POLYGON ((4214392.138 3182887.177, 4214576.093... 59 SOUTHEAST LJUNITS SN\u00c5RESTAD SN\u00c5RESTADS 233 263 248.0 1 5 AUG 1712 AUG 1713 38 POLYGON ((4226882.184 3177334.009, 4226917.376... 60 SOUTHEAST LJUNITS VILLIE VILLIE 544 614 579.0 3 4 AUG 1712 AUG 1712 ? POLYGON ((4220610.739 3185080.301, 4220857.620... 61 SOUTHEAST LJUNITS V\u00c4STRA N\u00d6BBEL\u00d6V V\u00c4STRA N\u00d6BBEL\u00d6VS 221 249 235.0 3 4 AUG 1712 AUG 1712 2 POLYGON ((4221023.524 3176346.729, 4221462.715... <p>We need to modify manually the geographical information assigned to Sk\u00f6rup and R\u00f6rum for Sj\u00f6rups and R\u00f6rums information,respectively.</p> In\u00a0[43]: Copied! <pre># Get the geometry from the map\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == 'SJ\u00d6RUPS', 'geometry'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME']\n                    == 'R\u00d6RUMS', 'geometry'].values[0]\n\n# Get the name from the map\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'SJ\u00d6RUPS', 'G_NAME'].values[0]\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'R\u00d6RUMS', 'G_NAME'].values[0]\n</pre> # Get the geometry from the map parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == 'SJ\u00d6RUPS', 'geometry'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME']                     == 'R\u00d6RUMS', 'geometry'].values[0]  # Get the name from the map parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'SJ\u00d6RUPS', 'G_NAME'].values[0] parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'R\u00d6RUMS', 'G_NAME'].values[0]  Out[43]: <pre>'R\u00d6RUMS'</pre> In\u00a0[44]: Copied! <pre>southeastParishMap.at[5, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'R\u00d6RUMS', 'G_NAME'].values[0]\nsoutheastParishMap.at[5, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                           == 'R\u00d6RUMS', 'geometry'].values[0]\nsoutheastParishMap.at[58, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                          == 'SJ\u00d6RUPS', 'G_NAME'].values[0]\nsoutheastParishMap.at[58, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                            == 'SJ\u00d6RUPS', 'geometry'].values[0]\n</pre> southeastParishMap.at[5, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'R\u00d6RUMS', 'G_NAME'].values[0] southeastParishMap.at[5, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                            == 'R\u00d6RUMS', 'geometry'].values[0] southeastParishMap.at[58, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                           == 'SJ\u00d6RUPS', 'G_NAME'].values[0] southeastParishMap.at[58, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                             == 'SJ\u00d6RUPS', 'geometry'].values[0]  In\u00a0[45]: Copied! <pre>type(southeastParishMap)\n</pre> type(southeastParishMap) Out[45]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[46]: Copied! <pre>southeastScaniaMap = gpd.GeoDataFrame(southeastParishMap, geometry='geometry')\n# Add 1 if the parish was affected by the plague, 0 otherwise\nclassByPlague(southeastScaniaMap)\n</pre> southeastScaniaMap = gpd.GeoDataFrame(southeastParishMap, geometry='geometry') # Add 1 if the parish was affected by the plague, 0 otherwise classByPlague(southeastScaniaMap) In\u00a0[49]: Copied! <pre>southeastScaniaMap.to_csv('southeastScania.csv', index=False)\n</pre> southeastScaniaMap.to_csv('southeastScania.csv', index=False)  In\u00a0[47]: Copied! <pre>southeastScaniaMap = get_area(southeastScaniaMap)\nsoutheastScaniaMap = get_centroid(southeastScaniaMap)\nfrom shapely.geometry import Point, mapping\nsoutheastScaniaMap['centroid'] = southeastScaniaMap['centroid'].apply(mapping)\n</pre> southeastScaniaMap = get_area(southeastScaniaMap) southeastScaniaMap = get_centroid(southeastScaniaMap) from shapely.geometry import Point, mapping southeastScaniaMap['centroid'] = southeastScaniaMap['centroid'].apply(mapping) <p>Plotting the southeast parishes</p> <p>Before to plot the map of parishes, for a given Geodataframe, we assign 'red' to the parishes affected by the plague and blue for the others. This information is added as a column with heading 'color'.</p> In\u00a0[48]: Copied! <pre>colorByColumn(southeastScaniaMap, 'BeginPlaguePeriod')\nsoutheastMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nsoutheastScaniaMap.explore(\n    m = southeastMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"Southeast Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    southeastMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(southeastMap)  # use folium to add layer control\n\nsoutheastMap  # show map\n</pre> colorByColumn(southeastScaniaMap, 'BeginPlaguePeriod') southeastMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  southeastScaniaMap.explore(     m = southeastMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"Southeast Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     southeastMap )  # use folium to add alternative tiles folium.LayerControl().add_to(southeastMap)  # use folium to add layer control  southeastMap  # show map Out[48]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[30]: Copied! <pre>type(southeastParishMap)\n</pre> type(southeastParishMap) Out[30]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[31]: Copied! <pre># Assuming you have a GeoDataFrame named 'gdf'\ndef calculate_quotient(gdf, col1, col2):\n    gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')\n    gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')\n    \n    # Calculate the death rate per 1000 inhabitants\n    gdf['quotient'] = (gdf[col1] / gdf[col2])*1000\n    pass\n\ncalculate_quotient(southeastScaniaMap, 'VictimsNumber', 'BEF1699')\n</pre> # Assuming you have a GeoDataFrame named 'gdf' def calculate_quotient(gdf, col1, col2):     gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')     gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')          # Calculate the death rate per 1000 inhabitants     gdf['quotient'] = (gdf[col1] / gdf[col2])*1000     pass  calculate_quotient(southeastScaniaMap, 'VictimsNumber', 'BEF1699') In\u00a0[32]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel1')\nsoutheastScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel1') southeastScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show()  <ol> <li>We will process the census file to get the population size by parish. This file corresponds to all of Sweden, so we process it to keep only the information for Scania for the closest year to the plague outbreaks. We start setting the directory and reading the census file:</li> </ol> In\u00a0[33]: Copied! <pre># Set the working directory for private files\ndata_private_folder = \"data/private\"\ncensus_path = os.path.join(data_private_folder, 'FILE01_FALD.csv')\ncensusSweden = pd.read_csv(census_path, sep=';')\ncensusSweden.shape\n</pre> # Set the working directory for private files data_private_folder = \"data/private\" census_path = os.path.join(data_private_folder, 'FILE01_FALD.csv') censusSweden = pd.read_csv(census_path, sep=';') censusSweden.shape  Out[33]: <pre>(102360, 50)</pre> <p>Checking the memory usage (this is not necessary)</p> In\u00a0[34]: Copied! <pre># censusSweden.info(memory_usage='deep')\n</pre> # censusSweden.info(memory_usage='deep')  <p>Checking the names of all columns in the data</p> In\u00a0[35]: Copied! <pre>columns = censusSweden.columns\n</pre> columns = censusSweden.columns <p>Calling the data only with specific columns to reduce the memory usage.</p> In\u00a0[36]: Copied! <pre>censusSweden = pd.read_csv(census_path, sep=';', usecols=[\n                           'LANGENNMN'  # Standard name of the county for the geographical area in plain text\n                           , 'GEOIDNMN'  # Standard name of the geographical area in plain text, i.e. not a source name\n                           , 'GEOIDTYP'  # Type of breakdown of the geographical area  0 =Assembly, 1 = Pastorate, 2 = Other type, 3 = Several parishes, 9 = Part of a parish\n                           , 'AR'  # Year\n                           , 'KON'  # 1 = Man  2 = Female. I choose 1 but it could be 2 for the total population\n                           , 'BEF_TOT'  # Total population at source\n                           , 'BEF_GENTOT'  # Total population, generated\n                           ])\n</pre> censusSweden = pd.read_csv(census_path, sep=';', usecols=[                            'LANGENNMN'  # Standard name of the county for the geographical area in plain text                            , 'GEOIDNMN'  # Standard name of the geographical area in plain text, i.e. not a source name                            , 'GEOIDTYP'  # Type of breakdown of the geographical area  0 =Assembly, 1 = Pastorate, 2 = Other type, 3 = Several parishes, 9 = Part of a parish                            , 'AR'  # Year                            , 'KON'  # 1 = Man  2 = Female. I choose 1 but it could be 2 for the total population                            , 'BEF_TOT'  # Total population at source                            , 'BEF_GENTOT'  # Total population, generated                            ])  <p>Processing the census data such that corresponds only to Scania.</p> In\u00a0[37]: Copied! <pre>censusScania = censusSweden.loc[((censusSweden['LANGENNMN'] == 'KRISTIANSTADS L\u00c4N') | (\n    censusSweden['LANGENNMN'] == 'MALM\u00d6HUS L\u00c4N')) &amp; (censusSweden['KON'] == 1)]\ncensusScania.shape\n</pre> censusScania = censusSweden.loc[((censusSweden['LANGENNMN'] == 'KRISTIANSTADS L\u00c4N') | (     censusSweden['LANGENNMN'] == 'MALM\u00d6HUS L\u00c4N')) &amp; (censusSweden['KON'] == 1)] censusScania.shape  Out[37]: <pre>(8748, 7)</pre> <p>Cleaning the data: Now, we remove given strings and white spaces at the end of a word. To do so, we must provide the string list to delete. In this step, you can use regular expressions.</p> In\u00a0[38]: Copied! <pre># Regex to delete the following strings:\n# ', DEL (KRISTIANSTAD)', ', DEL (MALM\u00d6HUS)', ', DEL (MALM\u00d6HUS L\u00c4N)'\n# ,', DEL AV (FROSTA H\u00c4RAD, MALM\u00d6HUS L\u00c4N)', ', DEL (EVER\u00d6D, MALM\u00d6HUS)'\n# ,' DEL (HYLLINGE, MALM\u00d6HUS)', ' (MALM\u00d6 SF)', '(STAFFANSTORP)'\n# ,' GARNISONSF\u00d6RS.', ' OCH GARNISIONSF\u00d6RS.', ' STADS', ' STAD'\n\n# regex = r'((,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISIONSF\u00d6RS\\.?)|(\\s+STADS?))$'\nregex = r'(,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISONSF\u00d6RS\\.?)'\ncensusScania = replace_strings_and_regex(censusScania, 'GEOIDNMN', [\n    'PASTORAT', 'HOSPITAL', ' LANDS', ' SLOTTSF\u00d6RSAMLING', ' DOMKYRKOF\u00d6RSAMLING', ' STADS', regex\n])\ncensusScania.shape\n</pre> # Regex to delete the following strings: # ', DEL (KRISTIANSTAD)', ', DEL (MALM\u00d6HUS)', ', DEL (MALM\u00d6HUS L\u00c4N)' # ,', DEL AV (FROSTA H\u00c4RAD, MALM\u00d6HUS L\u00c4N)', ', DEL (EVER\u00d6D, MALM\u00d6HUS)' # ,' DEL (HYLLINGE, MALM\u00d6HUS)', ' (MALM\u00d6 SF)', '(STAFFANSTORP)' # ,' GARNISONSF\u00d6RS.', ' OCH GARNISIONSF\u00d6RS.', ' STADS', ' STAD'  # regex = r'((,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISIONSF\u00d6RS\\.?)|(\\s+STADS?))$' regex = r'(,?\\s+DEL\\s(AV\\s)?\\((\\w+,?\\s?)+\\))|(\\s*\\(([^YSTAD]+,?\\s?)+\\))|(\\s*(OCH\\s)?GARNISONSF\u00d6RS\\.?)' censusScania = replace_strings_and_regex(censusScania, 'GEOIDNMN', [     'PASTORAT', 'HOSPITAL', ' LANDS', ' SLOTTSF\u00d6RSAMLING', ' DOMKYRKOF\u00d6RSAMLING', ' STADS', regex ]) censusScania.shape  Out[38]: <pre>(8748, 7)</pre> <p>Process the data from Scania only to keep the first population size registered for each parish. This was done following two approaches.</p> <ol> <li>First approach: We group the data by parish name and then select the minimum year. As the minimum year is not unique after deleted strings, this approach allows repetitions.</li> </ol> In\u00a0[39]: Copied! <pre>popSizeScania_rep = process_dataframe_rep(censusScania, 'GEOIDNMN', 'AR')\nprint(popSizeScania_rep.shape)\npopSizeScania_rep.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')\n</pre> popSizeScania_rep = process_dataframe_rep(censusScania, 'GEOIDNMN', 'AR') print(popSizeScania_rep.shape) popSizeScania_rep.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')  <pre>(475, 7)\n</pre> Out[39]: LANGENNMN GEOIDNMN GEOIDTYP AR KON BEF_TOT BEF_GENTOT 57393 MALM\u00d6HUS L\u00c4N HELSINGBORGS 0 1775 1 1290 1290 57426 MALM\u00d6HUS L\u00c4N HELSINGBORGS 0 1775 1 453 453 <ol> <li>Second approach: This method explores the given DataFrame exhaustively and keeps the required information in a dictionary. In our case, this information corresponds to the position associated with each parish name and the minimum year, according to the original DataFrame. This approach doesn't allow repetitions since the condition for replacing the information in the dictionary is strict (&lt;).</li> </ol> In\u00a0[40]: Copied! <pre>popSizeScania = process_dataframe(censusScania, 'GEOIDNMN', 'AR')\nprint(popSizeScania.shape)\npopSizeScania.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')\n</pre> popSizeScania = process_dataframe(censusScania, 'GEOIDNMN', 'AR') print(popSizeScania.shape) popSizeScania.groupby(['GEOIDNMN']).get_group('HELSINGBORGS')  <pre>(473, 7)\n</pre> Out[40]: LANGENNMN GEOIDNMN GEOIDTYP AR KON BEF_TOT BEF_GENTOT 57393 MALM\u00d6HUS L\u00c4N HELSINGBORGS 0 1775 1 1290 1290"},{"location":"PlagueProject/southwestScaniadatabase/","title":"southwestScaniadatabase","text":"In\u00a0[43]: Copied! <pre>%load_ext autoreload\n%autoreload 3\n</pre> %load_ext autoreload %autoreload 3 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[44]: Copied! <pre># Python 3.11.2\nfrom funct_process_data import *  # Import all functions from funct_process_data.py\n%matplotlib inline\n</pre> # Python 3.11.2 from funct_process_data import *  # Import all functions from funct_process_data.py %matplotlib inline <p>We have three different data sources.</p> <ol> <li>The data collected by Bodil corresponds to the plague period.</li> <li>The data provided by Lennart Palm which contains the population size for each parish in 1699 and 1718 based on a combination of tax records and estimations of population totals for Scania.</li> <li>The geographical information (polygons) for some parishes. This information doesn't correspond to the plague period.</li> </ol> <p>Our goal is to create a unique database for our project: Plague spread across Scania, Sweden, from 1710 to 1715.</p> <ol> <li>We start working with Bodil's information which we store in two databases: One database corresponds to the parishes affected by the plague, the region where parishes are located in Scania, the beginning and end of the outbreaks, and the number of victims. The second database corresponds to all the parishes in Scania during the plague period, the district, and the region they belonged to.</li> </ol> <p>The goal is to merge these two databases. First we set the working directory for private and public files.</p> In\u00a0[45]: Copied! <pre># For public files paths\ndata_folder = \"data\"\nappendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")\n\n# For private files paths\ndata_private_folder = \"data/private\"\nallParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\")\n</pre> # For public files paths data_folder = \"data\" appendix6_path = os.path.join(data_folder, \"Appendix6Bodil.csv\")  # For private files paths data_private_folder = \"data/private\" allParishes_path = os.path.join(data_private_folder, \"allParishesScania.xlsx\") <p>Reading the different data sources (.xlsx, and .csv files)</p> In\u00a0[46]: Copied! <pre># Bodil's data Appendix 6 plague parishes\nplagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")\n\n# All parishes from the Southwest Scania with population data from Lennart Palm file\nsouthwestScania = pd.read_excel(allParishes_path, sheet_name=\"southwest\")\n</pre> # Bodil's data Appendix 6 plague parishes plagueParishesScania = pd.read_csv(appendix6_path, sep=\",\", encoding=\"utf-8\")  # All parishes from the Southwest Scania with population data from Lennart Palm file southwestScania = pd.read_excel(allParishes_path, sheet_name=\"southwest\") <p>Transforming the lowercase to uppercase and checking the type</p> In\u00a0[47]: Copied! <pre>plagueParishesScania = plagueParishesScania.apply(\n    lambda x: x.astype(str).str.upper())\nsouthwestScania = southwestScania.apply(\n    lambda x: x.astype(str).str.upper())\n</pre> plagueParishesScania = plagueParishesScania.apply(     lambda x: x.astype(str).str.upper()) southwestScania = southwestScania.apply(     lambda x: x.astype(str).str.upper()) <p>Merging the two datasets (southwestScania and plagueParishesScania)</p> In\u00a0[48]: Copied! <pre>southwestParishesPop = pd.merge(\n    southwestScania, plagueParishesScania, how='left', on=['ParishName', 'Region'])\n</pre> southwestParishesPop = pd.merge(     southwestScania, plagueParishesScania, how='left', on=['ParishName', 'Region']) <p>Extracting the parishes' names from the data frame</p> In\u00a0[49]: Copied! <pre>parishesSouthwestScania_names = get_Names(\n    southwestParishesPop, 'ParishName').unique().tolist()\nlen(parishesSouthwestScania_names)\n</pre> parishesSouthwestScania_names = get_Names(     southwestParishesPop, 'ParishName').unique().tolist() len(parishesSouthwestScania_names) Out[49]: <pre>131</pre> <ol> <li>The geographical information for Scania is already projected on the plane, i.e. the measures are in meters not in longitude and latitude. To process the shape file, we set the directory and chose the columns to work with.</li> </ol> In\u00a0[50]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\")\nSkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\nselected_columns = ['G_NAME', 'GET_END_YE', 'geometry']\nSkaneMap = SkaneMap[selected_columns]\nparishScaniaMap = parishScaniaMap[selected_columns]\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = os.path.join(data_folder, \"Parishes1720_1890.shp\") SkaneMap_path = os.path.join(data_folder, \"Skane1720_1890.shp\")  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) selected_columns = ['G_NAME', 'GET_END_YE', 'geometry'] SkaneMap = SkaneMap[selected_columns] parishScaniaMap = parishScaniaMap[selected_columns] <p>Now, we remove white spaces and patterns. Then, the shape file is filtered considering the column \"GET_END_YE\" of the polygon.</p> In\u00a0[51]: Copied! <pre>parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'\n                                                                    ])\n</pre> parishScaniaMap = replace_strings_and_regex(parishScaniaMap, 'G_NAME', [' F\u00d6RSAMLING', ' L L\u00c4N', ' S L\u00c4N', ' M L\u00c4N', ' HELIGA TREFALDIGHETS', ' LANDSF\u00d6RSAMLING', ' STADS', ' LANDS', ' SK\u00c5NES'                                                                     ]) <p>Working only with Southwest Scania: southwestParishesPop and parishScaniaMap</p> In\u00a0[52]: Copied! <pre>df1 = southwestParishesPop\ndf2 = parishScaniaMap\n\ndf_matches = fuzzy_match(\n    df1,\n    df2,\n    'ParishName',\n    'G_NAME',\n    threshold=80,\n    limit=1\n)\n\ndf_output = df1.merge(\n    df_matches,\n    how='left',\n    left_index=True,\n    right_on='df_left_id'\n).merge(\n    df2,\n    how='left',\n    left_on='df_right_id',\n    right_index=True,\n    suffixes=['_df1', '_df2']\n)\n\n# For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table\ndf_output.set_index('df_left_id', inplace=True)\n\n# df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching\ndf_output.index.name = 'id'\n</pre> df1 = southwestParishesPop df2 = parishScaniaMap  df_matches = fuzzy_match(     df1,     df2,     'ParishName',     'G_NAME',     threshold=80,     limit=1 )  df_output = df1.merge(     df_matches,     how='left',     left_index=True,     right_on='df_left_id' ).merge(     df2,     how='left',     left_on='df_right_id',     right_index=True,     suffixes=['_df1', '_df2'] )  # For some reason the first merge operation wrecks the dataframe's index. Recreated from the value we have in the matches lookup table df_output.set_index('df_left_id', inplace=True)  # df_output = df_output[['col_a_df1', 'col_b_df1', 'col_b_df2']]      # Drop columns used in the matching df_output.index.name = 'id' In\u00a0[53]: Copied! <pre>southwestParishMap = df_output[[\n    'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']]\n</pre> southwestParishMap = df_output[[     'Region', 'District(H\u00e4rad)', 'ParishName', 'G_NAME', 'BEF1699', 'BEF1718', 'AV_BEF', 'ChurchBook', 'OtherSources', 'BeginPlaguePeriod', 'EndPlaguePeriod', 'VictimsNumber', 'geometry']] In\u00a0[54]: Copied! <pre>southwestParishMap.loc[southwestParishMap['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']\n</pre> southwestParishMap.loc[southwestParishMap['ParishName'] == 'L\u00d6DDEK\u00d6PINGE']  Out[54]: Region District(H\u00e4rad) ParishName G_NAME BEF1699 BEF1718 AV_BEF ChurchBook OtherSources BeginPlaguePeriod EndPlaguePeriod VictimsNumber geometry id 24 SOUTHWEST HARJAGER L\u00d6DDEK\u00d6PINGE L\u00d6DDEK\u00d6PINGE 269 304 286.5 3 4 AUG 1712 DEC 1712 ? POLYGON ((4185510.738 3208330.617, 4185787.224... <p>After checking the previous data, we found some issues when we assign the polygons to H\u00f6g (21), Esl\u00f6v (31), Fosie (36) Limhamn (40), Malm\u00f6 (43), Oxie (45) H\u00e5sl\u00f6v (67), and Lund (95). We will modify the geographical data for these parishes using the indexes and the shape file.</p> In\u00a0[55]: Copied! <pre># Check the index of the parish that is not matched\nsouthwestParishMap.loc[southwestParishMap['ParishName'] == 'H\u00d6G']\nsouthwestParishMap.loc[southwestParishMap['ParishName'] == 'ESL\u00d6V']\nsouthwestParishMap.loc[southwestParishMap['ParishName'] == 'FOSIE']\nsouthwestParishMap.loc[southwestParishMap['ParishName'] == 'LIMHAMN']\nsouthwestParishMap.loc[southwestParishMap['ParishName'] == 'H\u00c5SL\u00d6V']\nsouthwestParishMap.loc[southwestParishMap['ParishName'] == 'LUND']\n\n# Check the correct geographical information in the shapefile\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'H\u00d6GS']\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'ESL\u00d6VS']\nparishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'HASL\u00d6VS']\nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LUNDS DOMKYRKOF\u00d6RSAMLING')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1961)\n                     ]\nparishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PETRI') \n                     &amp; (parishScaniaMap['GET_END_YE'] == 1905)\n                     ) \n                    | ((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PAULI') \n                       &amp; (parishScaniaMap['GET_END_YE'] == 1948)\n                       )\n                    | (parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT JOHANNES')\n                     ]\nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LIMHAMNS')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1968)\n        ]  \nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'FOSIE')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1968)\n        ] \nparishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'OXIE')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1982)\n        ]\n</pre> # Check the index of the parish that is not matched southwestParishMap.loc[southwestParishMap['ParishName'] == 'H\u00d6G'] southwestParishMap.loc[southwestParishMap['ParishName'] == 'ESL\u00d6V'] southwestParishMap.loc[southwestParishMap['ParishName'] == 'FOSIE'] southwestParishMap.loc[southwestParishMap['ParishName'] == 'LIMHAMN'] southwestParishMap.loc[southwestParishMap['ParishName'] == 'H\u00c5SL\u00d6V'] southwestParishMap.loc[southwestParishMap['ParishName'] == 'LUND']  # Check the correct geographical information in the shapefile parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'H\u00d6GS'] parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'ESL\u00d6VS'] parishScaniaMap.loc[parishScaniaMap['G_NAME'] == 'HASL\u00d6VS'] parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LUNDS DOMKYRKOF\u00d6RSAMLING')                      &amp; (parishScaniaMap['GET_END_YE'] == 1961)                      ] parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PETRI')                       &amp; (parishScaniaMap['GET_END_YE'] == 1905)                      )                      | ((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PAULI')                         &amp; (parishScaniaMap['GET_END_YE'] == 1948)                        )                     | (parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT JOHANNES')                      ] parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LIMHAMNS')                      &amp; (parishScaniaMap['GET_END_YE'] == 1968)         ]   parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'FOSIE')                      &amp; (parishScaniaMap['GET_END_YE'] == 1968)         ]  parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'OXIE')                      &amp; (parishScaniaMap['GET_END_YE'] == 1982)         ]  Out[55]: G_NAME GET_END_YE geometry 248 OXIE 1982 POLYGON ((4190322.447 3183903.804, 4187920.789... <p>Modifying the geographical information</p> In\u00a0[56]: Copied! <pre># H\u00d6G\nsouthwestParishMap.at[21, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00d6GS', 'G_NAME'].values[0]\nsouthwestParishMap.at[21, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00d6GS', 'geometry'].values[0]\n# ESL\u00d6V\nsouthwestParishMap.at[31, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'ESL\u00d6VS', 'G_NAME'].values[0]\nsouthwestParishMap.at[31, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'ESL\u00d6VS', 'geometry'].values[0]\n# FOSIE\nsouthwestParishMap.at[36, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'FOSIE')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'G_NAME'].values[0]\nsouthwestParishMap.at[36, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'FOSIE')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'geometry'].values[0]\n# OXIE\nsouthwestParishMap.at[45, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'OXIE')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1982), 'G_NAME'].values[0]\nsouthwestParishMap.at[45, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'OXIE')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1982), 'geometry'].values[0]\n# LIMHAMN\nsouthwestParishMap.at[40, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LIMHAMNS')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'G_NAME'].values[0]\nsouthwestParishMap.at[40, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LIMHAMNS')\n                     &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'geometry'].values[0]\n        \n# MALM\u00d6\nsouthwestParishMap.at[43, 'G_NAME'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PETRI') \n                                                      &amp; (parishScaniaMap['GET_END_YE'] == 1905)\n                                                      ) \n                                                   | ((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PAULI') \n                                                      &amp; (parishScaniaMap['GET_END_YE'] == 1948)\n                                                      )\n                                                   | (parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT JOHANNES'), 'G_NAME'].values[0]\n# Use unary_union to merge the polygons that satisfied the given conditions\nsouthwestParishMap.at[43, 'geometry'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PETRI') \n                     &amp; (parishScaniaMap['GET_END_YE'] == 1905)\n                     ) \n                    | ((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PAULI') \n                       &amp; (parishScaniaMap['GET_END_YE'] == 1948)\n                       )\n                    | (parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT JOHANNES'), 'geometry'].unary_union\n# H\u00c5SL\u00d6V\nsouthwestParishMap.at[67, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00c5SL\u00d6VS', 'G_NAME'].values[0]\nsouthwestParishMap.at[67, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']\n                                                         == 'H\u00c5SL\u00d6VS', 'geometry'].values[0]\n# LUND\nsouthwestParishMap.at[95, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME']\n                                                         == 'LUNDS DOMKYRKOF\u00d6RSAMLING') &amp; \n                                                         (parishScaniaMap['GET_END_YE'] == 1961), 'G_NAME'].values[0]\nsouthwestParishMap.at[95, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME']\n                                                         == 'LUNDS DOMKYRKOF\u00d6RSAMLING') &amp;\n                                                         (parishScaniaMap['GET_END_YE'] == 1961), 'geometry'].values[0]\n</pre> # H\u00d6G southwestParishMap.at[21, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00d6GS', 'G_NAME'].values[0] southwestParishMap.at[21, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00d6GS', 'geometry'].values[0] # ESL\u00d6V southwestParishMap.at[31, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'ESL\u00d6VS', 'G_NAME'].values[0] southwestParishMap.at[31, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'ESL\u00d6VS', 'geometry'].values[0] # FOSIE southwestParishMap.at[36, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'FOSIE')                      &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'G_NAME'].values[0] southwestParishMap.at[36, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'FOSIE')                      &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'geometry'].values[0] # OXIE southwestParishMap.at[45, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'OXIE')                      &amp; (parishScaniaMap['GET_END_YE'] == 1982), 'G_NAME'].values[0] southwestParishMap.at[45, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'OXIE')                      &amp; (parishScaniaMap['GET_END_YE'] == 1982), 'geometry'].values[0] # LIMHAMN southwestParishMap.at[40, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LIMHAMNS')                      &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'G_NAME'].values[0] southwestParishMap.at[40, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME'] == 'LIMHAMNS')                      &amp; (parishScaniaMap['GET_END_YE'] == 1968), 'geometry'].values[0]          # MALM\u00d6 southwestParishMap.at[43, 'G_NAME'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PETRI')                                                        &amp; (parishScaniaMap['GET_END_YE'] == 1905)                                                       )                                                     | ((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PAULI')                                                        &amp; (parishScaniaMap['GET_END_YE'] == 1948)                                                       )                                                    | (parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT JOHANNES'), 'G_NAME'].values[0] # Use unary_union to merge the polygons that satisfied the given conditions southwestParishMap.at[43, 'geometry'] = parishScaniaMap.loc[((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PETRI')                       &amp; (parishScaniaMap['GET_END_YE'] == 1905)                      )                      | ((parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT PAULI')                         &amp; (parishScaniaMap['GET_END_YE'] == 1948)                        )                     | (parishScaniaMap['G_NAME'] == 'MALM\u00d6 SANKT JOHANNES'), 'geometry'].unary_union # H\u00c5SL\u00d6V southwestParishMap.at[67, 'G_NAME'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00c5SL\u00d6VS', 'G_NAME'].values[0] southwestParishMap.at[67, 'geometry'] = parishScaniaMap.loc[parishScaniaMap['G_NAME']                                                          == 'H\u00c5SL\u00d6VS', 'geometry'].values[0] # LUND southwestParishMap.at[95, 'G_NAME'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME']                                                          == 'LUNDS DOMKYRKOF\u00d6RSAMLING') &amp;                                                           (parishScaniaMap['GET_END_YE'] == 1961), 'G_NAME'].values[0] southwestParishMap.at[95, 'geometry'] = parishScaniaMap.loc[(parishScaniaMap['G_NAME']                                                          == 'LUNDS DOMKYRKOF\u00d6RSAMLING') &amp;                                                          (parishScaniaMap['GET_END_YE'] == 1961), 'geometry'].values[0]                                                   In\u00a0[57]: Copied! <pre>southwestScaniaMap = gpd.GeoDataFrame(southwestParishMap, geometry='geometry')\n</pre> southwestScaniaMap = gpd.GeoDataFrame(southwestParishMap, geometry='geometry') In\u00a0[58]: Copied! <pre>#southwestScaniaMap.to_csv('southwestScania.csv', index=False)\n</pre> #southwestScaniaMap.to_csv('southwestScania.csv', index=False) In\u00a0[59]: Copied! <pre>southwestScaniaMap = get_area(southwestScaniaMap)\nsouthwestScaniaMap = get_centroid(southwestScaniaMap)\nfrom shapely.geometry import Point, mapping\nsouthwestScaniaMap['centroid'] = southwestScaniaMap['centroid'].apply(mapping)\n</pre> southwestScaniaMap = get_area(southwestScaniaMap) southwestScaniaMap = get_centroid(southwestScaniaMap) from shapely.geometry import Point, mapping southwestScaniaMap['centroid'] = southwestScaniaMap['centroid'].apply(mapping) <p>Plotting the southwest parishes</p> <p>Before to plot the map of parishes, for a given Geodataframe, we assign 'red' to the parishes affected by the plague and blue for the others. This information is added as a column with heading 'color'.</p> In\u00a0[60]: Copied! <pre>colorByColumn(southwestScaniaMap, 'EndPlaguePeriod')\nsouthwestMap : folium.folium.Map = SkaneMap.explore(\n    column=\"G_NAME\",\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    tooltip=False,\n    zoom_control=False,\n    legend=False,\n    #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme\n    legend_kwds=dict(colorbar=False),  # do not use colorbar\n    name=\"Scania\",  # name of the layer in the map\n)\n\nsouthwestScaniaMap.explore(\n    m = southwestMap,  # pass the map object\n    column=\"color\",  # use \"name\" column to assign colors\n    cmap=['blue','red'],  # color map to use\n    legend=False,  # show legend\n    style_kwds=dict(color=\"black\"),  # use black for borders\n    marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill\n    # show \"name\" column in the tooltip\n    tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],\n    tooltip_kwds=dict(labels=True),  # show column label in the tooltip\n    name=\"Southwest Scania\",  # name of the layer in the map,\n    zoom_control=False,\n)\n\nfolium.TileLayer(\"Stamen Toner\", show=False).add_to(\n    southwestMap\n)  # use folium to add alternative tiles\nfolium.LayerControl().add_to(southwestMap)  # use folium to add layer control\n\nsouthwestMap  # show map\n</pre> colorByColumn(southwestScaniaMap, 'EndPlaguePeriod') southwestMap : folium.folium.Map = SkaneMap.explore(     column=\"G_NAME\",     style_kwds=dict(color=\"black\"),  # use black for borders     tooltip=False,     zoom_control=False,     legend=False,     #scheme=\"naturalbreaks\",  # use mapclassify's natural breaks scheme     legend_kwds=dict(colorbar=False),  # do not use colorbar     name=\"Scania\",  # name of the layer in the map )  southwestScaniaMap.explore(     m = southwestMap,  # pass the map object     column=\"color\",  # use \"name\" column to assign colors     cmap=['blue','red'],  # color map to use     legend=False,  # show legend     style_kwds=dict(color=\"black\"),  # use black for borders     marker_kwds=dict(radius=5, fill=True),  # make marker radius 10px with fill     # show \"name\" column in the tooltip     tooltip=[\"G_NAME\", \"BEF1699\", \"BeginPlaguePeriod\", \"EndPlaguePeriod\", \"VictimsNumber\"],     tooltip_kwds=dict(labels=True),  # show column label in the tooltip     name=\"Southwest Scania\",  # name of the layer in the map,     zoom_control=False, )  folium.TileLayer(\"Stamen Toner\", show=False).add_to(     southwestMap )  # use folium to add alternative tiles folium.LayerControl().add_to(southwestMap)  # use folium to add layer control  southwestMap  # show map Out[60]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[61]: Copied! <pre>type(southwestParishMap)\n</pre> type(southwestParishMap) Out[61]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[62]: Copied! <pre># Assuming you have a GeoDataFrame named 'gdf'\ndef calculate_quotient(gdf, col1, col2):\n    gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')\n    gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')\n    \n    # Calculate the death rate per 1000 inhabitants\n    gdf['quotient'] = (gdf[col1] / gdf[col2])*1000\n    pass\n\ncalculate_quotient(southwestScaniaMap, 'VictimsNumber', 'BEF1699')\n</pre> # Assuming you have a GeoDataFrame named 'gdf' def calculate_quotient(gdf, col1, col2):     gdf[col1] = pd.to_numeric(gdf[col1], errors='coerce')     gdf[col2] = pd.to_numeric(gdf[col2], errors='coerce')          # Calculate the death rate per 1000 inhabitants     gdf['quotient'] = (gdf[col1] / gdf[col2])*1000     pass  calculate_quotient(southwestScaniaMap, 'VictimsNumber', 'BEF1699') In\u00a0[63]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 7))\nSkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',\n              legend=False, cmap='Pastel2')\nsouthwestScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',\n                        edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(10, 7)) SkaneMap.plot(ax=ax, column=\"G_NAME\", edgecolor='black',               legend=False, cmap='Pastel2') southwestScaniaMap.plot(ax=ax, column=\"color\", cmap='Reds',                         edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') plt.show()"},{"location":"PlagueProject/CodeBookKeeling/SIRMetapol/","title":"SIR Metapol","text":"In\u00a0[1]: Copied! <pre>#!/usr/bin/env python\n\n####################################################################\n###    This is the PYTHON version of program 7.2 from page 242 of  #\n### \"Modeling Infectious Disease in humans and animals\"            #\n### by Keeling &amp; Rohani.\t\t\t\t\t\t\t\t\t\t   #\n### #\n### It is the SIR epidemic in a metapopulationFor simplicity births#\n### and deaths have been ignored, and we work with numbers of      #\n### individuals.                                                   #\n### Y[i][j] refers to infected individual who are currently in i   #\n### but live in j..                                                #\n####################################################################\n\n###################################\n### Written by Ilias Soumpasis    #\n### ilias.soumpasis@ucd.ie (work) #\n### ilias.soumpasis@gmail.com\t  #\n###################################\n\nimport scipy.integrate as spi\nimport numpy as np\nimport pylab as pl\nfrom matplotlib.font_manager import FontProperties as fmp\n\nn = 5    # Number of subpopulations\nbeta = 1.0*np.ones(n)  # vector of infection rates with lenght n\ngamma = 0.3*np.ones(n)  # vector of removal or recovery rates with lenght n\n\n# Xij represents the number of susceptibles in subpopulation i that live in subpopulation j\n# Yij represents the number of infected in subpopulation i that live in subpopulation j\n# Nij represents the number of total hosts currently in subpopulation i that live in subpopulation j\n\n\n# vector of size n*n where N0_ij is the number of individuals in each spatial class\nN0 = np.zeros(n*n)\n# is the initial number of susceptible individuals in each spatial class; X(0) is a vector of size n \u00d7 n.\nX0 = np.zeros(n*n)\nfor i in np.arange(0, n*n, n+1):\n    N0[i] = 1000.0\n    X0[i] = 800.0\n\nY0 = np.zeros(n*n)\nY0[0] = 1.0\nND = MaxTime = 60.\nTS = 1.0\n\nl = np.zeros((n, n))\nr = np.zeros((n, n))\nfor i in range(n):\n    for j in range(n):\n        if abs(i-j) == 1:\n            l[i][j] = 0.1\nr = 2*np.ones((n, n))\nr = r-np.diag(np.diag(r))#putting 0 in the diagonal of the r matrix by extracting the diagonal and subtracting it from the matrix\n\n\nINPUT0 = np.hstack((X0, Y0, N0))#The numpy.hstack() function takes the arrays X0, Y0, N0 and stacks them horizontally to make a single array of (n*n,3). \nINPUT = np.zeros((3*n*n))\nfor i in range(n*n):\n    INPUT[3*i] = INPUT0[i]\n    INPUT[1+3*i] = INPUT0[n*n+i]\n    INPUT[2+3*i] = INPUT0[2*n*n+i]\n\n\ndef diff_eqs(y, t):\n    '''The main set of equations'''\n    Y = np.zeros((3*n*n))\n    V = y\n    sumY = np.zeros(n)\n    sumN = np.zeros(n)\n\n    # Calculate number currently in Subpopulation i\n    for i in range(n):\n        sumY[i] = 0.0\n        sumN[i] = 0.0\n        for j in range(n):\n            k = 3*(j+i*n)\n            sumN[i] += V[2+k]\n            sumY[i] += V[1+k]\n\n    # Set all rates to zeros\n    for i in range(n):\n        for j in range(n):\n            k = 3*(j+i*n) #first compute i*n, then add j\n            Y[k] = 0\n            Y[1+k] = 0\n            Y[2+k] = 0\n\n    for i in range(n):\n        for j in range(n):\n            # Calculate the rates\n            k = 3 * (j+i*n) #first compute i*n, then add j\n            K = 3 * (i+j*n) #first compute j*n, then add i\n            h = 3 * (i+i*n) #first compute i*n, then add i\n            H = 3 * (j+j*n) #first compute j*n, then add j\n\n            Y[k] -= (beta[i]*V[k]*(sumY[i]/sumN[i])) # Xii and Xij\n            Y[k+1] += (beta[i]*V[k]*(sumY[i]/sumN[i])) # Yii and Yij\n            Y[k+1] -= (gamma[i]*V[k+1]) # Yii and Yij\n\n            # Movement\n            Y[h] += r[j][i]*V[K]\n            Y[h] -= l[j][i]*V[h]\n\n            Y[h+1] += r[j][i]*V[K+1]\n            Y[h+1] -= l[j][i]*V[h+1]\n\n            Y[h+2] += r[j][i]*V[K+2]\n            Y[h+2] -= l[j][i]*V[h+2]\n\n            Y[k] += l[i][j]*V[H]\n            Y[k] -= r[i][j]*V[k]\n\n            Y[1+k] += l[i][j]*V[1+H]\n            Y[1+k] -= r[i][j]*V[1+k]\n\n            Y[2+k] += l[i][j]*V[2+H]\n            Y[2+k] -= r[i][j]*V[2+k]\n    return Y   # For odeint\n\n\nt_start = 0.0\nt_end = ND\nt_inc = TS\nt_range = np.arange(t_start, t_end+t_inc, t_inc)\nt_course = spi.odeint(diff_eqs, INPUT, t_range)\ntc = t_course\n\n# Plotting\ntotalS = np.zeros((len(tc), 5))\ntotalI = np.zeros((len(tc), 5))\n\nfor i in range(n):\n    for j in range(n):\n        k = 3*(j+i*n)\n        totalS[:, i] += tc[:, k]\n        totalI[:, i] += tc[:, k+1]\n\n\n# print len(totalS)\npl.subplot(211)\nfor i in range(5):\n    pl.plot(t_range, totalS[:, i], label=('data %s' %\n            (i+1)), color=(0.3, i/10.+0.5, 0.1))\npl.xlabel('Time')\npl.ylabel('Susceptibles')\npl.legend(loc=1, prop=fmp(size='smaller'))\npl.subplot(212)\nfor i in range(5):\n    pl.plot(t_range, totalI[:, i], label=('data %s' %\n            (i+1)), color=(0.8, i/10.+0., 0.3))\npl.xlabel('Time')\npl.ylabel('Infectious')\npl.legend(loc=1, prop=fmp(size='smaller'))\n\npl.show()\n</pre> #!/usr/bin/env python  #################################################################### ###    This is the PYTHON version of program 7.2 from page 242 of  # ### \"Modeling Infectious Disease in humans and animals\"            # ### by Keeling &amp; Rohani.\t\t\t\t\t\t\t\t\t\t   # ### # ### It is the SIR epidemic in a metapopulationFor simplicity births# ### and deaths have been ignored, and we work with numbers of      # ### individuals.                                                   # ### Y[i][j] refers to infected individual who are currently in i   # ### but live in j..                                                # ####################################################################  ################################### ### Written by Ilias Soumpasis    # ### ilias.soumpasis@ucd.ie (work) # ### ilias.soumpasis@gmail.com\t  # ###################################  import scipy.integrate as spi import numpy as np import pylab as pl from matplotlib.font_manager import FontProperties as fmp  n = 5    # Number of subpopulations beta = 1.0*np.ones(n)  # vector of infection rates with lenght n gamma = 0.3*np.ones(n)  # vector of removal or recovery rates with lenght n  # Xij represents the number of susceptibles in subpopulation i that live in subpopulation j # Yij represents the number of infected in subpopulation i that live in subpopulation j # Nij represents the number of total hosts currently in subpopulation i that live in subpopulation j   # vector of size n*n where N0_ij is the number of individuals in each spatial class N0 = np.zeros(n*n) # is the initial number of susceptible individuals in each spatial class; X(0) is a vector of size n \u00d7 n. X0 = np.zeros(n*n) for i in np.arange(0, n*n, n+1):     N0[i] = 1000.0     X0[i] = 800.0  Y0 = np.zeros(n*n) Y0[0] = 1.0 ND = MaxTime = 60. TS = 1.0  l = np.zeros((n, n)) r = np.zeros((n, n)) for i in range(n):     for j in range(n):         if abs(i-j) == 1:             l[i][j] = 0.1 r = 2*np.ones((n, n)) r = r-np.diag(np.diag(r))#putting 0 in the diagonal of the r matrix by extracting the diagonal and subtracting it from the matrix   INPUT0 = np.hstack((X0, Y0, N0))#The numpy.hstack() function takes the arrays X0, Y0, N0 and stacks them horizontally to make a single array of (n*n,3).  INPUT = np.zeros((3*n*n)) for i in range(n*n):     INPUT[3*i] = INPUT0[i]     INPUT[1+3*i] = INPUT0[n*n+i]     INPUT[2+3*i] = INPUT0[2*n*n+i]   def diff_eqs(y, t):     '''The main set of equations'''     Y = np.zeros((3*n*n))     V = y     sumY = np.zeros(n)     sumN = np.zeros(n)      # Calculate number currently in Subpopulation i     for i in range(n):         sumY[i] = 0.0         sumN[i] = 0.0         for j in range(n):             k = 3*(j+i*n)             sumN[i] += V[2+k]             sumY[i] += V[1+k]      # Set all rates to zeros     for i in range(n):         for j in range(n):             k = 3*(j+i*n) #first compute i*n, then add j             Y[k] = 0             Y[1+k] = 0             Y[2+k] = 0      for i in range(n):         for j in range(n):             # Calculate the rates             k = 3 * (j+i*n) #first compute i*n, then add j             K = 3 * (i+j*n) #first compute j*n, then add i             h = 3 * (i+i*n) #first compute i*n, then add i             H = 3 * (j+j*n) #first compute j*n, then add j              Y[k] -= (beta[i]*V[k]*(sumY[i]/sumN[i])) # Xii and Xij             Y[k+1] += (beta[i]*V[k]*(sumY[i]/sumN[i])) # Yii and Yij             Y[k+1] -= (gamma[i]*V[k+1]) # Yii and Yij              # Movement             Y[h] += r[j][i]*V[K]             Y[h] -= l[j][i]*V[h]              Y[h+1] += r[j][i]*V[K+1]             Y[h+1] -= l[j][i]*V[h+1]              Y[h+2] += r[j][i]*V[K+2]             Y[h+2] -= l[j][i]*V[h+2]              Y[k] += l[i][j]*V[H]             Y[k] -= r[i][j]*V[k]              Y[1+k] += l[i][j]*V[1+H]             Y[1+k] -= r[i][j]*V[1+k]              Y[2+k] += l[i][j]*V[2+H]             Y[2+k] -= r[i][j]*V[2+k]     return Y   # For odeint   t_start = 0.0 t_end = ND t_inc = TS t_range = np.arange(t_start, t_end+t_inc, t_inc) t_course = spi.odeint(diff_eqs, INPUT, t_range) tc = t_course  # Plotting totalS = np.zeros((len(tc), 5)) totalI = np.zeros((len(tc), 5))  for i in range(n):     for j in range(n):         k = 3*(j+i*n)         totalS[:, i] += tc[:, k]         totalI[:, i] += tc[:, k+1]   # print len(totalS) pl.subplot(211) for i in range(5):     pl.plot(t_range, totalS[:, i], label=('data %s' %             (i+1)), color=(0.3, i/10.+0.5, 0.1)) pl.xlabel('Time') pl.ylabel('Susceptibles') pl.legend(loc=1, prop=fmp(size='smaller')) pl.subplot(212) for i in range(5):     pl.plot(t_range, totalI[:, i], label=('data %s' %             (i+1)), color=(0.8, i/10.+0., 0.3)) pl.xlabel('Time') pl.ylabel('Infectious') pl.legend(loc=1, prop=fmp(size='smaller'))  pl.show()"},{"location":"PlagueProject/CodeBookKeeling/SeasonFunction/","title":"Season function","text":"In\u00a0[1]: Copied! <pre>#!/usr/bin/env python\n\n####################################################################\n###    This is the PYTHON version of program 5.1 from page 160 of  #\n### \"Modeling Infectious Disease in humans and animals\"            #\n### by Keeling &amp; Rohani.\t\t\t\t\t\t\t\t\t\t   #\n###\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   #\n### It is the simple SIR epidemic with sinusoidal forcing of the   #\n### transmission rate.\t\t\t\t\t\t\t\t\t\t\t   #\n### Note: setting beta1 too high can cause numerical difficulties. #\n####################################################################\n\n###################################\n### Written by Ilias Soumpasis    #\n### ilias.soumpasis@ucd.ie (work) #\n### ilias.soumpasis@gmail.com\t  #\n###################################\n\nimport scipy.integrate as spi\nimport numpy as np\nimport pylab as pl\n\nbeta0=17/13.0;\nbeta1=([0.1]);\ngamma=1/13.0;\nmu=1/(50*365.0);\nS0=1/17.0;\nI0=1e-4;\nND=MaxTime=60*365.0;\nTS=1.0\n\n\n### This code can also be used to generate bifurcation diagrams, by setting\n### beta1 equal to a vector of seasonality rates. The bifurcation diagram is\n### constructed using extrapolated initial conditions. Try:\n#(beta0,beta1,gamma,mu,S0,I0,ND)=(17/13.,np.arange(0.001,0.251,0.001),1/13.,1./(50*365),1/17.,1e-4,20*365)\n\nINPUT=np.array((S0,I0, 1-S0-I0))\n\ndef diff_eqs(INP,t):  \n\t'''The main set of equations'''\n\tY=np.zeros((3))\n\tV = INP   \n\tbeta=beta0*(1+beta1*np.sin(2*np.pi*t/365))\n\tY[0] = mu - beta*V[0]*V[1] - mu*V[0]\n\tY[1] = beta*V[0]*V[1] - mu*V[1] - gamma*V[1]\n\tY[2] = gamma * V[1] - mu * V[2]\n\treturn Y   # For odeint\n\nif len(beta1)==1:\n\tbeta1=beta1[0]\n\tt_start = 0.0; t_end = ND; t_inc = TS\n\tt_range = np.arange(t_start, t_end+t_inc, t_inc)\n\tRES = spi.odeint(diff_eqs,INPUT,t_range)\n\n\tprint(RES)\n\t\n\tt=(np.arange(ND)/365.)\n\t#Ploting\n\tpl.subplot(311)\n\tpl.plot(t,RES[1:,0], 'g', label='S')\n\tpl.xlabel ('Time (years)')\n\tpl.ylabel ('Susceptibles')\n\tpl.subplot(312)\n\tpl.plot(t,RES[1:,1], 'r', label='I')\n\tpl.xlabel ('Time (years)')\n\tpl.ylabel ('Infectious')\n\tpl.subplot(313)\n\tpl.plot(t,1-(RES[1:,0]+RES[1:,1]), 'k', label='R')\n\tpl.xlabel ('Time (years)')\n\tpl.ylabel ('Recovereds')\n\nelse:\n\tpl.ion()\n\tif ND &lt; 3650:\n\t\tND = 3650\n\tbeta2=beta1\n\tBifur_I=np.zeros((len(beta2),10))\n\tfor i in range(len(beta2)):\n\t\tbeta1 = beta2[i]\n\n\t\tt_start = 0.0; t_end = ND; t_inc = TS\n\t\tt_range = np.arange(t_start, t_end+t_inc, t_inc)\n\t\tRES = spi.odeint(diff_eqs,INPUT,t_range)\n\t\tINPUT=RES[-1]\n\n\t\tfor j in range(10):\n\t\t\tBifur_I[i,j]=RES[np.arange(ND)[((ND-j*365.)-1)],1]\n\t\t\t\n\t\t### Producing the plot step by step but it is slow\n\t\t### Prefer to make a video as shown in spatial models\n#\t\tif i &gt; 1:\n#\t\t\tpl.semilogy (beta2, Bifur_I, '.k')\n#\t\t\tpl.xlabel (r'Seasonality, $\\beta_1$')\n#\t\t\tpl.ylabel (r'Level of Infection')\n\tpl.ioff()\n\tpl.semilogy (beta2, (Bifur_I), '.k')\n\t### if TeX commands do not work comment comment the next line\n\tpl.xlabel (r'Seasonality, $\\beta_1$')\n\tpl.ylabel (r'Level of Infection')\n\t### if TeX commands do not work comment uncomment the next line\n#\tpl.xlabel ('Seasonality, beta1')\n#\tpl.ylabel ('Level of Infection (log_10)')\npl.show()\n</pre> #!/usr/bin/env python  #################################################################### ###    This is the PYTHON version of program 5.1 from page 160 of  # ### \"Modeling Infectious Disease in humans and animals\"            # ### by Keeling &amp; Rohani.\t\t\t\t\t\t\t\t\t\t   # ###\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   # ### It is the simple SIR epidemic with sinusoidal forcing of the   # ### transmission rate.\t\t\t\t\t\t\t\t\t\t\t   # ### Note: setting beta1 too high can cause numerical difficulties. # ####################################################################  ################################### ### Written by Ilias Soumpasis    # ### ilias.soumpasis@ucd.ie (work) # ### ilias.soumpasis@gmail.com\t  # ###################################  import scipy.integrate as spi import numpy as np import pylab as pl  beta0=17/13.0; beta1=([0.1]); gamma=1/13.0; mu=1/(50*365.0); S0=1/17.0; I0=1e-4; ND=MaxTime=60*365.0; TS=1.0   ### This code can also be used to generate bifurcation diagrams, by setting ### beta1 equal to a vector of seasonality rates. The bifurcation diagram is ### constructed using extrapolated initial conditions. Try: #(beta0,beta1,gamma,mu,S0,I0,ND)=(17/13.,np.arange(0.001,0.251,0.001),1/13.,1./(50*365),1/17.,1e-4,20*365)  INPUT=np.array((S0,I0, 1-S0-I0))  def diff_eqs(INP,t):   \t'''The main set of equations''' \tY=np.zeros((3)) \tV = INP    \tbeta=beta0*(1+beta1*np.sin(2*np.pi*t/365)) \tY[0] = mu - beta*V[0]*V[1] - mu*V[0] \tY[1] = beta*V[0]*V[1] - mu*V[1] - gamma*V[1] \tY[2] = gamma * V[1] - mu * V[2] \treturn Y   # For odeint  if len(beta1)==1: \tbeta1=beta1[0] \tt_start = 0.0; t_end = ND; t_inc = TS \tt_range = np.arange(t_start, t_end+t_inc, t_inc) \tRES = spi.odeint(diff_eqs,INPUT,t_range)  \tprint(RES) \t \tt=(np.arange(ND)/365.) \t#Ploting \tpl.subplot(311) \tpl.plot(t,RES[1:,0], 'g', label='S') \tpl.xlabel ('Time (years)') \tpl.ylabel ('Susceptibles') \tpl.subplot(312) \tpl.plot(t,RES[1:,1], 'r', label='I') \tpl.xlabel ('Time (years)') \tpl.ylabel ('Infectious') \tpl.subplot(313) \tpl.plot(t,1-(RES[1:,0]+RES[1:,1]), 'k', label='R') \tpl.xlabel ('Time (years)') \tpl.ylabel ('Recovereds')  else: \tpl.ion() \tif ND &lt; 3650: \t\tND = 3650 \tbeta2=beta1 \tBifur_I=np.zeros((len(beta2),10)) \tfor i in range(len(beta2)): \t\tbeta1 = beta2[i]  \t\tt_start = 0.0; t_end = ND; t_inc = TS \t\tt_range = np.arange(t_start, t_end+t_inc, t_inc) \t\tRES = spi.odeint(diff_eqs,INPUT,t_range) \t\tINPUT=RES[-1]  \t\tfor j in range(10): \t\t\tBifur_I[i,j]=RES[np.arange(ND)[((ND-j*365.)-1)],1] \t\t\t \t\t### Producing the plot step by step but it is slow \t\t### Prefer to make a video as shown in spatial models #\t\tif i &gt; 1: #\t\t\tpl.semilogy (beta2, Bifur_I, '.k') #\t\t\tpl.xlabel (r'Seasonality, $\\beta_1$') #\t\t\tpl.ylabel (r'Level of Infection') \tpl.ioff() \tpl.semilogy (beta2, (Bifur_I), '.k') \t### if TeX commands do not work comment comment the next line \tpl.xlabel (r'Seasonality, $\\beta_1$') \tpl.ylabel (r'Level of Infection') \t### if TeX commands do not work comment uncomment the next line #\tpl.xlabel ('Seasonality, beta1') #\tpl.ylabel ('Level of Infection (log_10)') pl.show() <pre>[[5.88235294e-02 1.00000000e-04 9.41076471e-01]\n [5.88673975e-02 1.00004016e-04 9.41032598e-01]\n [5.89112431e-02 1.00027030e-04 9.40988730e-01]\n ...\n [5.39734800e-02 2.16000619e-04 9.45810519e-01]\n [5.40101547e-02 2.14589230e-04 9.45775256e-01]\n [5.40468888e-02 2.13223195e-04 9.45739888e-01]]\n</pre>"},{"location":"PlagueProject/MapScaniaSweden/mapDataScania/","title":"mapDataScania","text":"In\u00a0[2]: Copied! <pre>#Python 3.11.2\n#Import packages\nimport scipy.integrate as scipy\nimport scipy.optimize as optimize\nimport scipy.stats as stats\nimport pandas as pd\nimport numpy as np\nimport pylab as pl\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport json # for pretty printing\nimport geopandas as gpd\n\nimport shutil\nimport sys\nimport os.path\n</pre> #Python 3.11.2 #Import packages import scipy.integrate as scipy import scipy.optimize as optimize import scipy.stats as stats import pandas as pd import numpy as np import pylab as pl import random import matplotlib.pyplot as plt from collections import defaultdict import json # for pretty printing import geopandas as gpd  import shutil import sys import os.path  In\u00a0[3]: Copied! <pre># import logging\n\n# console_handler = logging.StreamHandler()\n# formatter = logging.Formatter(\"%(levelname)s:%(message)s\")\n# console_handler.setFormatter(formatter)\n# logger = logging.getLogger(\"pyproj\")\n# logger.addHandler(console_handler)\n# logger.setLevel(logging.DEBUG)\n\n# logging.basicConfig(filename ='example.log', format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n</pre> # import logging  # console_handler = logging.StreamHandler() # formatter = logging.Formatter(\"%(levelname)s:%(message)s\") # console_handler.setFormatter(formatter) # logger = logging.getLogger(\"pyproj\") # logger.addHandler(console_handler) # logger.setLevel(logging.DEBUG)  # logging.basicConfig(filename ='example.log', format=\"%(levelname)s:%(message)s\", level=logging.DEBUG) In\u00a0[4]: Copied! <pre># logging.info(\"Start of the program\")\n# logging.debug(\"Start of the program\")\n</pre> # logging.info(\"Start of the program\") # logging.debug(\"Start of the program\") <p>Reading the shapefile</p> In\u00a0[5]: Copied! <pre># Get the current working directory\ndata_folder = \"MapScaniaSweden\"\nparishScania_path = \"Parishes1720_1890.shp\"\nSkaneMap_path = \"Skane1720_1890.shp\"\n\nSkaneMap = gpd.read_file(SkaneMap_path)\nparishScaniaMap = gpd.read_file(parishScania_path)\n</pre> # Get the current working directory data_folder = \"MapScaniaSweden\" parishScania_path = \"Parishes1720_1890.shp\" SkaneMap_path = \"Skane1720_1890.shp\"  SkaneMap = gpd.read_file(SkaneMap_path) parishScaniaMap = gpd.read_file(parishScania_path) In\u00a0[6]: Copied! <pre>len(parishScaniaMap)\n</pre> len(parishScaniaMap) Out[6]: <pre>437</pre> In\u00a0[7]: Copied! <pre>parishScaniaMap.loc[2]\n</pre> parishScaniaMap.loc[2] Out[7]: <pre>G_NAME            KRISTIANSTADS HELIGA TREFALDIGHETS F\u00d6RSAMLING\nGET_END_YE                                                 9999\nGET_START_                                                  NaN\nG_UNIT_TYP                                             SWE_KYRK\nG_UNIT                                               10744052.0\nG_SEQ                                                   98647.0\ngeometry      POLYGON ((4249757.033680517 3241445.338076748,...\nName: 2, dtype: object</pre> <p>Checking the projection to see if we can compute the area. In this case, we don't need to reproject our data since it's already on the plane.</p> In\u00a0[8]: Copied! <pre>parishScaniaMap.crs\n</pre> parishScaniaMap.crs Out[8]: <pre>&lt;Projected CRS: EPSG:3034&gt;\nName: ETRS89-extended / LCC Europe\nAxis Info [cartesian]:\n- N[north]: Northing (metre)\n- E[east]: Easting (metre)\nArea of Use:\n- name: Europe - European Union (EU) countries and candidates. Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Iceland; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal including Madeira and Azores; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain including Canary Islands; Sweden; Switzerland; T\u00fcrkiye (Turkey); United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\n- bounds: (-35.58, 24.6, 44.83, 84.73)\nCoordinate Operation:\n- name: Europe Conformal 2001\n- method: Lambert Conic Conformal (2SP)\nDatum: European Terrestrial Reference System 1989 ensemble\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich</pre> In\u00a0[9]: Copied! <pre>parishScaniaMap.info(memory_usage='deep')\n</pre> parishScaniaMap.info(memory_usage='deep') <pre>&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 437 entries, 0 to 436\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   G_NAME      437 non-null    object  \n 1   GET_END_YE  437 non-null    int64   \n 2   GET_START_  44 non-null     float64 \n 3   G_UNIT_TYP  437 non-null    object  \n 4   G_UNIT      437 non-null    float64 \n 5   G_SEQ       437 non-null    float64 \n 6   geometry    437 non-null    geometry\ndtypes: float64(3), geometry(1), int64(1), object(2)\nmemory usage: 88.3 KB\n</pre> In\u00a0[10]: Copied! <pre>type(parishScaniaMap)\n</pre> type(parishScaniaMap) Out[10]: <pre>geopandas.geodataframe.GeoDataFrame</pre> <p>Computing and adding the areas in m2 and km2 to the DataFrame</p> In\u00a0[11]: Copied! <pre>from shapely.geometry import shape\ndef get_area(gpd: gpd.GeoDataFrame):\n    for i in range(len(gpd)):\n        gpd.loc[i, 'area_m2 '] = shape(gpd.loc[i, 'geometry']).area\n        gpd['area_km2 '] = gpd['area_m2 ']/1000000\n    return gpd\n\nparishScaniaMap = get_area(parishScaniaMap)\n</pre> from shapely.geometry import shape def get_area(gpd: gpd.GeoDataFrame):     for i in range(len(gpd)):         gpd.loc[i, 'area_m2 '] = shape(gpd.loc[i, 'geometry']).area         gpd['area_km2 '] = gpd['area_m2 ']/1000000     return gpd  parishScaniaMap = get_area(parishScaniaMap) In\u00a0[17]: Copied! <pre>parishScaniaMap.loc[parishScaniaMap['G_NAME']=='VIBY F\u00d6RSAMLING']\n</pre> parishScaniaMap.loc[parishScaniaMap['G_NAME']=='VIBY F\u00d6RSAMLING'] Out[17]: G_NAME GET_END_YE GET_START_ G_UNIT_TYP G_UNIT G_SEQ geometry area_m2 area_km2 <p>Plotting interactive maps</p> In\u00a0[138]: Copied! <pre>import folium #we need folium to create the interactive maps\nparishScaniaMap.explore(column = \"G_NAME\", \n                        tooltip = \"G_NAME\",\n                        popup = True, #show all values in popup (on click)\n                        cmap = \"Set2\", #color map\n                        legend = False, #show legend\n                        style_kwds = dict(color = \"black\"),#use black for borders\n                       )\n</pre> import folium #we need folium to create the interactive maps parishScaniaMap.explore(column = \"G_NAME\",                          tooltip = \"G_NAME\",                         popup = True, #show all values in popup (on click)                         cmap = \"Set2\", #color map                         legend = False, #show legend                         style_kwds = dict(color = \"black\"),#use black for borders                        ) Out[138]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook <p>Compute and add the centroids as a new column to the original GeoDataFrame</p> In\u00a0[143]: Copied! <pre>def get_centroid(gpd: gpd.GeoDataFrame):\n    for i in range(len(gpd)):\n        gpd.loc[i, 'centroid'] = gpd.geometry.centroid[i]\n    return gpd\n\nparishScaniaMap = get_centroid(parishScaniaMap)\n</pre> def get_centroid(gpd: gpd.GeoDataFrame):     for i in range(len(gpd)):         gpd.loc[i, 'centroid'] = gpd.geometry.centroid[i]     return gpd  parishScaniaMap = get_centroid(parishScaniaMap) <p>Plot the geometry and centroids</p> In\u00a0[71]: Copied! <pre>fig, ax = plt.subplots(figsize=(13,10))\nparishScaniaMap.plot(ax=ax, column = \"G_NAME\", edgecolor='black', legend=False, color = 'lightgray')\nparishScaniaMap.geometry.centroid.plot(ax=ax, marker = 'o', markersize = 5, label = 'centroid', color = 'red')\n#plt.xlabel('Longitude')\n#plt.ylabel('Latitude')\n#plt.legend()\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(13,10)) parishScaniaMap.plot(ax=ax, column = \"G_NAME\", edgecolor='black', legend=False, color = 'lightgray') parishScaniaMap.geometry.centroid.plot(ax=ax, marker = 'o', markersize = 5, label = 'centroid', color = 'red') #plt.xlabel('Longitude') #plt.ylabel('Latitude') #plt.legend() plt.show()  <pre>/opt/homebrew/lib/python3.11/site-packages/geopandas/plotting.py:656: UserWarning: Only specify one of 'column' or 'color'. Using 'color'.\n  warnings.warn(\n</pre> <p>Compute the Euclidian distance in meters between centroids and shared borders. The polygons are projected onto the plane</p> In\u00a0[166]: Copied! <pre>def compute_info(gdp: gpd.GeoDataFrame,\n                 column_name: str,\n                 column_geometry: str = 'geometry',\n                 column_centroid: str = 'centroid',\n                 units: int = 1) -&gt; dict:\n\n    nPolygons = len(gdp)\n    info = defaultdict(dict)\n\n    for i in range(nPolygons):\n        polygon_i = gdp.iloc[i][column_geometry]\n        centroid_i = gdp.loc[i, column_centroid]\n        name_i = gdp.iloc[i][column_name]\n\n        for j in range(i+1, nPolygons):\n            polygon_j = gdp.iloc[j][column_geometry]\n            centroid_j = gdp.loc[j, column_centroid]\n            name_j = gdp.iloc[j][column_name]\n\n            distance = centroid_i.distance(centroid_j) / units * 1.0\n            info[\"distance\"][(i, j)] = distance  # in meters\n            info[\"distance\"][(j, i)] = distance  # in meters\n            info[\"distance\"][(name_i, name_j)] = distance  # in meters\n            info[\"distance\"][(name_j, name_i)] = distance  # in meters\n\n            shared_border = polygon_i.intersection(polygon_j)\n            info[\"shared_border\"][(\n                i, j)] = shared_border.length if shared_border != None else 0  # in meters\n            info[\"shared_border\"][(\n                j, i)] = shared_border.length if shared_border != None else 0  # in meters\n            info[\"shared_border\"][(name_i, name_j)\n                                  ] = info[\"shared_border\"][(i, j)]  # in meters\n            info[\"shared_border\"][(name_j, name_i)\n                                  ] = info[\"shared_border\"][(j, i)]  # in meters\n\n    return info\n\n\ninfo = compute_info(\n    parishScaniaMap, 'G_NAME')\n</pre> def compute_info(gdp: gpd.GeoDataFrame,                  column_name: str,                  column_geometry: str = 'geometry',                  column_centroid: str = 'centroid',                  units: int = 1) -&gt; dict:      nPolygons = len(gdp)     info = defaultdict(dict)      for i in range(nPolygons):         polygon_i = gdp.iloc[i][column_geometry]         centroid_i = gdp.loc[i, column_centroid]         name_i = gdp.iloc[i][column_name]          for j in range(i+1, nPolygons):             polygon_j = gdp.iloc[j][column_geometry]             centroid_j = gdp.loc[j, column_centroid]             name_j = gdp.iloc[j][column_name]              distance = centroid_i.distance(centroid_j) / units * 1.0             info[\"distance\"][(i, j)] = distance  # in meters             info[\"distance\"][(j, i)] = distance  # in meters             info[\"distance\"][(name_i, name_j)] = distance  # in meters             info[\"distance\"][(name_j, name_i)] = distance  # in meters              shared_border = polygon_i.intersection(polygon_j)             info[\"shared_border\"][(                 i, j)] = shared_border.length if shared_border != None else 0  # in meters             info[\"shared_border\"][(                 j, i)] = shared_border.length if shared_border != None else 0  # in meters             info[\"shared_border\"][(name_i, name_j)                                   ] = info[\"shared_border\"][(i, j)]  # in meters             info[\"shared_border\"][(name_j, name_i)                                   ] = info[\"shared_border\"][(j, i)]  # in meters      return info   info = compute_info(     parishScaniaMap, 'G_NAME')  In\u00a0[167]: Copied! <pre>info[\"shared_border\"]\n</pre> info[\"shared_border\"] Out[167]: <pre>{(0, 1): 0.0,\n (1, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TOREKOVS F\u00d6RSAMLING'): 0.0,\n ('TOREKOVS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 2): 0.0,\n (2, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N',\n  'KRISTIANSTADS HELIGA TREFALDIGHETS F\u00d6RSAMLING'): 0.0,\n ('KRISTIANSTADS HELIGA TREFALDIGHETS F\u00d6RSAMLING',\n  'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 3): 0.0,\n (3, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA \u00c5SUMS F\u00d6RSAMLING'): 0.0,\n ('NORRA \u00c5SUMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 4): 0.0,\n (4, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00c4SPHULTS F\u00d6RSAMLING'): 0.0,\n ('\u00c4SPHULTS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 5): 0.0,\n (5, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'DJURR\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('DJURR\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 6): 0.0,\n (6, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TR\u00c4NE F\u00d6RSAMLING'): 0.0,\n ('TR\u00c4NE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 7): 0.0,\n (7, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SKEPPARSL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('SKEPPARSL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 8): 0.0,\n (8, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4 F\u00d6RSAMLING'): 0.0,\n ('V\u00c4 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 9): 0.0,\n (9, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'K\u00d6PINGE F\u00d6RSAMLING'): 0.0,\n ('K\u00d6PINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 10): 0.0,\n (10, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NOSABY F\u00d6RSAMLING'): 0.0,\n ('NOSABY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 11): 0.0,\n (11, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FJ\u00c4LKESTADS F\u00d6RSAMLING'): 0.0,\n ('FJ\u00c4LKESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 12): 0.0,\n (12, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STERSL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STERSL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 13): 0.0,\n (13, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA STR\u00d6 F\u00d6RSAMLING'): 0.0,\n ('NORRA STR\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 14): 0.0,\n (14, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6NNESTADS F\u00d6RSAMLING'): 0.0,\n ('\u00d6NNESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 15): 0.0,\n (15, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'F\u00c4RL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('F\u00c4RL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 16): 0.0,\n (16, 0): 0.0,\n (0, 17): 0.0,\n (17, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'ARASL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('ARASL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 18): 0.0,\n (18, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00c5HUS F\u00d6RSAMLING'): 0.0,\n ('\u00c5HUS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 19): 0.0,\n (19, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA S\u00d6NNARSL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA S\u00d6NNARSL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 20): 0.0,\n (20, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'EVER\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('EVER\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 21): 0.0,\n (21, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LYNGSJ\u00d6 F\u00d6RSAMLING'): 0.0,\n ('LYNGSJ\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 22): 0.0,\n (22, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'RINKABY F\u00d6RSAMLING L L\u00c4N'): 0.0,\n ('RINKABY F\u00d6RSAMLING L L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 23): 0.0,\n (23, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NYM\u00d6 F\u00d6RSAMLING'): 0.0,\n ('NYM\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 24): 0.0,\n (24, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TROLLE LJUNGBY F\u00d6RSAMLING'): 0.0,\n ('TROLLE LJUNGBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 25): 0.0,\n (25, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'IV\u00d6 F\u00d6RSAMLING'): 0.0,\n ('IV\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 26): 0.0,\n (26, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'KIABY F\u00d6RSAMLING'): 0.0,\n ('KIABY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 27): 0.0,\n (27, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FJ\u00c4LKINGE F\u00d6RSAMLING'): 0.0,\n ('FJ\u00c4LKINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 28): 0.0,\n (28, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GUSTAV ADOLFS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n ('GUSTAV ADOLFS F\u00d6RSAMLING L L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 29): 0.0,\n (29, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MAGLEHEMS F\u00d6RSAMLING'): 0.0,\n ('MAGLEHEMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 30): 0.0,\n (30, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6RR\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('H\u00d6RR\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 31): 0.0,\n (31, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HUAR\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('HUAR\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 32): 0.0,\n (32, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'DEGEBERGA F\u00d6RSAMLING'): 0.0,\n ('DEGEBERGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 33): 0.0,\n (33, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VITTSK\u00d6VLE F\u00d6RSAMLING'): 0.0,\n ('VITTSK\u00d6VLE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 34): 0.0,\n (34, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA VRAMS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA VRAMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 35): 0.0,\n (35, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA VRAMS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA VRAMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 36): 0.0,\n (36, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LINDER\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('LINDER\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 37): 0.0,\n (37, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c5NGA F\u00d6RSAMLING L L\u00c4N'): 0.0,\n ('V\u00c5NGA F\u00d6RSAMLING L L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 38): 0.0,\n (38, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'OPPMANNA F\u00d6RSAMLING'): 0.0,\n ('OPPMANNA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 39): 0.0,\n (39, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SIMRISHAMNS F\u00d6RSAMLING'): 0.0,\n ('SIMRISHAMNS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 40): 0.0,\n (40, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA N\u00d6BBEL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA N\u00d6BBEL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 41): 0.0,\n (41, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'J\u00c4RRESTADS F\u00d6RSAMLING'): 0.0,\n ('J\u00c4RRESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 42): 0.0,\n (42, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA HOBY F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA HOBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 43): 0.0,\n (43, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BORRBY F\u00d6RSAMLING'): 0.0,\n ('BORRBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 44): 0.0,\n (44, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA TOMMARPS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA TOMMARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 45): 0.0,\n (45, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STIBY F\u00d6RSAMLING'): 0.0,\n ('STIBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 46): 0.0,\n (46, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'RAVLUNDA F\u00d6RSAMLING'): 0.0,\n ('RAVLUNDA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 47): 0.0,\n (47, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA HERRESTADS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA HERRESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 48): 0.0,\n (48, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HAMMENH\u00d6GS F\u00d6RSAMLING'): 0.0,\n ('HAMMENH\u00d6GS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 49): 0.0,\n (49, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SANKT OLOFS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n ('SANKT OLOFS F\u00d6RSAMLING L L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 50): 0.0,\n (50, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'R\u00d6RUMS F\u00d6RSAMLING'): 0.0,\n ('R\u00d6RUMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 51): 0.0,\n (51, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VITABY F\u00d6RSAMLING'): 0.0,\n ('VITABY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 52): 0.0,\n (52, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'REBBELBERGA F\u00d6RSAMLING'): 0.0,\n ('REBBELBERGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 53): 0.0,\n (53, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6JA F\u00d6RSAMLING'): 0.0,\n ('H\u00d6JA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 54): 0.0,\n (54, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MUNKA LJUNGBY F\u00d6RSAMLING'): 0.0,\n ('MUNKA LJUNGBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 55): 0.0,\n (55, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STR\u00d6VELSTORPS F\u00d6RSAMLING'): 0.0,\n ('STR\u00d6VELSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 56): 0.0,\n (56, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STARBY F\u00d6RSAMLING'): 0.0,\n ('STARBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 57): 0.0,\n (57, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BARK\u00c5KRA F\u00d6RSAMLING'): 0.0,\n ('BARK\u00c5KRA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 58): 0.0,\n (58, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HJ\u00c4RNARPS F\u00d6RSAMLING'): 0.0,\n ('HJ\u00c4RNARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 59): 0.0,\n (59, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6SSJ\u00d6 F\u00d6RSAMLING'): 0.0,\n ('\u00d6SSJ\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 60): 0.0,\n (60, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00c4SSLEHOLMS F\u00d6RSAMLING'): 0.0,\n ('H\u00c4SSLEHOLMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 61): 0.0,\n (61, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STOBY F\u00d6RSAMLING'): 0.0,\n ('STOBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 62): 0.0,\n (62, 0): 0.0,\n (0, 63): 0.0,\n (63, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA SANDBY F\u00d6RSAMLING'): 0.0,\n ('NORRA SANDBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 64): 0.0,\n (64, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GUML\u00d6SA F\u00d6RSAMLING'): 0.0,\n ('GUML\u00d6SA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 65): 0.0,\n (65, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00d6RBY F\u00d6RSAMLING L L\u00c4N'): 0.0,\n ('S\u00d6RBY F\u00d6RSAMLING L L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 66): 0.0,\n (66, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00c4STVEDA F\u00d6RSAMLING'): 0.0,\n ('H\u00c4STVEDA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 67): 0.0,\n (67, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FARSTORPS F\u00d6RSAMLING'): 0.0,\n ('FARSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 68): 0.0,\n (68, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VITTSJ\u00d6 F\u00d6RSAMLING'): 0.0,\n ('VITTSJ\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 69): 0.0,\n (69, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA \u00c5KARPS F\u00d6RSAMLING'): 0.0,\n ('NORRA \u00c5KARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 70): 0.0,\n (70, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA MELLBY F\u00d6RSAMLING'): 0.0,\n ('NORRA MELLBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 71): 0.0,\n (71, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00c4GLINGE F\u00d6RSAMLING'): 0.0,\n ('H\u00c4GLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 72): 0.0,\n (72, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MATTER\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('MATTER\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 73): 0.0,\n (73, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FINJA F\u00d6RSAMLING'): 0.0,\n ('FINJA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 74): 0.0,\n (74, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'R\u00d6KE F\u00d6RSAMLING'): 0.0,\n ('R\u00d6KE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 75): 0.0,\n (75, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TIRUPS F\u00d6RSAMLING'): 0.0,\n ('TIRUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 76): 0.0,\n (76, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SVAL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('SVAL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 77): 0.0,\n (77, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STENESTADS F\u00d6RSAMLING'): 0.0,\n ('STENESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 78): 0.0,\n (78, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HALMSTADS F\u00d6RSAMLING'): 0.0,\n ('HALMSTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 79): 0.0,\n (79, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SIREK\u00d6PINGE F\u00d6RSAMLING'): 0.0,\n ('SIREK\u00d6PINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 80): 0.0,\n (80, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'K\u00c4LLS N\u00d6BBEL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('K\u00c4LLS N\u00d6BBEL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 81): 0.0,\n (81, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA SKR\u00c4VLINGE F\u00d6RSAMLING'): 0.0,\n ('NORRA SKR\u00c4VLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 82): 0.0,\n (82, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'KONGA F\u00d6RSAMLING'): 0.0,\n ('KONGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 83): 0.0,\n (83, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'R\u00d6ST\u00c5NGA F\u00d6RSAMLING'): 0.0,\n ('R\u00d6ST\u00c5NGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 84): 0.0,\n (84, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TORRL\u00d6SA F\u00d6RSAMLING'): 0.0,\n ('TORRL\u00d6SA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 85): 0.0,\n (85, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'KYRKHEDDINGE F\u00d6RSAMLING'): 0.0,\n ('KYRKHEDDINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 86): 0.0,\n (86, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BJ\u00c4LLERUPS F\u00d6RSAMLING'): 0.0,\n ('BJ\u00c4LLERUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 87): 0.0,\n (87, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'UPP\u00c5KRA F\u00d6RSAMLING'): 0.0,\n ('UPP\u00c5KRA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 88): 0.0,\n (88, 0): 0.0,\n (0, 89): 0.0,\n (89, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STAFFANSTORPS F\u00d6RSAMLING'): 0.0,\n ('STAFFANSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 90): 0.0,\n (90, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BR\u00c5GARPS F\u00d6RSAMLING'): 0.0,\n ('BR\u00c5GARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 91): 0.0,\n (91, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'M\u00d6LLEBERGA F\u00d6RSAMLING'): 0.0,\n ('M\u00d6LLEBERGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 92): 0.0,\n (92, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00c4RSL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('S\u00c4RSL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 93): 0.0,\n (93, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TOTTARPS F\u00d6RSAMLING'): 0.0,\n ('TOTTARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 94): 0.0,\n (94, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GESSIE F\u00d6RSAMLING'): 0.0,\n ('GESSIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 95): 0.0,\n (95, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6K\u00d6PINGE F\u00d6RSAMLING'): 0.0,\n ('H\u00d6K\u00d6PINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 96): 0.0,\n (96, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VELLINGE F\u00d6RSAMLING'): 0.0,\n ('VELLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 97): 0.0,\n (97, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FALSTERBO F\u00d6RSAMLING'): 0.0,\n ('FALSTERBO F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 98): 0.0,\n (98, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00c5SL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('H\u00c5SL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 99): 0.0,\n (99, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'R\u00c4NGS F\u00d6RSAMLING'): 0.0,\n ('R\u00c4NGS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 100): 0.0,\n (100, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00d6DRA \u00c5KARPS F\u00d6RSAMLING'): 0.0,\n ('S\u00d6DRA \u00c5KARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 101): 0.0,\n (101, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA GREVIE F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA GREVIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 102): 0.0,\n (102, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'ARRIE F\u00d6RSAMLING'): 0.0,\n ('ARRIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 103): 0.0,\n (103, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BJUVS F\u00d6RSAMLING'): 0.0,\n ('BJUVS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 104): 0.0,\n (104, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA VRAMS F\u00d6RSAMLING'): 0.0,\n ('NORRA VRAMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 105): 0.0,\n (105, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'EKEBY F\u00d6RSAMLING M L\u00c4N'): 0.0,\n ('EKEBY F\u00d6RSAMLING M L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 106): 0.0,\n (106, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'K\u00c4VLINGE F\u00d6RSAMLING'): 0.0,\n ('K\u00c4VLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 107): 0.0,\n (107, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FILBORNA F\u00d6RSAMLING'): 0.0,\n ('FILBORNA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 108): 0.0,\n (108, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6GAN\u00c4S F\u00d6RSAMLING'): 0.0,\n ('H\u00d6GAN\u00c4S F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 109): 0.0,\n (109, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4SBY F\u00d6RSAMLING'): 0.0,\n ('V\u00c4SBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 110): 0.0,\n (110, 0): 0.0,\n (0, 111): 0.0,\n (111, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VIKENS F\u00d6RSAMLING'): 0.0,\n ('VIKENS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 112): 0.0,\n (112, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BRUNNBY F\u00d6RSAMLING'): 0.0,\n ('BRUNNBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 113): 0.0,\n (113, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'JONSTORPS F\u00d6RSAMLING'): 0.0,\n ('JONSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 114): 0.0,\n (114, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FARHULTS F\u00d6RSAMLING'): 0.0,\n ('FARHULTS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 115): 0.0,\n (115, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'ESL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('ESL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 116): 0.0,\n (116, 0): 0.0,\n (0, 117): 0.0,\n (117, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'REMMARL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('REMMARL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 118): 0.0,\n (118, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STEHAGS F\u00d6RSAMLING'): 0.0,\n ('STEHAGS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 119): 0.0,\n (119, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BOSARPS F\u00d6RSAMLING'): 0.0,\n ('BOSARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 120): 0.0,\n (120, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TROLLEN\u00c4S F\u00d6RSAMLING'): 0.0,\n ('TROLLEN\u00c4S F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 121): 0.0,\n (121, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA STR\u00d6 F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA STR\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 122): 0.0,\n (122, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6RTOFTA F\u00d6RSAMLING'): 0.0,\n ('\u00d6RTOFTA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 123): 0.0,\n (123, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BILLINGE F\u00d6RSAMLING'): 0.0,\n ('BILLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 124): 0.0,\n (124, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HURVA F\u00d6RSAMLING'): 0.0,\n ('HURVA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 125): 0.0,\n (125, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'RESL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('RESL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 126): 0.0,\n (126, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA KARABY F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA KARABY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 127): 0.0,\n (127, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6GSER\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('H\u00d6GSER\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 128): 0.0,\n (128, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HARL\u00d6SA F\u00d6RSAMLING'): 0.0,\n ('HARL\u00d6SA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 129): 0.0,\n (129, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HAMMARLUNDA F\u00d6RSAMLING'): 0.0,\n ('HAMMARLUNDA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 130): 0.0,\n (130, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HOLMBY F\u00d6RSAMLING'): 0.0,\n ('HOLMBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 131): 0.0,\n (131, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'G\u00c5RDST\u00c5NGA F\u00d6RSAMLING'): 0.0,\n ('G\u00c5RDST\u00c5NGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 132): 0.0,\n (132, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SKEGLINGE F\u00d6RSAMLING'): 0.0,\n ('SKEGLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 133): 0.0,\n (133, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BORLUNDA F\u00d6RSAMLING'): 0.0,\n ('BORLUNDA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 134): 0.0,\n (134, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SKARHULTS F\u00d6RSAMLING'): 0.0,\n ('SKARHULTS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 135): 0.0,\n (135, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA STR\u00d6 F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA STR\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 136): 0.0,\n (136, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'YSTADS F\u00d6RSAMLING'): 0.0,\n ('YSTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 137): 0.0,\n (137, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SK\u00c5RBY F\u00d6RSAMLING'): 0.0,\n ('SK\u00c5RBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 138): 0.0,\n (138, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SJ\u00d6RUPS F\u00d6RSAMLING'): 0.0,\n ('SJ\u00d6RUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 139): 0.0,\n (139, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SN\u00c5RESTADS F\u00d6RSAMLING'): 0.0,\n ('SN\u00c5RESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 140): 0.0,\n (140, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BALK\u00c5KRA F\u00d6RSAMLING'): 0.0,\n ('BALK\u00c5KRA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 141): 0.0,\n (141, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BJ\u00c4RESJ\u00d6 F\u00d6RSAMLING'): 0.0,\n ('BJ\u00c4RESJ\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 142): 0.0,\n (142, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HEDESKOGA F\u00d6RSAMLING'): 0.0,\n ('HEDESKOGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 143): 0.0,\n (143, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BROMMA F\u00d6RSAMLING M L\u00c4N'): 0.0,\n ('BROMMA F\u00d6RSAMLING M L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 144): 0.0,\n (144, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6JA F\u00d6RSAMLING M L\u00c4N'): 0.0,\n ('\u00d6JA F\u00d6RSAMLING M L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 145): 0.0,\n (145, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STORA HERRESTADS F\u00d6RSAMLING'): 0.0,\n ('STORA HERRESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 146): 0.0,\n (146, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STORA K\u00d6PINGE F\u00d6RSAMLING'): 0.0,\n ('STORA K\u00d6PINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 147): 0.0,\n (147, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6GESTADS F\u00d6RSAMLING'): 0.0,\n ('H\u00d6GESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 148): 0.0,\n (148, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BORRIE F\u00d6RSAMLING'): 0.0,\n ('BORRIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 149): 0.0,\n (149, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00d6VESTADS F\u00d6RSAMLING'): 0.0,\n ('S\u00d6VESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 150): 0.0,\n (150, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6RUPS F\u00d6RSAMLING'): 0.0,\n ('H\u00d6RUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 151): 0.0,\n (151, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'L\u00d6DERUPS F\u00d6RSAMLING'): 0.0,\n ('L\u00d6DERUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 152): 0.0,\n (152, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'INGELSTORPS F\u00d6RSAMLING'): 0.0,\n ('INGELSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 153): 0.0,\n (153, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TRELLEBORGS F\u00d6RSAMLING'): 0.0,\n ('TRELLEBORGS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 154): 0.0,\n (154, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TRELLEBORGS STADSF\u00d6RSAMLING'): 0.0,\n ('TRELLEBORGS STADSF\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 155): 0.0,\n (155, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MAGLARPS F\u00d6RSAMLING'): 0.0,\n ('MAGLARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 156): 0.0,\n (156, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA VEMMERL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA VEMMERL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 157): 0.0,\n (157, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HAMMARL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('HAMMARL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 158): 0.0,\n (158, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BODARPS F\u00d6RSAMLING'): 0.0,\n ('BODARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 159): 0.0,\n (159, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FUGLIE F\u00d6RSAMLING'): 0.0,\n ('FUGLIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 160): 0.0,\n (160, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA ALSTADS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA ALSTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 161): 0.0,\n (161, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FRU ALSTADS F\u00d6RSAMLING'): 0.0,\n ('FRU ALSTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 162): 0.0,\n (162, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STORA SL\u00c5GARPS F\u00d6RSAMLING'): 0.0,\n ('STORA SL\u00c5GARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 163): 0.0,\n (163, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GYLLE F\u00d6RSAMLING'): 0.0,\n ('GYLLE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 164): 0.0,\n (164, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'DALK\u00d6PINGE F\u00d6RSAMLING'): 0.0,\n ('DALK\u00d6PINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 165): 0.0,\n (165, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GISL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('GISL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 166): 0.0,\n (166, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'B\u00d6SARPS F\u00d6RSAMLING'): 0.0,\n ('B\u00d6SARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 167): 0.0,\n (167, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LILLA ISIE F\u00d6RSAMLING'): 0.0,\n ('LILLA ISIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 168): 0.0,\n (168, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA TORPS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA TORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 169): 0.0,\n (169, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA KLAGSTORPS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA KLAGSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 170): 0.0,\n (170, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00c4SP\u00d6 F\u00d6RSAMLING'): 0.0,\n ('\u00c4SP\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 171): 0.0,\n (171, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TULLSTORPS F\u00d6RSAMLING'): 0.0,\n ('TULLSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 172): 0.0,\n (172, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'K\u00c4LLSTORPS F\u00d6RSAMLING'): 0.0,\n ('K\u00c4LLSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 173): 0.0,\n (173, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'G\u00c4RDSL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('G\u00c4RDSL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 174): 0.0,\n (174, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GR\u00d6NBY F\u00d6RSAMLING'): 0.0,\n ('GR\u00d6NBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 175): 0.0,\n (175, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'DAGSTORPS F\u00d6RSAMLING'): 0.0,\n ('DAGSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 176): 0.0,\n (176, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA KARABY F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA KARABY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 177): 0.0,\n (177, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LACKAL\u00c4NGA F\u00d6RSAMLING'): 0.0,\n ('LACKAL\u00c4NGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 178): 0.0,\n (178, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'ST\u00c4VIE F\u00d6RSAMLING'): 0.0,\n ('ST\u00c4VIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 179): 0.0,\n (179, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00d6DERVIDINGE F\u00d6RSAMLING'): 0.0,\n ('S\u00d6DERVIDINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 180): 0.0,\n (180, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VIRKE F\u00d6RSAMLING'): 0.0,\n ('VIRKE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 181): 0.0,\n (181, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LILLA HARRIE F\u00d6RSAMLING'): 0.0,\n ('LILLA HARRIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 182): 0.0,\n (182, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'STORA HARRIE F\u00d6RSAMLING'): 0.0,\n ('STORA HARRIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 183): 0.0,\n (183, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HOFTERUPS F\u00d6RSAMLING'): 0.0,\n ('HOFTERUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 184): 0.0,\n (184, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BARSEB\u00c4CKS F\u00d6RSAMLING'): 0.0,\n ('BARSEB\u00c4CKS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 185): 0.0,\n (185, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'L\u00d6DDEK\u00d6PINGE F\u00d6RSAMLING'): 0.0,\n ('L\u00d6DDEK\u00d6PINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 186): 0.0,\n (186, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6GS F\u00d6RSAMLING M L\u00c4N'): 0.0,\n ('H\u00d6GS F\u00d6RSAMLING M L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 187): 0.0,\n (187, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LOMMA F\u00d6RSAMLING'): 0.0,\n ('LOMMA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 188): 0.0,\n (188, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BORGEBY F\u00d6RSAMLING'): 0.0,\n ('BORGEBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 189): 0.0,\n (189, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FL\u00c4DIE F\u00d6RSAMLING'): 0.0,\n ('FL\u00c4DIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 190): 0.0,\n (190, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FJELIE F\u00d6RSAMLING'): 0.0,\n ('FJELIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 191): 0.0,\n (191, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SVEDALA F\u00d6RSAMLING'): 0.0,\n ('SVEDALA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 192): 0.0,\n (192, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'B\u00d6RRINGE F\u00d6RSAMLING'): 0.0,\n ('B\u00d6RRINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 193): 0.0,\n (193, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA K\u00c4RRSTORPS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA K\u00c4RRSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 194): 0.0,\n (194, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'T\u00d6RRINGE F\u00d6RSAMLING'): 0.0,\n ('T\u00d6RRINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 195): 0.0,\n (195, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BJ\u00c4RSH\u00d6GS F\u00d6RSAMLING'): 0.0,\n ('BJ\u00c4RSH\u00d6GS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 196): 0.0,\n (196, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SKABERSJ\u00d6 F\u00d6RSAMLING'): 0.0,\n ('SKABERSJ\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 197): 0.0,\n (197, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BARA F\u00d6RSAMLING'): 0.0,\n ('BARA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 198): 0.0,\n (198, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HYBY F\u00d6RSAMLING'): 0.0,\n ('HYBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 199): 0.0,\n (199, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SKURUPS F\u00d6RSAMLING'): 0.0,\n ('SKURUPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 200): 0.0,\n (200, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HASSLE B\u00d6SARPS F\u00d6RSAMLING'): 0.0,\n ('HASSLE B\u00d6SARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 201): 0.0,\n (201, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SVENSTORPS F\u00d6RSAMLING'): 0.0,\n ('SVENSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 202): 0.0,\n (202, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA VEMMENH\u00d6GS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA VEMMENH\u00d6GS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 203): 0.0,\n (203, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRA VEMMENH\u00d6GS F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRA VEMMENH\u00d6GS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 204): 0.0,\n (204, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SKIVARPS F\u00d6RSAMLING'): 0.0,\n ('SKIVARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 205): 0.0,\n (205, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA N\u00d6BBEL\u00d6VS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA N\u00d6BBEL\u00d6VS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 206): 0.0,\n (206, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'KATSL\u00d6SA F\u00d6RSAMLING'): 0.0,\n ('KATSL\u00d6SA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 207): 0.0,\n (207, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SOLBERGA F\u00d6RSAMLING M L\u00c4N'): 0.0,\n ('SOLBERGA F\u00d6RSAMLING M L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 208): 0.0,\n (208, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6RSJ\u00d6 F\u00d6RSAMLING M L\u00c4N'): 0.0,\n ('\u00d6RSJ\u00d6 F\u00d6RSAMLING M L\u00c4N', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 209): 0.0,\n (209, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SLIMMINGE F\u00d6RSAMLING'): 0.0,\n ('SLIMMINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 210): 0.0,\n (210, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'VILLIE F\u00d6RSAMLING'): 0.0,\n ('VILLIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 211): 0.0,\n (211, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'ILSTORPS F\u00d6RSAMLING'): 0.0,\n ('ILSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 212): 0.0,\n (212, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BJ\u00d6RKA F\u00d6RSAMLING'): 0.0,\n ('BJ\u00d6RKA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 213): 0.0,\n (213, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00d6VDE F\u00d6RSAMLING'): 0.0,\n ('S\u00d6VDE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 214): 0.0,\n (214, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BLENTARPS F\u00d6RSAMLING'): 0.0,\n ('BLENTARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 215): 0.0,\n (215, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TOL\u00c5NGA F\u00d6RSAMLING'): 0.0,\n ('TOL\u00c5NGA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 216): 0.0,\n (216, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'R\u00d6DDINGE F\u00d6RSAMLING'): 0.0,\n ('R\u00d6DDINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 217): 0.0,\n (217, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'L\u00d6VESTADS F\u00d6RSAMLING'): 0.0,\n ('L\u00d6VESTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 218): 0.0,\n (218, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FR\u00c4NNINGE F\u00d6RSAMLING'): 0.0,\n ('FR\u00c4NNINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 219): 0.0,\n (219, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BRANDSTADS F\u00d6RSAMLING'): 0.0,\n ('BRANDSTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 220): 0.0,\n (220, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6VEDS F\u00d6RSAMLING'): 0.0,\n ('\u00d6VEDS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 221): 0.0,\n (221, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6RBY F\u00d6RSAMLING'): 0.0,\n ('H\u00d6RBY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 222): 0.0,\n (222, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'L\u00c5NGAR\u00d6DS F\u00d6RSAMLING'): 0.0,\n ('L\u00c5NGAR\u00d6DS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 223): 0.0,\n (223, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SVENSK\u00d6PS F\u00d6RSAMLING'): 0.0,\n ('SVENSK\u00d6PS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 224): 0.0,\n (224, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'S\u00d6DRA R\u00d6RUMS F\u00d6RSAMLING'): 0.0,\n ('S\u00d6DRA R\u00d6RUMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 225): 0.0,\n (225, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'FULLTOFTA F\u00d6RSAMLING'): 0.0,\n ('FULLTOFTA F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 226): 0.0,\n (226, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', '\u00d6STRABY F\u00d6RSAMLING'): 0.0,\n ('\u00d6STRABY F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 227): 0.0,\n (227, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STERSTADS F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STERSTADS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 228): 0.0,\n (228, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'H\u00d6\u00d6RS F\u00d6RSAMLING'): 0.0,\n ('H\u00d6\u00d6RS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 229): 0.0,\n (229, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BOSJ\u00d6KLOSTERS F\u00d6RSAMLING'): 0.0,\n ('BOSJ\u00d6KLOSTERS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 230): 0.0,\n (230, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MUNKARPS F\u00d6RSAMLING'): 0.0,\n ('MUNKARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 231): 0.0,\n (231, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'NORRA R\u00d6RUMS F\u00d6RSAMLING'): 0.0,\n ('NORRA R\u00d6RUMS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 232): 0.0,\n (232, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TJ\u00d6RNARPS F\u00d6RSAMLING'): 0.0,\n ('TJ\u00d6RNARPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 233): 0.0,\n (233, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MALM\u00d6 SANKT PETRI F\u00d6RSAMLING'): 0.0,\n ('MALM\u00d6 SANKT PETRI F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 234): 0.0,\n (234, 0): 0.0,\n (0, 235): 0.0,\n (235, 0): 0.0,\n (0, 236): 0.0,\n (236, 0): 0.0,\n (0, 237): 0.0,\n (237, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'KIRSEBERGS F\u00d6RSAMLING'): 0.0,\n ('KIRSEBERGS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 238): 0.0,\n (238, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'MALM\u00d6 SANKT PAULI F\u00d6RSAMLING'): 0.0,\n ('MALM\u00d6 SANKT PAULI F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 239): 0.0,\n (239, 0): 0.0,\n (0, 240): 0.0,\n (240, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'M\u00d6LLEV\u00c5NGENS F\u00d6RSAMLING'): 0.0,\n ('M\u00d6LLEV\u00c5NGENS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 241): 0.0,\n (241, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LIMHAMNS F\u00d6RSAMLING'): 0.0,\n ('LIMHAMNS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 242): 0.0,\n (242, 0): 0.0,\n (0, 243): 0.0,\n (243, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'V\u00c4STRA SKR\u00c4VLINGE F\u00d6RSAMLING'): 0.0,\n ('V\u00c4STRA SKR\u00c4VLINGE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 244): 0.0,\n (244, 0): 0.0,\n (0, 245): 0.0,\n (245, 0): 0.0,\n (0, 246): 0.0,\n (246, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HUSIE F\u00d6RSAMLING'): 0.0,\n ('HUSIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 247): 0.0,\n (247, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'OXIE F\u00d6RSAMLING'): 0.0,\n ('OXIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 248): 0.0,\n (248, 0): 0.0,\n (0, 249): 0.0,\n (249, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'GLOSTORPS F\u00d6RSAMLING'): 0.0,\n ('GLOSTORPS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 250): 0.0,\n (250, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SOFIELUNDS F\u00d6RSAMLING'): 0.0,\n ('SOFIELUNDS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 251): 0.0,\n (251, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'HYLLIE F\u00d6RSAMLING'): 0.0,\n ('HYLLIE F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 252): 0.0,\n (252, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'KULLADALS F\u00d6RSAMLING'): 0.0,\n ('KULLADALS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 253): 0.0,\n (253, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'BUNKEFLO F\u00d6RSAMLING'): 0.0,\n ('BUNKEFLO F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 254): 0.0,\n (254, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'TYGELSJ\u00d6 F\u00d6RSAMLING'): 0.0,\n ('TYGELSJ\u00d6 F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 255): 0.0,\n (255, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'LUNDS LANDSF\u00d6RSAMLING'): 0.0,\n ('LUNDS LANDSF\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 256): 0.0,\n (256, 0): 0.0,\n ('HOVS F\u00d6RSAMLING L L\u00c4N', 'SANKT PETERS KLOSTERS F\u00d6RSAMLING'): 0.0,\n ('SANKT PETERS KLOSTERS F\u00d6RSAMLING', 'HOVS F\u00d6RSAMLING L L\u00c4N'): 0.0,\n (0, 257): 0.0,\n (257, 0): 0.0,\n ...}</pre> In\u00a0[168]: Copied! <pre>info[\"distance\"][(2, 10)]\n</pre> info[\"distance\"][(2, 10)] Out[168]: <pre>3483.9364790236477</pre> <p>To calculate the distance between two points given their latitude and longitude, you can use the Haversine formula. The Haversine formula (see https://en.wikipedia.org/wiki/Haversine_formula) calculates the great-circle distance between two points on a sphere, which is an approximation for the Earth's surface.</p> <p>Getting access to the coordinates of each centroid:</p> In\u00a0[\u00a0]: Copied! <pre>x_coord = gpd.GeoSeries(centroid[0]).iloc[0].x\ny_coord = gpd.GeoSeries(centroid[0]).iloc[0].y\nprint([x_coord, y_coord])\n</pre> x_coord = gpd.GeoSeries(centroid[0]).iloc[0].x y_coord = gpd.GeoSeries(centroid[0]).iloc[0].y print([x_coord, y_coord]) <pre>[4162353.095562172, 3282055.860910411]\n</pre> In\u00a0[\u00a0]: Copied! <pre>def getCoordinates(centroidParishScania: gpd.GeoSeries):\n    longitude = gpd.GeoSeries(centroidParishScania).iloc[0].x\n    latitude = gpd.GeoSeries(centroidParishScania).iloc[0].y\n    return [latitude,longitude]\n\nprint(getCoordinates(centroidParishScania[0]),getCoordinates(centroidParishScania[1]))\n</pre> def getCoordinates(centroidParishScania: gpd.GeoSeries):     longitude = gpd.GeoSeries(centroidParishScania).iloc[0].x     latitude = gpd.GeoSeries(centroidParishScania).iloc[0].y     return [latitude,longitude]  print(getCoordinates(centroidParishScania[0]),getCoordinates(centroidParishScania[1]))  <pre>[3282055.860910411, 4162353.095562172] [3279338.7830117564, 4157750.0618345523]\n</pre> <p>Defining the Haversine distance:</p> In\u00a0[\u00a0]: Copied! <pre>import math\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the great circle distance in kilometers between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    \n    # Convert latitude and longitude from degrees to radians\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n\n    # Haversine formula\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    #c = 2 * math.asin(math.sqrt(a))\n\n    # Earth radius in kilometers (mean radius)\n    R = 6371.01\n\n    # Calculate the distance\n    distance = R * c\n\n    return distance\n\n# Example usage\ndistance = haversine_distance(40.7128, -74.0060, 34.0522, -118.2437)\nprint(distance, \"kilometers\")\n\nhaversine_distance(getCoordinates(centroidParishScania[0])[0], getCoordinates(centroidParishScania[0])[\n                   1], getCoordinates(centroidParishScania[1])[0], getCoordinates(centroidParishScania[1])[1])\n</pre> import math  def haversine_distance(lat1, lon1, lat2, lon2):     \"\"\"     Calculate the great circle distance in kilometers between two points      on the earth (specified in decimal degrees)     \"\"\"          # Convert latitude and longitude from degrees to radians     lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])      # Haversine formula     dlat = lat2 - lat1     dlon = lon2 - lon1     a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2     c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))     #c = 2 * math.asin(math.sqrt(a))      # Earth radius in kilometers (mean radius)     R = 6371.01      # Calculate the distance     distance = R * c      return distance  # Example usage distance = haversine_distance(40.7128, -74.0060, 34.0522, -118.2437) print(distance, \"kilometers\")  haversine_distance(getCoordinates(centroidParishScania[0])[0], getCoordinates(centroidParishScania[0])[                    1], getCoordinates(centroidParishScania[1])[0], getCoordinates(centroidParishScania[1])[1])   <pre>3935.7524322054765 kilometers\n</pre> Out[\u00a0]: <pre>17205.486192064203</pre> <p>Using Haversine formula to compute the distance between the polygons' centroids:</p> In\u00a0[\u00a0]: Copied! <pre>nPolygons = len(parishScaniaMap)\nHaversine_distance_matrix = np.zeros((nPolygons,nPolygons))\n\nfor i in range(len(centroid)):\n    for j in range(i + 1, len(centroid)):\n        distance = haversine_distance(getCoordinates(centroid[i])[0], getCoordinates(centroid[i])[\n                                      1], getCoordinates(centroid[j])[0], getCoordinates(centroid[j])[1])\n        Haversine_distance_matrix[i, j] = distance\n        Haversine_distance_matrix[j, i] = distance\n        #print(Haversine_distance_matrix)\n        #print(f\"The Haversine distance between centroid {i+1} and centroid {j+1} is {distance} meters\")\n</pre> nPolygons = len(parishScaniaMap) Haversine_distance_matrix = np.zeros((nPolygons,nPolygons))  for i in range(len(centroid)):     for j in range(i + 1, len(centroid)):         distance = haversine_distance(getCoordinates(centroid[i])[0], getCoordinates(centroid[i])[                                       1], getCoordinates(centroid[j])[0], getCoordinates(centroid[j])[1])         Haversine_distance_matrix[i, j] = distance         Haversine_distance_matrix[j, i] = distance         #print(Haversine_distance_matrix)         #print(f\"The Haversine distance between centroid {i+1} and centroid {j+1} is {distance} meters\") In\u00a0[\u00a0]: Copied! <pre># Create a dataframe from the distance matrix\n#Haversine_distance_matrix = pd.DataFrame(Haversine_distance_matrix)\n\n#Rename the columns and rows\n#Haversine_distance_matrix.columns = labels\n#Haversine_distance_matrix.rows = labels\n\n#Print the updated distance matrix\n#Haversine_distance_matrix.head()\n\n# Export the distance matrix to a csv file\n#Haversine_distance_matrix.to_csv('Haversine_distance_matrix.csv', encoding='utf-8', index=True, header=True)\n</pre> # Create a dataframe from the distance matrix #Haversine_distance_matrix = pd.DataFrame(Haversine_distance_matrix)  #Rename the columns and rows #Haversine_distance_matrix.columns = labels #Haversine_distance_matrix.rows = labels  #Print the updated distance matrix #Haversine_distance_matrix.head()  # Export the distance matrix to a csv file #Haversine_distance_matrix.to_csv('Haversine_distance_matrix.csv', encoding='utf-8', index=True, header=True) <p>Plotting</p> In\u00a0[\u00a0]: Copied! <pre>fig,ax = plt.subplots(figsize=(13,10))\nparishScaniaMap.plot(ax=ax, column = \"G_NAME\", edgecolor='black', legend=False)\nplt.xlabel('Meters')\nplt.ylabel('Meters')\n# legend = ax.get_legend()\n# legend.set_bbox_to_anchor((1, 0.2))\nplt.show()\n</pre> fig,ax = plt.subplots(figsize=(13,10)) parishScaniaMap.plot(ax=ax, column = \"G_NAME\", edgecolor='black', legend=False) plt.xlabel('Meters') plt.ylabel('Meters') # legend = ax.get_legend() # legend.set_bbox_to_anchor((1, 0.2)) plt.show() In\u00a0[\u00a0]: Copied! <pre>parishScaniaMap.columns\n</pre> parishScaniaMap.columns Out[\u00a0]: <pre>Index(['G_NAME', 'GET_END_YE', 'GET_START_', 'G_UNIT_TYP', 'G_UNIT', 'G_SEQ',\n       'geometry', 'area_m2 ', 'area_km2 '],\n      dtype='object')</pre> In\u00a0[\u00a0]: Copied! <pre>fig,ax = plt.subplots(figsize=(13,10))\nparishScaniaMap.plot(column = \"area_km2 \", ax=ax,legend=True,\n                     legend_kwds={'label': \"Area in km2 by parish\", 'orientation': \"horizontal\"}\n                     )\n</pre> fig,ax = plt.subplots(figsize=(13,10)) parishScaniaMap.plot(column = \"area_km2 \", ax=ax,legend=True,                      legend_kwds={'label': \"Area in km2 by parish\", 'orientation': \"horizontal\"}                      ) Out[\u00a0]: <pre>&lt;Axes: &gt;</pre>"},{"location":"blog/","title":"Blog","text":""}]}